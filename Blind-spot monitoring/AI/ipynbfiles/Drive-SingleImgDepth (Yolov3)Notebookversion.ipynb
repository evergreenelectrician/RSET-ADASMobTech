{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SingleImgDepthYoloNotebook (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1EU_yqgXM4XV",
        "aNulh2XvM748",
        "5crs6F9brpwq",
        "takpM5RvrupZ",
        "t51RsFcXEMHk",
        "m0jfQRuyqhHq",
        "IwvIc7iFZ__d",
        "8_2iY7HwCTlm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4393c29efccd450283c6b1d91aa962e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3f893cbaa30a46ddabc070fe68322b00",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3685483498b246dd988eeec445bada69",
              "IPY_MODEL_312286c600e142dfa6e5d1787795f332"
            ]
          }
        },
        "3f893cbaa30a46ddabc070fe68322b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3685483498b246dd988eeec445bada69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1d6d5090e43049dbaa709791dc241a2c",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ad365513b484a22ad8b8b52cfb6e7c1"
          }
        },
        "312286c600e142dfa6e5d1787795f332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ad684af2a3304e14a658d7975284aa0e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/3 [02:32&lt;?, ?epoch/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2459bfcdc8b94865bc8d3975d2892511"
          }
        },
        "1d6d5090e43049dbaa709791dc241a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ad365513b484a22ad8b8b52cfb6e7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad684af2a3304e14a658d7975284aa0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2459bfcdc8b94865bc8d3975d2892511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EU_yqgXM4XV"
      },
      "source": [
        "#Anchor Box Calculation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "F2N5uR4jM32T",
        "outputId": "979e3ad7-cef3-4a91-f591-b5e4a272012a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class main:\n",
        "  def __init__(self,no):\n",
        "    self.num_clusters = no\n",
        "\n",
        "   \n",
        "args = main(9)\n",
        "\n",
        "'''\n",
        "Created on Feb 20, 2017\n",
        "@author: jumabek\n",
        "'''\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import argparse\n",
        "#import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import random \n",
        "import math\n",
        "\n",
        "width_in_cfg_file = 1248.\n",
        "height_in_cfg_file = 1248.\n",
        "\n",
        "def IOU(X,centroids):\n",
        "    similarities = []\n",
        "    k = len(centroids)\n",
        "    for centroid in centroids:\n",
        "        c_x,c_y,c_z = centroid\n",
        "        x,y,z = X\n",
        "        inner_vol = min(x,c_x)*min(y,c_y)*min(z,c_z)\n",
        "        similarity = inner_vol/((x*y*z+c_x*c_y*c_z)-inner_vol)\n",
        "        similarities.append(similarity) # will become (k,) shape\n",
        "    return np.array(similarities) \n",
        "\n",
        "def avg_IOU(X,centroids):\n",
        "    n,d = X.shape\n",
        "    sum = 0.\n",
        "    for i in range(X.shape[0]):\n",
        "        #note IOU() will return array which contains IoU for each centroid and X[i] // slightly ineffective, but I am too lazy\n",
        "        sum+= max(IOU(X[i],centroids)) \n",
        "    return sum/n\n",
        "\n",
        "def write_anchors_to_file(centroids,X,anchor_file):\n",
        "    f = open(anchor_file,'w')\n",
        "    \n",
        "    anchors = centroids.copy()\n",
        "    print(anchors.shape)\n",
        "\n",
        "    for i in range(anchors.shape[0]):\n",
        "        anchors[i][0]*=width_in_cfg_file\n",
        "        anchors[i][1]*=height_in_cfg_file\n",
        "        anchors[i][2]*= 5\n",
        "         \n",
        "\n",
        "    widths = anchors[:,0]+ anchors[:,1]\n",
        "    sorted_indices = np.argsort(widths)\n",
        "\n",
        "    print('Anchors = ', anchors[sorted_indices])\n",
        "        \n",
        "    for i in sorted_indices[:-1]:\n",
        "        f.write('%0.5f,%0.5f,%0.5f, '%(anchors[i,0],anchors[i,1],anchors[i,2]))\n",
        "\n",
        "    #there should not be comma after last anchor, that's why\n",
        "    f.write('%0.5f,%0.5f,%0.5f\\n'%(anchors[sorted_indices[-1:],0],anchors[sorted_indices[-1:],1],anchors[sorted_indices[-1:],2]))\n",
        "    \n",
        "    f.write('%f\\n'%(avg_IOU(X,centroids)))\n",
        "    plt.scatter(centroids.shape[0], avg_IOU(X,centroids))\n",
        "\n",
        "def kmeans(X,centroids,eps,anchor_file):\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    iterations = 0\n",
        "    k,dim = centroids.shape\n",
        "    print(\"k,dim =\",k,dim)\n",
        "    prev_assignments = np.ones(N)*(-1)    \n",
        "    iter = 0\n",
        "    old_D = np.zeros((N,k))\n",
        "\n",
        "    while True:\n",
        "        D = [] \n",
        "        iter+=1           \n",
        "        for i in range(N):\n",
        "            d = 1 - IOU(X[i],centroids)\n",
        "            D.append(d)\n",
        "        D = np.array(D) # D.shape = (N,k)\n",
        "        \n",
        "        print(\"iter {}: dists = {}\".format(iter,np.sum(np.abs(old_D-D))))\n",
        "            \n",
        "        #assign samples to centroids \n",
        "        assignments = np.argmin(D,axis=1)\n",
        "        \n",
        "        if (assignments == prev_assignments).all() :\n",
        "            print(\"Centroids = \",centroids)\n",
        "            write_anchors_to_file(centroids,X,anchor_file)\n",
        "            return\n",
        "\n",
        "        #calculate new centroids\n",
        "        centroid_sums=np.zeros((k,dim),np.float)\n",
        "        for i in range(N):\n",
        "            centroid_sums[assignments[i]]+=X[i]        \n",
        "        for j in range(k):            \n",
        "            centroids[j] = centroid_sums[j]/(np.sum(assignments==j))\n",
        "        \n",
        "        prev_assignments = assignments.copy()     \n",
        "        old_D = D.copy()  \n",
        "\n",
        "    \n",
        "\n",
        "annotation_dims = []\n",
        "\n",
        "size = np.zeros((1,1,3))\n",
        "file_path = '/gdrive/My Drive/data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  sensor = 'LIDAR_TOP'\n",
        "  lidar_top_data = nusc.get('sample_data', sample['data'][sensor])\n",
        "  ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "  for annotation in sample['anns']:\n",
        "    annotation = nusc.get('sample_annotation',annotation)\n",
        "    x,y,z = annotation['size']\n",
        "    x = float(x) / 140.\n",
        "    y = float(y) / 140.\n",
        "    z = float(z) / 5.\n",
        "    annotation_dims.append(tuple(map(float,(x,y,z))))\n",
        "#print(annotation_dims)  \n",
        "annotation_dims = np.array(annotation_dims)\n",
        "\n",
        "eps = 0.005\n",
        "\n",
        "if args.num_clusters == 0:\n",
        "    for num_clusters in range(1,11): #we make 1 through 10 clusters \n",
        "        anchor_file = 'anchors%d.txt'%(num_clusters)\n",
        "\n",
        "        indices = [ random.randrange(annotation_dims.shape[0]) for i in range(num_clusters)]\n",
        "        centroids = annotation_dims[indices]\n",
        "        kmeans(annotation_dims,centroids,eps,anchor_file)\n",
        "        print('centroids.shape', centroids.shape)\n",
        "else:\n",
        "    anchor_file = 'anchors%d.txt'%(args.num_clusters)\n",
        "    indices = [ random.randrange(annotation_dims.shape[0]) for i in range(args.num_clusters)]\n",
        "    centroids = annotation_dims[indices]\n",
        "    kmeans(annotation_dims,centroids,eps,anchor_file)\n",
        "    print('centroids.shape', centroids.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-72f4f8892842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/gdrive/My Drive/data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mscene\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m   \u001b[0msample_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_sample_token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nusc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "Tu1geigrSzaD",
        "outputId": "95a6be29-b57e-4d2c-a01f-d9fd6e1e70df"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "annotation_dims = []\n",
        "cat = []\n",
        "size = np.zeros((1,1,3))\n",
        "file_path = '/gdrive/My Drive/data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  sensor = 'LIDAR_TOP'\n",
        "  lidar_top_data = nusc.get('sample_data', sample['data'][sensor])\n",
        "  ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "  for annotation in sample['anns']:\n",
        "    annotation = nusc.get('sample_annotation',annotation)\n",
        "    print(annotation['size'],annotation['category_name'])\n",
        "    x,y,z = annotation['size']\n",
        "    cat.append(annotation['category_name'])\n",
        "    #x = float(x) / 140.\n",
        "    #y = float(y) / 140.\n",
        "    #z = float(z) / 5.\n",
        "    annotation_dims.append(tuple(map(float,(x,y,z))))\n",
        "print(annotation_dims)\n",
        "d = Counter(cat)   \n",
        "print(d)\n",
        "print(annotation_dims[1])\n",
        "print(max(x[0] for x in annotation_dims))\n",
        "print(max(x[1] for x in annotation_dims))\n",
        "print(max(x[2] for x in annotation_dims))\n",
        "#print(sum(annotation_dims[:,1]))\n",
        "#print(sum(annotation_dims[:,2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d83d1b63f439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/gdrive/My Drive/data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mscene\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0msample_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_sample_token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nusc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLnS-Foc5Zav"
      },
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes \n",
        "\n",
        "nusc = NuScenes(version='v1.0-trainval', dataroot='data', verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vfkhkwxPhJa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNulh2XvM748"
      },
      "source": [
        "#Start off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5rdnxJVl4LZ",
        "outputId": "59d63825-45ff-4b4c-dbab-fe76b36e2062"
      },
      "source": [
        "!pip3 install terminaltables\n",
        "!pip install nuscenes-devkit\n",
        "!pip install turfpy\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Building wheels for collected packages: terminaltables\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=f80763f771d8ca14c53abd6c2b5e08546ed618082411979bee4f7c7d19f16662\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built terminaltables\n",
            "Installing collected packages: terminaltables\n",
            "Successfully installed terminaltables-3.1.0\n",
            "Collecting nuscenes-devkit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/2f/8738468c899c92733466574d700d9ab9f4f745948847b85c7ecfec4e336f/nuscenes_devkit-1.1.2-py3-none-any.whl (282kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 286kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (4.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (7.0.0)\n",
            "Collecting pyquaternion>=0.9.5\n",
            "  Downloading https://files.pythonhosted.org/packages/49/b3/d8482e8cacc8ea15a356efea13d22ce1c5914a9ee36622ba250523240bf2/pyquaternion-0.9.9-py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (1.4.1)\n",
            "Requirement already satisfied: descartes in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (1.7.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (4.41.1)\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from nuscenes-devkit) (0.22.2.post1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->nuscenes-devkit) (4.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->nuscenes-devkit) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->nuscenes-devkit) (7.6.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->nuscenes-devkit) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->nuscenes-devkit) (5.0.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->nuscenes-devkit) (5.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire->nuscenes-devkit) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->nuscenes-devkit) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.1->nuscenes-devkit) (53.0.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.1->nuscenes-devkit) (0.29.21)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nuscenes-devkit) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nuscenes-devkit) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nuscenes-devkit) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nuscenes-devkit) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nuscenes-devkit) (1.0.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (4.3.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->nuscenes-devkit) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nuscenes-devkit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nuscenes-devkit) (4.7.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nuscenes-devkit) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nuscenes-devkit) (0.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nuscenes-devkit) (2.11.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->nuscenes-devkit) (5.1.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->nuscenes-devkit) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->nuscenes-devkit) (3.5.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->nuscenes-devkit) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->nuscenes-devkit) (1.0.18)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nuscenes-devkit) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->nuscenes-devkit) (22.0.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->nuscenes-devkit) (0.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->nuscenes-devkit) (4.8.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->nuscenes-devkit) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->nuscenes-devkit) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->nuscenes-devkit) (2.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->nuscenes-devkit) (0.2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->nuscenes-devkit) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->nuscenes-devkit) (20.9)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=f2a525d66a66e9b37ac7384da2a68a4cccc1a86f6470f7441d6915c2be5a6c4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "Successfully built fire\n",
            "Installing collected packages: pyquaternion, fire, nuscenes-devkit\n",
            "Successfully installed fire-0.4.0 nuscenes-devkit-1.1.2 pyquaternion-0.9.9\n",
            "Collecting turfpy\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/bb/a33df98337909ad69111b8afc9fb5ef5f7b9b1516282a8aeeb33a5a47394/turfpy-0.0.5.tar.gz\n",
            "Collecting geojson\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from turfpy) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from turfpy) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from turfpy) (1.19.5)\n",
            "Building wheels for collected packages: turfpy\n",
            "  Building wheel for turfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for turfpy: filename=turfpy-0.0.5-cp36-none-any.whl size=35769 sha256=9609a9cdeca59925dc2f80e5b0c5f6657169ee16ac9bc9f0e94f7ff1aee5cbff\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/bc/f1/08b44fd3724a9c07109aba1dcae130ec547d4e1dc5aa364ac8\n",
            "Successfully built turfpy\n",
            "Installing collected packages: geojson, turfpy\n",
            "Successfully installed geojson-2.5.0 turfpy-0.0.5\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/48/4b59e775d7fd3917201de7975892241a5145ce482bc739bf22e83735e5c4/wandb-0.10.19-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/74/59016eecaefa52443cd69cbb50e01851fa8bf3d9526771e2fae60ac6270c/sentry_sdk-0.20.3-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/67/47a04d8a9d7f94645676fe683f1ee3fe9be01fe407686c180768a92abaac/GitPython-3.1.13-py3-none-any.whl (159kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 21.0MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (53.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 10.9MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6490 sha256=5d65788d339379022a4392f85d0f8ee103b3a134bce5152c3f6ada1c765ab5ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=6c2a921403edf4446c3c5b77927c4a9aa3122048a0ed12d69170dc580a5b302b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, sentry-sdk, configparser, docker-pycreds, shortuuid, smmap, gitdb, GitPython, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.13 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-0.20.3 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTQzGTHQ0FfF"
      },
      "source": [
        "#import wandb\n",
        "#wandb.init()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9xA9XuX-UvJ",
        "outputId": "461e1b81-27d3-40df-a4d2-6948a3a1f2f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH_zIRd2IP1j"
      },
      "source": [
        "\n",
        "from __future__ import division\n",
        "\n",
        "from terminaltables import AsciiTable\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crs6F9brpwq"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q0mYRDmhNzp"
      },
      "source": [
        "#Angle Decoder\n",
        "\n",
        "def angle_decoder(r):\n",
        "  teta1 = torch.asin(2*r[0] - 1)\n",
        "  teta2 = torch.acos(2*r[1] - 1)\n",
        "  teta = 0\n",
        "  if 2*r[0] - 1 >= 0 and 2*r[1] - 1 >= 0:\n",
        "    teta = (teta1+teta2)/2\n",
        "  elif 2*r[0] - 1 >= 0 and 2*r[1] - 1 < 0:\n",
        "    teta = (math.pi-teta1+teta2)/2\n",
        "  elif 2*r[0] - 1 < 0 and 2*r[1] - 1 <= 0:\n",
        "    teta = (math.pi - teta1+2*math.pi -teta2)/2\n",
        "  elif 2*r[0] - 1 < 0 and 2*r[1] - 1 > 0:\n",
        "    teta = (2*math.pi + teta1+2*math.pi - teta2)/2\n",
        "  return teta"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8QQyIXmraeD"
      },
      "source": [
        "#Utils\n",
        "\n",
        "from __future__ import division\n",
        "from shapely.geometry import Polygon\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from turfpy.transformation import intersect\n",
        "from turfpy.measurement import area\n",
        "from geojson import Feature\n",
        "\n",
        "\n",
        "def to_cpu(tensor):\n",
        "    return tensor.detach().cpu()\n",
        "\n",
        "def rotate_around_point(point, radians, origin=(0, 0)):\n",
        "  \"\"\"Rotate a point around a given point.\n",
        "  \n",
        "  I call this the \"low performance\" version since it's recalculating\n",
        "  the same values more than once [cos(radians), sin(radians), x-ox, y-oy).\n",
        "  It's more readable than the next function, though.\n",
        "  \"\"\"\n",
        "  x, y = point\n",
        "  ox, oy = origin\n",
        "\n",
        "  qx = ox + math.cos(radians) * (x - ox) + math.sin(radians) * (y - oy)\n",
        "  qy = oy + -math.sin(radians) * (x - ox) + math.cos(radians) * (y - oy)\n",
        "\n",
        "  return qx.item(), qy.item()\n",
        "\n",
        "def load_classes(path):\n",
        "    \"\"\"\n",
        "    Loads class labels at 'path'\n",
        "    \"\"\"\n",
        "    fp = open(path, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def rescale_boxes(boxes, current_dim, original_shape):\n",
        "    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n",
        "    orig_h, orig_w = original_shape\n",
        "    # The amount of padding that was added\n",
        "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
        "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
        "    # Image height and width after padding is removed\n",
        "    unpad_h = current_dim - pad_y\n",
        "    unpad_w = current_dim - pad_x\n",
        "    # Rescale bounding boxes to dimension of original image\n",
        "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
        "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    y = x.new(x.shape)\n",
        "    y[..., 0] = x[..., 0] - x[..., 3] / 2\n",
        "    y[..., 1] = x[..., 1] - x[..., 4] / 2\n",
        "    y[..., 2] = x[..., 2] - x[..., 5] / 2\n",
        "    y[..., 3] = x[..., 0] + x[..., 3] / 2\n",
        "    y[..., 4] = x[..., 1] + x[..., 4] / 2\n",
        "    y[..., 5] = x[..., 2] + x[..., 5] / 2\n",
        "    return y\n",
        "\n",
        "\n",
        "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:    True positives (list).\n",
        "        conf:  Objectness value from 0-1 (list).\n",
        "        pred_cls: Predicted object classes (list).\n",
        "        target_cls: True object classes (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort by objectness\n",
        "    i = np.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes = np.unique(target_cls)\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    ap, p, r = [], [], []\n",
        "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
        "        i = pred_cls == c\n",
        "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
        "        n_p = i.sum()  # Number of predicted objects\n",
        "\n",
        "        if n_p == 0 and n_gt == 0:\n",
        "            continue\n",
        "        elif n_p == 0 or n_gt == 0:\n",
        "            ap.append(0)\n",
        "            r.append(0)\n",
        "            p.append(0)\n",
        "        else:\n",
        "            # Accumulate FPs and TPs\n",
        "            fpc = (1 - tp[i]).cumsum()\n",
        "            tpc = (tp[i]).cumsum()\n",
        "\n",
        "            # Recall\n",
        "            recall_curve = tpc / (n_gt + 1e-16)\n",
        "            r.append(recall_curve[-1])\n",
        "\n",
        "            # Precision\n",
        "            precision_curve = tpc / (tpc + fpc)\n",
        "            p.append(precision_curve[-1])\n",
        "\n",
        "            # AP from recall-precision curve\n",
        "            ap.append(compute_ap(recall_curve, precision_curve))\n",
        "\n",
        "    # Compute F1 score (harmonic mean of precision and recall)\n",
        "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
        "    f1 = 2 * p * r / (p + r + 1e-16)\n",
        "\n",
        "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def get_batch_statistics(outputs, targets, iou_threshold):\n",
        "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
        "    batch_metrics = []\n",
        "    for sample_i in range(len(outputs)):\n",
        "\n",
        "        if outputs[sample_i] is None:\n",
        "            continue\n",
        "\n",
        "        output = outputs[sample_i]\n",
        "        pred_boxes = output[:, :8]\n",
        "        pred_scores = output[:, 8]\n",
        "        pred_labels = output[:, -1]\n",
        "\n",
        "        true_positives = np.zeros(pred_boxes.shape[0])\n",
        "\n",
        "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
        "        target_labels = annotations[:, 0] if len(annotations) else []\n",
        "        if len(annotations):\n",
        "            detected_boxes = []\n",
        "            target_boxes = annotations[:, 1:]\n",
        "\n",
        "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
        "\n",
        "                # If targets are found break\n",
        "                if len(detected_boxes) == len(annotations):\n",
        "                    break\n",
        "\n",
        "                # Ignore if label is not one of the target labels\n",
        "                if pred_label not in target_labels:\n",
        "                    continue\n",
        "\n",
        "                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n",
        "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
        "                    true_positives[pred_i] = 1\n",
        "                    detected_boxes += [box_index]\n",
        "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
        "    return batch_metrics\n",
        "\n",
        "\n",
        "def bbox_wh_iou(wh1, wh2):\n",
        "    wh2 = wh2.t()\n",
        "    w1, l1, h1 = wh1[0], wh1[1] ,wh1[2]\n",
        "    w2, l2, h2 = wh2[0], wh2[1] ,wh1[2]\n",
        "    inter_area = torch.min(w1, w2) * torch.min(l1, l2) * torch.min(h1, h2)\n",
        "    union_area = (w1 * l1 * h1 + 1e-16) + (w2 * l2 * h2) - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "    \"\"\"\n",
        "    iou_scores = []\n",
        "    # Transform from center and width to exact coordinates\n",
        "    b1_x1, b1_x3 = box1[:,0] - box1[:,3] / 2, box1[:,0] + box1[:,3] / 2\n",
        "    b1_y1, b1_y3 = box1[:,1] - box1[:,4] / 2, box1[:,1] + box1[:,4] / 2\n",
        "    b1_x2, b1_x4 = b1_x3, b1_x1\n",
        "    b1_y2, b1_y4 = b1_y1, b1_y3\n",
        "    #b1_z1, b1_z3 = box1[2] - box1[5] / 2, box1[2] + box1[5] / 2\n",
        "    b2_x1, b2_x3 = box2[:,0] - box2[:,3] / 2, box2[:,0] + box2[:,3] / 2\n",
        "    b2_x2, b2_x4 = b2_x3, b2_x1\n",
        "    b2_y1, b2_y3 = box2[:,1] - box2[:,4] / 2, box2[:,1] + box2[:,4] / 2\n",
        "    b2_y2, b2_y4 = b2_y1, b2_y3\n",
        "    #b2_z1, b2_z3 = box2[2] - box2[5] / 2, box2[2] + box2[5] / 2\n",
        "\n",
        "    rotation1 = [angle_decoder(r[6:8]) for r in box1]\n",
        "    rotation2 = [angle_decoder(r[6:8]) for r in box2]\n",
        "    \n",
        "\n",
        "\n",
        "    if box1.shape == box2.shape:\n",
        "      for x11,x12,x13,x14,y11,y12,y13,y14,x21,x22,x23,x24,y21,y22,y23,y24,r1,r2, b1,b2 in zip(b1_x1,b1_x2,b1_x3,b1_x4, b1_y1,b1_y2,b1_y3,b1_y4, b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation1,rotation2, box1,box2):\n",
        "        p1 = Polygon([rotate_around_point((x11, y11), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x12, y12), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x13, y13), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x14, y14), r1, (b1[0], b1[1]))])\n",
        "        p2 = Polygon([rotate_around_point((x21, y21), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x22, y22), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x23, y23), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x24, y24), r2, (b2[0], b2[1]))])\n",
        "        p3 = p1.intersection(p2)\n",
        "        inter_area = p3.area\n",
        "        b1_area = p1.area\n",
        "        b2_area = p2.area\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "        return torch.Tensor(iou_scores).to(device)\n",
        "    else:\n",
        "      x11,x12,x13,x14, y11,y12,y13,y14 = b1_x1[0],b1_x2[0],b1_x3[0],b1_x4[0], b1_y1[0],b1_y2[0],b1_y3[0],b1_y4[0]\n",
        "      r1, b1 = rotation1[0], box1[0]\n",
        "      for x21,x22,x23,x24,y21,y22,y23,y24,r2 ,b2 in zip(b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation2, box2):\n",
        "        p1 = Polygon([rotate_around_point((x11, y11), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x12, y12), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x13, y13), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x14, y14), r1, (b1[0], b1[1]))])\n",
        "        p2 = Polygon([rotate_around_point((x21, y21), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x22, y22), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x23, y23), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x24, y24), r2, (b2[0], b2[1]))])\n",
        "        p3 = p1.intersection(p2)\n",
        "        inter_area = p3.area\n",
        "        b1_area = p1.area\n",
        "        b2_area = p2.area\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "        return torch.Tensor(iou_scores).to(device)\n",
        "\n",
        "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.5):\n",
        "    \"\"\"\n",
        "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
        "    Non-Maximum Suppression to further filter detections.\n",
        "    Returns detections with shape:\n",
        "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
        "    \"\"\"\n",
        "\n",
        "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "    #prediction[..., :6] = xywh2xyxy(prediction[..., :6])\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    for image_i, image_pred in enumerate(prediction):\n",
        "        # Filter out confidence scores below threshold\n",
        "        image_pred = image_pred[image_pred[:, 8] >= conf_thres]\n",
        "        # If none are remaining => process next image\n",
        "        if not image_pred.size(0):\n",
        "            continue\n",
        "        # Object confidence times class confidence\n",
        "        score = image_pred[:, 8] * image_pred[:, 9:].max(1)[0]\n",
        "        # Sort by it\n",
        "        image_pred = image_pred[(-score).argsort()]\n",
        "        class_confs, class_preds = image_pred[:, 9:].max(1, keepdim=True)\n",
        "        detections = torch.cat((image_pred[:, :9], class_confs.float(), class_preds.float()), 1)\n",
        "        # Perform non-maximum suppression\n",
        "        keep_boxes = []\n",
        "        while detections.size(0):\n",
        "            large_overlap = bbox_iou(detections[0, :8].unsqueeze(0), detections[:, :8]) > nms_thres\n",
        "            label_match = detections[0, -1] == detections[:, -1]\n",
        "            label_match = label_match.cuda()\n",
        "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
        "            invalid = large_overlap & label_match\n",
        "            weights = detections[invalid, 8:9]\n",
        "            # Merge overlapping bboxes by order of confidence\n",
        "            detections[0, :8] = (weights * detections[invalid, :8]).sum(0) / weights.sum()\n",
        "            keep_boxes += [detections[0]]\n",
        "            detections = detections[~invalid]\n",
        "        if keep_boxes:\n",
        "            output[image_i] = torch.stack(keep_boxes)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
        "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
        "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
        "\n",
        "    nB = pred_boxes.size(0)\n",
        "    nA = pred_boxes.size(1)\n",
        "    nC = pred_cls.size(-1)\n",
        "    nG = pred_boxes.size(2)\n",
        "    \n",
        "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
        "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
        "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tz = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tl = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tr1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tr2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
        "    \n",
        "    target_boxes = target[:, 2:10]\n",
        "    target_boxes[:, :2] = target_boxes[:, :2] * nG\n",
        "    target_boxes[:, 3:5] = target_boxes[:, 3:5] * nG\n",
        "    target_boxes[:, 5] = target_boxes[:, 5] * 5\n",
        "\n",
        "    gxy = target_boxes[:, :2]\n",
        "    gwlh = target_boxes[:, 3:6]\n",
        "\n",
        "    ious = torch.stack([bbox_wh_iou(anchor, gwlh) for anchor in anchors]) #here\n",
        "    try:\n",
        "      best_ious, best_n = ious.max(0)\n",
        "    except:\n",
        "      tconf = obj_mask.float()\n",
        "      return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf\n",
        "\n",
        "    b, target_labels = target[:, :2].long().t()\n",
        "    \n",
        "    gx, gy = gxy.t()\n",
        "    gw, gl, gh = gwlh.t()\n",
        "    gi, gj = gxy.long().t()\n",
        "    \n",
        "    # Set masks\n",
        "    obj_mask[b, best_n, gj, gi] = 1\n",
        "    noobj_mask[b, best_n, gj, gi] = 0\n",
        "\n",
        "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
        "    for i, anchor_ious in enumerate(ious.t()):\n",
        "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
        "\n",
        "    # Coordinates\n",
        "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
        "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
        "    tz[b, best_n, gj, gi] = target_boxes[:,2]\n",
        "    # Width and height\n",
        "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
        "    tl[b, best_n, gj, gi] = torch.log(gl / anchors[best_n][:, 1] + 1e-16)\n",
        "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 2] + 1e-16)\n",
        "    \n",
        "    tr1[b, best_n, gj, gi] = target_boxes[:,6]\n",
        "    tr2[b, best_n, gj, gi] = target_boxes[:,7]\n",
        "    # One-hot encoding of label\n",
        "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
        "    # Compute label correctness and iou at best anchor\n",
        "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
        "    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes)\n",
        "    tconf = obj_mask.float()\n",
        "    target_boxes[:, :2] = target_boxes[:, :2] / nG\n",
        "    target_boxes[:, 3:5] = target_boxes[:, 3:5] / nG\n",
        "    target_boxes[:, 5] = target_boxes[:, 5] / 5\n",
        "    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfNuZWdMrK2w"
      },
      "source": [
        "#Parse_config\n",
        "\n",
        "def parse_model_config(path):\n",
        "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
        "    file = open(path, 'r')\n",
        "    lines = file.read().split('\\n')\n",
        "    lines = [x for x in lines if x and not x.startswith('#')]\n",
        "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
        "    module_defs = []\n",
        "    for line in lines:\n",
        "        if line.startswith('['): # This marks the start of a new block\n",
        "            module_defs.append({})\n",
        "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
        "            if module_defs[-1]['type'] == 'convolutional':\n",
        "                module_defs[-1]['batch_normalize'] = 0\n",
        "        else:\n",
        "            key, value = line.split(\"=\")\n",
        "            value = value.strip()\n",
        "            module_defs[-1][key.rstrip()] = value.strip()\n",
        "\n",
        "    return module_defs\n",
        "\n",
        "def parse_data_config(path):\n",
        "    \"\"\"Parses the data configuration file\"\"\"\n",
        "    options = dict()\n",
        "    options['gpus'] = '0,1,2,3'\n",
        "    options['num_workers'] = '10'\n",
        "    with open(path, 'r') as fp:\n",
        "        lines = fp.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line == '' or line.startswith('#'):\n",
        "            continue\n",
        "        key, value = line.split('=')\n",
        "        options[key.strip()] = value.strip()\n",
        "    return options\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xUCTALNYmpR"
      },
      "source": [
        "#Draw\n",
        "def draw(targets):\n",
        "  fig = plt.figure(figsize=(8,8))\n",
        "  ax = fig.add_subplot(111)\n",
        "  for target in targets:\n",
        "    rotation = angle_decoder(target[6:8])\n",
        "    length = target[3]\n",
        "    width = target[4]\n",
        "    x_temp, y_temp = rotate_around_point((target[0],-target[1]), -(rotation), origin=(target[0]-width/2, -target[1] - length/2))\n",
        "    x_offset, y_offset = x_temp - target[0], y_temp + target[1]\n",
        "    rectas = patches.Rectangle(xy=((target[0]-width/2) - x_offset, (-target[1] - length/2) - y_offset) ,width=width, height=length, angle = (rotation)*180/math.pi, linewidth=1, color='blue', fill=False)\n",
        "    ax.add_patch(rectas)\n",
        "    ax.scatter(target[0], -target[1], color = 'red', s=10)\n",
        "  ax.scatter(0.5, -0.5)\n",
        "  plt.xlim(0, 1)\n",
        "  plt.ylim(-1,0)\n",
        "  plt.show()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLTv_sBOJrxK"
      },
      "source": [
        "#Sampler\r\n",
        "import torch\r\n",
        "\r\n",
        "class ResumableRandomSampler(torch.utils.data.Sampler):\r\n",
        "    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\r\n",
        "    If with replacement, then user can specify :attr:`num_samples` to draw.\r\n",
        "    Arguments:\r\n",
        "        data_source (Dataset): dataset to sample from\r\n",
        "        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\r\n",
        "        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument\r\n",
        "            is supposed to be specified only when `replacement` is ``True``.\r\n",
        "        generator (Generator): Generator used in sampling.\r\n",
        "    \"\"\"\r\n",
        "    #data_source: Sized\r\n",
        "    #replacement: bool\r\n",
        "\r\n",
        "    def __init__(self, data_source):\r\n",
        "        self.data_source = data_source\r\n",
        "        self.generator = torch.Generator()\r\n",
        "        self.generator.manual_seed(47)\r\n",
        "        \r\n",
        "        self.perm_index = 0\r\n",
        "        self.perm = torch.randperm(self.num_samples, generator=self.generator)\r\n",
        "        \r\n",
        "    @property\r\n",
        "    def num_samples(self) -> int:\r\n",
        "        return len(self.data_source)\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        if self.perm_index >= len(self.perm):\r\n",
        "            self.perm_index = 0\r\n",
        "            self.perm = torch.randperm(self.num_samples, generator=self.generator)\r\n",
        "            \r\n",
        "        while self.perm_index < len(self.perm):\r\n",
        "            self.perm_index += 1\r\n",
        "            yield self.perm[self.perm_index-1]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.num_samples\r\n",
        "    \r\n",
        "    def get_state(self):\r\n",
        "        return {\"perm\": self.perm, \"perm_index\": self.perm_index, \"generator_state\": self.generator.get_state()}\r\n",
        "    \r\n",
        "    def set_state(self, state):\r\n",
        "        self.perm = state[\"perm\"]\r\n",
        "        self.perm_index = state[\"perm_index\"]\r\n",
        "        self.generator.set_state(state[\"generator_state\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0phLTHEsL0P"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pyquaternion import Quaternion\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def pad_to_square(img, pad_value):\n",
        "    c, h, w = img.shape\n",
        "    dim_diff = np.abs(h - w)\n",
        "    # (upper / left) padding and (lower / right) padding\n",
        "    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
        "    # Determine padding\n",
        "    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n",
        "    # Add padding\n",
        "    img = F.pad(img, pad, \"constant\", value=pad_value)\n",
        "\n",
        "    return img, pad\n",
        "\n",
        "\n",
        "def resize(image, size):\n",
        "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_resize(images, min_size=288, max_size=448):\n",
        "    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n",
        "    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n",
        "    return images\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, folder_path, img_size=1248):\n",
        "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[index % len(self.files)]\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path))\n",
        "        # Pad to square resolution\n",
        "        img, _ = pad_to_square(img, 0)\n",
        "        # Resize\n",
        "        img = resize(img, self.img_size)\n",
        "\n",
        "        return img_path, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, sample_mapping, img_size=1248, augment=True, multiscale=False, normalized_labels=False, max_height=5, max_length=35, max_width=35):\n",
        "       \n",
        "        self.img_size = img_size\n",
        "        self.max_objects = 100\n",
        "        self.augment = augment\n",
        "        self.multiscale = multiscale\n",
        "        self.normalized_labels = normalized_labels\n",
        "        self.min_size = self.img_size - 3 * 32\n",
        "        self.max_size = self.img_size + 3 * 32\n",
        "        self.batch_count = 0\n",
        "        self.sample_mapping = sample_mapping\n",
        "        self.file_path = '/content/drive/MyDrive/data'\n",
        "        \n",
        "        #sample token : folder no\n",
        "        self.hash = {}\n",
        "        for i in range(85):\n",
        "          with open(\"/content/drive/MyDrive/data/sample\"+str(i)+\"/sample.json\") as f:\n",
        "            samples = json.load(f)\n",
        "          for sample in samples.keys():\n",
        "            self.hash[sample] = i\n",
        "          f.close()\n",
        "\n",
        "        #get categories\n",
        "        self.categories = []\n",
        "        with open('/content/drive/MyDrive/data/v1.0-trainval/category.json') as f:\n",
        "          data = json.load(f)\n",
        "        for d in data:\n",
        "          self.categories.append(d['name'])\n",
        "        self.num_category = len(self.categories)\n",
        "        \n",
        "        #important items for tweking\n",
        "        self.max_height = max_height\n",
        "        self.max_width = max_width\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def quaternion_yaw(self, q: Quaternion) -> float:\n",
        "      \"\"\"\n",
        "      Calculate the yaw angle from a quaternion.\n",
        "      See https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles.\n",
        "      :param q: Quaternion of interest.\n",
        "      :return: Yaw angle in radians.\n",
        "      \"\"\"\n",
        "\n",
        "      a = 2.0 * (q[0] * q[3] + q[1] * q[2])\n",
        "      b = 1.0 - 2.0 * (q[2] ** 2 + q[3] ** 2)\n",
        "\n",
        "      return np.arctan2(a, b)\n",
        "\n",
        "    def rotate_around_point_lowperf(self, point, radians, origin=(0, 0)):\n",
        "      \"\"\"Rotate a point around a given point.\n",
        "      \n",
        "      I call this the \"low performance\" version since it's recalculating\n",
        "      the same values more than once [cos(radians), sin(radians), x-ox, y-oy).\n",
        "      It's more readable than the next function, though.\n",
        "      \"\"\"\n",
        "      x, y = point\n",
        "      ox, oy = origin\n",
        "\n",
        "      qx = ox + math.cos(radians) * (x - ox) + math.sin(radians) * (y - oy)\n",
        "      qy = oy + -math.sin(radians) * (x - ox) + math.cos(radians) * (y - oy)\n",
        "\n",
        "      return qx, qy\n",
        "\n",
        "    def convert_to_top_corner(self, point):\n",
        "      \"\"\"Convert the position with respect to the top left corner\"\"\"\n",
        "      point[0] = self.max_length + point[0]\n",
        "      point[1] = self.max_width - point[1]\n",
        "      return point\n",
        "\n",
        "    def check_cameraregion(self,coordinates,cameras,sample,folder_no,verbose = False):\n",
        "      \"\"\"To check if the coordinate of the object lies in blacked out camera region\n",
        "       and if so return flag as 0\"\"\"\n",
        "      if verbose:\n",
        "        print(\"Start of Checking for the point :\",coordinates)\n",
        "      angle = 0\n",
        "      flag = False\n",
        "      self.calibrated_sensor = {}\n",
        "      with open(\"/content/drive/MyDrive/data/sample\"+str(folder_no)+\"/calibrated_sensor.json\") as f:\n",
        "          self.calibrated_sensor = json.load(f)\n",
        "      f.close()\n",
        "      for camera in cameras:\n",
        "        flag = False\n",
        "        sample_data = self.sample_data[sample['data'][camera]]\n",
        "        sensor = self.calibrated_sensor[sample_data['calibrated_sensor_token']]\n",
        "        if camera == 'CAM_BACK':\n",
        "          angle = 55/180 * math.pi\n",
        "        else :\n",
        "          angle = 35/180 * math.pi\n",
        "        if verbose:\n",
        "          print(\"Sensor before rotation:\", sensor['translation'][0:2])\n",
        "        x, y = self.rotate_around_point_lowperf(sensor['translation'][0:2], -math.pi/2)\n",
        "        if verbose:\n",
        "          print(\"Sensor after rotation:\", x,y)\n",
        "        rotation = self.quaternion_yaw(sensor['rotation']) +math.pi\n",
        "        vl = coordinates[1] - y - math.tan(rotation+angle) * (coordinates[0]-x)\n",
        "        vr = coordinates[1] - y - math.tan(rotation-angle) * (coordinates[0]-x)\n",
        "        if verbose:\n",
        "          print(camera,\"Angles :\",(rotation+angle)*180/math.pi,(rotation-angle)*180/math.pi)\n",
        "        if (rotation + angle >= math.pi/2 and rotation + angle <= math.pi*3/2):\n",
        "          if vl >= 0:\n",
        "            flag = True\n",
        "        else:\n",
        "          if vl <= 0:\n",
        "            flag = True\n",
        "        if flag:\n",
        "          if (rotation - angle >= math.pi/2 and rotation - angle <= math.pi*3/2):\n",
        "            if vr <= 0:\n",
        "              flag = True\n",
        "              break\n",
        "            else:\n",
        "              flag = False\n",
        "          else:\n",
        "            if vr >= 0:\n",
        "              flag = True\n",
        "              break\n",
        "            else:\n",
        "              flag = False\n",
        "      del self.calibrated_sensor\n",
        "      return flag\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        t1 = time.time()\n",
        "        print(\"Inside DATA Loader :\",t1)\n",
        "        #finding the annotation and image id and loading the corresponding files\n",
        "        token = self.sample_mapping[index]\n",
        "        folder_no = self.hash[token]\n",
        "        self.sample = {}\n",
        "        with open(\"/content/drive/MyDrive/data/sample\"+str(folder_no)+\"/sample.json\") as f:\n",
        "            self.sample = json.load(f)\n",
        "        f.close()\n",
        "        self.sample_data = {}\n",
        "        with open(\"/content/drive/MyDrive/data/sample\"+str(folder_no)+\"/sample_data.json\") as f:\n",
        "            self.sample_data = json.load(f)\n",
        "        f.close()\n",
        "        self.ego_poses = {}\n",
        "        with open(\"/content/drive/MyDrive/data/sample\"+str(folder_no)+\"/ego_pose.json\") as f:\n",
        "            self.ego_poses = json.load(f)\n",
        "        f.close()\n",
        "        self.visibility = {}\n",
        "        with open(\"/content/drive/MyDrive/data/sample\"+str(folder_no)+\"/visibility.json\") as f:\n",
        "            self.visibility = json.load(f)\n",
        "        f.close()\n",
        "        self.sample_annotation = {}\n",
        "        with open(\"/content/drive/MyDrive/data/sample\"+str(folder_no)+\"/sample_annotation.json\") as f:\n",
        "            self.sample_annotation = json.load(f)\n",
        "        f.close()\n",
        "        my_sample = self.sample[token]\n",
        "        \n",
        "        #Augmentation is mixed in with the camera and annotation loading.\n",
        "        t2 = t1\n",
        "        t1 = time.time()\n",
        "        print(\"Time taken to open the json files :\",t1-t2)\n",
        "        # ---------\n",
        "        #  Image\n",
        "        # ---------\n",
        "\n",
        "        front = ['CAM_FRONT_LEFT','CAM_FRONT','CAM_FRONT_RIGHT']\n",
        "        back = ['CAM_BACK_RIGHT','CAM_BACK','CAM_BACK_LEFT']\n",
        "\n",
        "        camera = []\n",
        "        blackout_cameras = []\n",
        "\n",
        "        #Augmentation using camera blackout\n",
        "        if self.augment:\n",
        "          if np.random.random() < 0.3:\n",
        "            camera = front + back\n",
        "            numbers = [1,2,3,4,5]\n",
        "            number = random.choice(numbers)\n",
        "            blackout_cameras = random.sample(camera,number)\n",
        "\n",
        "        #Camera image stitching and applying blackout from the selected cameras\n",
        "        for f,b in zip(front,back):\n",
        "          sensorf = self.sample_data[my_sample['data'][f]]\n",
        "          sensorb = self.sample_data[my_sample['data'][b]]\n",
        "\n",
        "          if f == 'CAM_FRONT_LEFT' and b == 'CAM_BACK_RIGHT':\n",
        "            image_dataf = transforms.ToTensor()(Image.open(self.file_path +'/sample'+str(folder_no)+'/'+ sensorf['filename'].split('/')[1]+'/'+sensorf['filename'].split('/')[2]).convert('RGB'))\n",
        "            if 'CAM_FRONT_LEFT' in blackout_cameras:\n",
        "              image_dataf = torch.zeros(image_dataf.shape)\n",
        "            image_datab = transforms.ToTensor()(Image.open(self.file_path +'/sample'+str(folder_no)+'/'+ sensorb['filename'].split('/')[1]+'/'+sensorb['filename'].split('/')[2]).convert('RGB'))\n",
        "            if 'CAM_BACK_RIGHT' in blackout_cameras:\n",
        "              image_datab = torch.zeros(image_dataf.shape)\n",
        "          else:\n",
        "            data = transforms.ToTensor()(Image.open(self.file_path +'/sample'+str(folder_no)+'/'+ sensorf['filename'].split('/')[1]+'/'+sensorf['filename'].split('/')[2]).convert('RGB'))\n",
        "            if f in blackout_cameras:\n",
        "              data = torch.zeros(data.shape)\n",
        "            image_dataf = torch.cat((image_dataf,data),2)\n",
        "            data = transforms.ToTensor()(Image.open(self.file_path +'/sample'+str(folder_no)+'/'+ sensorb['filename'].split('/')[1]+'/'+sensorb['filename'].split('/')[2]).convert('RGB'))\n",
        "            if b in blackout_cameras:\n",
        "              data = torch.zeros(data.shape)\n",
        "            image_datab = torch.cat((image_datab,data),2)\n",
        "        #fliping the bottem image for more consistency\n",
        "        image_datab = torch.flip(image_datab, [-1])\n",
        "        #concatinating the top and bottem image\n",
        "        image_data = torch.cat((image_dataf,image_datab),1)\n",
        "        #resizeing the image to square\n",
        "        image_data, _ = pad_to_square(image_data,0)\n",
        "\n",
        "        t2 = t1\n",
        "        t1 = time.time()\n",
        "        print(\"Time taken to prepare the image files :\",t1-t2)\n",
        "        # ---------\n",
        "        #  Label\n",
        "        # ---------\n",
        "\n",
        "        l_factor, w_factor, h_factor = (self.max_length, self.max_width, self.max_height) if self.normalized_labels else (1, 1, 1)\n",
        "        targets = None\n",
        "\n",
        "\n",
        "        annos_list = my_sample['anns']\n",
        "        converted_anotations = []\n",
        "        #ego pose\n",
        "        sensor = 'LIDAR_TOP'\n",
        "        lidar_top_data = self.sample_data[my_sample['data'][sensor]]\n",
        "        ego_pose = self.ego_poses[lidar_top_data['ego_pose_token']]\n",
        "        ego_yaw = self.quaternion_yaw(ego_pose['rotation']) - math.pi/2\n",
        "\n",
        "        boxes = []\n",
        "        t=[]\n",
        "        original_ego_yaw = ego_yaw + math.pi/2 #converting back to original value\n",
        "\n",
        "        for annos in annos_list:\n",
        "          annotation = self.sample_annotation[annos]\n",
        "          vis = self.visibility[annotation['visibility_token']]\n",
        "          vis = vis[1:].split(\"-\")\n",
        "          if int(vis[1]) <= 60 :\n",
        "            continue\n",
        "          #print(vis)\n",
        "          box = []\n",
        "\n",
        "          #xyz\n",
        "          flag = False\n",
        "          cordinates = [annotation['translation'][i] - ego_pose['translation'][i] for i in range(3)]\n",
        "          cordinates[0], cordinates[1] = self.rotate_around_point_lowperf(cordinates[:2], ego_yaw, origin=(0, 0))\n",
        "          if self.augment :\n",
        "            flag = self.check_cameraregion(cordinates,blackout_cameras,my_sample, folder_no)\n",
        "          cordinates = self.convert_to_top_corner(cordinates)\n",
        "          if cordinates[0] > 2*self.max_width or cordinates[0] < 0 or cordinates[1] > 2*self.max_length or cordinates[1] < 0 or flag:# or (self.augment and self.check_cameraregion() == 0):\n",
        "            continue\n",
        "\n",
        "          #whl\n",
        "          size = annotation['size']\n",
        "\n",
        "          #angle r1, r2\n",
        "            #converting to relative angle (0-360)\n",
        "          rotation_yaw = self.quaternion_yaw(annotation['rotation']) - original_ego_yaw\n",
        "          if rotation_yaw < 0:\n",
        "            rotation_yaw += math.pi*2\n",
        "          r1 = (1 + math.sin(rotation_yaw))/2\n",
        "          r2 = (1 + math.cos(rotation_yaw))/2\n",
        "\n",
        "          #category\n",
        "          category_index = self.categories.index(annotation['category_name'])\n",
        "\n",
        "          #Appending to Box\n",
        "          box.append(category_index)\n",
        "          for i,j in zip(cordinates, [self.max_width*2, self.max_length*2, self.max_height]):\n",
        "            box.append(i/j)\n",
        "          for i,j in zip(size, [self.max_width*2, self.max_length*2, self.max_height]):\n",
        "            box.append(i/j)\n",
        "          box.append(r1)\n",
        "          box.append(r2)\n",
        "\n",
        "          #Appending to Boxes\n",
        "          boxes.append(box)\n",
        "          t.append(annos)\n",
        "\n",
        "        boxes = torch.Tensor(boxes)\n",
        "\n",
        "        targets = torch.zeros((len(boxes), 10))\n",
        "        if len(boxes)> 0:\n",
        "          targets[:, 1:] = boxes\n",
        "\n",
        "        del self.sample, self.sample_data, self.sample_annotation, self.visibility, self.ego_poses\n",
        "        # Apply augmentations\n",
        "        #if self.augment:\n",
        "        #  image_data, targets = horisontal_flip(image_data, targets, Verbose = True)\n",
        "        #    if np.random.random() < 0.5:\n",
        "        t2 = t1\n",
        "        t1 = time.time()\n",
        "        print(\"Time taken to make labels :\",t1-t2)\n",
        "        print(\"End of dataloading CLASS : \",time.time())\n",
        "        return image_data, targets\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        imgs, targets = list(zip(*batch))\n",
        "        # Remove empty placeholder targets\n",
        "        targets = [boxes for boxes in targets if boxes is not None]\n",
        "        # Add sample index to targets\n",
        "        for i, boxes in enumerate(targets):\n",
        "            targets[i][:, 0] = i\n",
        "        targets = torch.cat(targets, 0)\n",
        "        \"\"\"\n",
        "        # Selects new image size every tenth batch\n",
        "        if self.multiscale and self.batch_count % 10 == 0:\n",
        "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
        "        \"\"\"\n",
        "        # Resize images to input shape\n",
        "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
        "        self.batch_count += 1\n",
        "        print(\"Finished colate function : \",time.time())\n",
        "        return imgs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_mapping)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOKpxjoY8_8i"
      },
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "#i = ListDataset(train_samples)\n",
        "#img, targets = i.__getitem__(2)\n",
        "#plt.imshow(  img.permute(1, 2, 0)  )\n",
        "#print(targets)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LklsSzDNP6s8"
      },
      "source": [
        "#img, targets = i.__getitem__(1)\n",
        "#plt.imshow(  img.permute(1, 2, 0)  )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takpM5RvrupZ"
      },
      "source": [
        "#MODEL.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDUlseGjmRJ"
      },
      "source": [
        "#MODEL\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "def create_modules(module_defs):\n",
        "    \"\"\"\n",
        "    Constructs module list of layer blocks from module configuration in module_defs\n",
        "    \"\"\"\n",
        "    hyperparams = module_defs.pop(0)\n",
        "    output_filters = [int(hyperparams[\"channels\"])]\n",
        "    module_list = nn.ModuleList()\n",
        "    for module_i, module_def in enumerate(module_defs):\n",
        "        modules = nn.Sequential()\n",
        "\n",
        "        if module_def[\"type\"] == \"convolutional\":\n",
        "            bn = int(module_def[\"batch_normalize\"])\n",
        "            filters = int(module_def[\"filters\"])\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            pad = (kernel_size - 1) // 2\n",
        "            modules.add_module(\n",
        "                f\"conv_{module_i}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=output_filters[-1],\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=int(module_def[\"stride\"]),\n",
        "                    padding=pad,\n",
        "                    bias=not bn,\n",
        "                ),\n",
        "            )\n",
        "            if bn:\n",
        "                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n",
        "            if module_def[\"activation\"] == \"leaky\":\n",
        "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
        "\n",
        "        elif module_def[\"type\"] == \"maxpool\":\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            stride = int(module_def[\"stride\"])\n",
        "            if kernel_size == 2 and stride == 1:\n",
        "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n",
        "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
        "\n",
        "        elif module_def[\"type\"] == \"upsample\":\n",
        "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
        "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
        "\n",
        "        elif module_def[\"type\"] == \"route\":\n",
        "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
        "            filters = sum([output_filters[1:][i] for i in layers])\n",
        "            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"shortcut\":\n",
        "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
        "            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"yolo\":\n",
        "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
        "            # Extract anchors\n",
        "            anchors = [float(x) for x in module_def[\"anchors\"].split(\",\")]\n",
        "            anchors = [(anchors[i], anchors[i + 1], anchors[i + 2]) for i in range(0, len(anchors), 3)]\n",
        "            anchors = [anchors[i] for i in anchor_idxs]\n",
        "            num_classes = int(module_def[\"classes\"])\n",
        "            img_size = int(hyperparams[\"height\"])\n",
        "            # Define detection layer\n",
        "            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n",
        "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
        "        # Register module list and number of output filters\n",
        "        module_list.append(modules)\n",
        "        output_filters.append(filters)\n",
        "\n",
        "    return hyperparams, module_list\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor, mode=\"nearest\"):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmptyLayer(nn.Module):\n",
        "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EmptyLayer, self).__init__()\n",
        "\n",
        "\n",
        "class YOLOLayer(nn.Module):\n",
        "    \"\"\"Detection layer\"\"\"\n",
        "\n",
        "    def __init__(self, anchors, num_classes, img_dim=1248):\n",
        "        super(YOLOLayer, self).__init__()\n",
        "        self.anchors = anchors\n",
        "        self.num_anchors = len(anchors)\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_thres = 0.5\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        self.obj_scale = 1\n",
        "        self.noobj_scale = 100\n",
        "        self.metrics = {}\n",
        "        self.img_dim = img_dim\n",
        "        self.grid_size = 0  # grid size\n",
        "\n",
        "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
        "        self.grid_size = grid_size\n",
        "        g = self.grid_size\n",
        "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "        self.stride = self.img_dim / self.grid_size\n",
        "        # Calculate offsets for each grid\n",
        "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_l / self.stride, a_h ) for a_w, a_l, a_h in self.anchors])\n",
        "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_l = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_h = self.scaled_anchors[:, 2:3].view((1, self.num_anchors, 1, 1))\n",
        "\n",
        "    def forward(self, x, targets=None, img_dim=None):\n",
        "\n",
        "        # Tensors for cuda support\n",
        "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
        "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
        "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
        "\n",
        "        self.img_dim = img_dim\n",
        "        num_samples = x.size(0)\n",
        "        grid_size = x.size(2)\n",
        "        prediction = (\n",
        "            x.view(num_samples, self.num_anchors, self.num_classes + 9, grid_size, grid_size)\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "        # Get outputs\n",
        "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
        "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
        "        z = torch.sigmoid(prediction[..., 2])  # Center z\n",
        "        l = prediction[..., 3]  # Length\n",
        "        w = prediction[..., 4]  # Width\n",
        "        h = prediction[..., 5]  # Height\n",
        "        r1 = torch.sigmoid(prediction[..., 6])  # Rotation r1\n",
        "        r2 = torch.sigmoid(prediction[..., 7])  # Rotation r2\n",
        "        pred_conf = torch.sigmoid(prediction[..., 8])  # Conf\n",
        "        pred_cls = torch.sigmoid(prediction[..., 9:])  # Cls pred.\n",
        "        \n",
        "        # If grid size does not match current we compute new offsets\n",
        "        if grid_size != self.grid_size:\n",
        "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
        "        \n",
        "        # Add offset and scale with anchors\n",
        "        pred_boxes = FloatTensor(prediction[..., :8].shape)\n",
        "        pred_boxes[..., 0] = x.data + self.grid_x\n",
        "        pred_boxes[..., 1] = y.data + self.grid_y\n",
        "        pred_boxes[..., 2] = z.data\n",
        "        pred_boxes[..., 3] = torch.exp(w.data) * self.anchor_w\n",
        "        pred_boxes[..., 4] = torch.exp(l.data) * self.anchor_l\n",
        "        pred_boxes[..., 5] = torch.exp(h.data) * self.anchor_h\n",
        "        pred_boxes[..., 6] = r1.data\n",
        "        pred_boxes[..., 7] = r2.data\n",
        "\n",
        "        replacement = pred_boxes.view(num_samples, -1, 8)\n",
        "        replacement[...,:2] = replacement[...,:2] * self.stride\n",
        "        replacement[...,2] = replacement[...,2] * 5\n",
        "        replacement[...,3:5] = replacement[...,3:5] * self.stride\n",
        "        replacement[...,5] = replacement[...,5]\n",
        "        replacement[...,6] = replacement[...,6]\n",
        "        replacement[...,7] = replacement[...,7]\n",
        "\n",
        "\n",
        "        output = torch.cat(\n",
        "            (\n",
        "                replacement,\n",
        "                pred_conf.view(num_samples, -1, 1),\n",
        "                pred_cls.view(num_samples, -1, self.num_classes),\n",
        "            ),\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "        if targets is None:\n",
        "            return output, 0\n",
        "        else:\n",
        "            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf = build_targets(\n",
        "                pred_boxes=pred_boxes,\n",
        "                pred_cls=pred_cls,\n",
        "                target=targets,\n",
        "                anchors=self.scaled_anchors,\n",
        "                ignore_thres=self.ignore_thres,\n",
        "            )\n",
        "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
        "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
        "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
        "            loss_z = self.mse_loss(z[obj_mask], tz[obj_mask])\n",
        "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
        "            loss_l = self.mse_loss(l[obj_mask], tl[obj_mask])\n",
        "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
        "            loss_r1 = self.mse_loss(r1[obj_mask], tr1[obj_mask])\n",
        "            loss_r2 = self.mse_loss(r2[obj_mask], tr2[obj_mask])\n",
        "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
        "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
        "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
        "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
        "            total_loss = loss_x + loss_y + loss_z + loss_w + loss_l + loss_h + loss_r1 + loss_r2 + loss_conf + loss_cls\n",
        "\n",
        "            # Metrics\n",
        "            cls_acc = 100 * class_mask[obj_mask].mean()\n",
        "            conf_obj = pred_conf[obj_mask].mean()\n",
        "            conf_noobj = pred_conf[noobj_mask].mean()\n",
        "            conf50 = (pred_conf > 0.5).float()\n",
        "            iou50 = (iou_scores > 0.5).float()\n",
        "            iou75 = (iou_scores > 0.75).float()\n",
        "            detected_mask = conf50 * class_mask * tconf\n",
        "            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n",
        "            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
        "            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n",
        "\n",
        "            self.metrics = {\n",
        "                \"loss\": to_cpu(total_loss).item(),\n",
        "                \"x\": to_cpu(loss_x).item(),\n",
        "                \"y\": to_cpu(loss_y).item(),\n",
        "                \"z\": to_cpu(loss_z).item(),\n",
        "                \"w\": to_cpu(loss_w).item(),\n",
        "                \"l\": to_cpu(loss_l).item(),\n",
        "                \"h\": to_cpu(loss_h).item(),\n",
        "                \"r1\": to_cpu(loss_r1).item(),\n",
        "                \"r2\": to_cpu(loss_r2).item(),\n",
        "                \"conf\": to_cpu(loss_conf).item(),\n",
        "                \"cls\": to_cpu(loss_cls).item(),\n",
        "                \"cls_acc\": to_cpu(cls_acc).item(),\n",
        "                \"recall50\": to_cpu(recall50).item(),\n",
        "                \"recall75\": to_cpu(recall75).item(),\n",
        "                \"precision\": to_cpu(precision).item(),\n",
        "                \"conf_obj\": to_cpu(conf_obj).item(),\n",
        "                \"conf_noobj\": to_cpu(conf_noobj).item(),\n",
        "                \"grid_size\": grid_size,\n",
        "            }\n",
        "\n",
        "            return output, total_loss\n",
        "\n",
        "\n",
        "class Darknet(nn.Module):\n",
        "    \"\"\"YOLOv3 object detection model\"\"\"\n",
        "\n",
        "    def __init__(self, config_path, img_size=1248):\n",
        "        super(Darknet, self).__init__()\n",
        "        self.module_defs = parse_model_config(config_path)\n",
        "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
        "        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n",
        "        self.img_size = img_size\n",
        "        self.seen = 0\n",
        "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        img_dim = x.shape[2]\n",
        "        loss = 0\n",
        "        layer_outputs, yolo_outputs = [], []\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
        "                x = module(x)\n",
        "            elif module_def[\"type\"] == \"route\":\n",
        "                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
        "            elif module_def[\"type\"] == \"shortcut\":\n",
        "                layer_i = int(module_def[\"from\"])\n",
        "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
        "            elif module_def[\"type\"] == \"yolo\":\n",
        "                x, layer_loss = module[0](x, targets, img_dim)\n",
        "                loss += layer_loss\n",
        "                yolo_outputs.append(x)\n",
        "            layer_outputs.append(x)\n",
        "        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n",
        "        return yolo_outputs if targets is None else (loss, yolo_outputs)\n",
        "\n",
        "    def load_darknet_weights(self, weights_path):\n",
        "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
        "\n",
        "        # Open the weights file\n",
        "        with open(weights_path, \"rb\") as f:\n",
        "            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n",
        "            self.header_info = header  # Needed to write header when saving weights\n",
        "            self.seen = header[3]  # number of images seen during training\n",
        "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
        "\n",
        "        # Establish cutoff for loading backbone weights\n",
        "        cutoff = None\n",
        "        if \"darknet53.conv.74\" in weights_path:\n",
        "            cutoff = 75\n",
        "\n",
        "        ptr = 0\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if i == cutoff:\n",
        "                break\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    # Load BN bias, weights, running mean and running variance\n",
        "                    bn_layer = module[1]\n",
        "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
        "                    # Bias\n",
        "                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
        "                    bn_layer.bias.data.copy_(bn_b)\n",
        "                    ptr += num_b\n",
        "                    # Weight\n",
        "                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
        "                    bn_layer.weight.data.copy_(bn_w)\n",
        "                    ptr += num_b\n",
        "                    # Running Mean\n",
        "                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
        "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
        "                    ptr += num_b\n",
        "                    # Running Var\n",
        "                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
        "                    bn_layer.running_var.data.copy_(bn_rv)\n",
        "                    ptr += num_b\n",
        "                else:\n",
        "                    # Load conv. bias\n",
        "                    num_b = conv_layer.bias.numel()\n",
        "                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
        "                    conv_layer.bias.data.copy_(conv_b)\n",
        "                    ptr += num_b\n",
        "                # Load conv. weights\n",
        "                num_w = conv_layer.weight.numel()\n",
        "                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
        "                conv_layer.weight.data.copy_(conv_w)\n",
        "                ptr += num_w\n",
        "\n",
        "    def save_darknet_weights(self, path, cutoff=-1):\n",
        "        \"\"\"\n",
        "            @:param path    - path of the new weights file\n",
        "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
        "        \"\"\"\n",
        "        fp = open(path, \"wb\")\n",
        "        self.header_info[3] = self.seen\n",
        "        self.header_info.tofile(fp)\n",
        "\n",
        "        # Iterate through layers\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                # If batch norm, load bn first\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    bn_layer = module[1]\n",
        "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv bias\n",
        "                else:\n",
        "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv weights\n",
        "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "\n",
        "        fp.close()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51RsFcXEMHk"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUGbtD3-FLvV"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def evaluate(model, data_samples, iou_thres, conf_thres, nms_thres, img_size, batch_size):\n",
        "    model.eval()\n",
        "\n",
        "    # Get dataloader\n",
        "    dataset = ListDataset(data_samples, img_size=img_size, augment=False, multiscale=False)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "    labels = []\n",
        "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
        "    for batch_i, (imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=\"Detecting objects\")):\n",
        "\n",
        "        # Extract labels\n",
        "        labels += targets[:, 1].tolist()\n",
        "        # Rescale target\n",
        "        targets[:, 2:8] = xywh2xyxy(targets[:, 2:8])\n",
        "        targets[:, 2:8] *= img_size\n",
        "\n",
        "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n",
        "\n",
        "        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n",
        "        #print(sample_metrics)\n",
        "        del imgs,targets,outputs\n",
        "        torch.cuda.empty_cache()\n",
        "    # Concatenate sample statistics\n",
        "    if not sample_metrics :\n",
        "      precision, recall, AP, f1, ap_class = np.asarray([0]),np.asarray([0]),np.asarray([0]),np.asarray([0]),[]\n",
        "    else :\n",
        "      true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
        "      precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
        "\n",
        "    return precision, recall, AP, f1, ap_class"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-CRslW0Xujc"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0zm8fD5ISjg"
      },
      "source": [
        "class parameters:\n",
        "  def __init__(self, pretrained_weights = \"/content/drive/MyDrive/SingleImgDepthYolo/weights/darknet53.conv.74\", epochs = 100, batch_size = 8, step_size = 4, model_def = \"/content/drive/MyDrive/SingleImgDepthYolo/config/yolov3custom.cfg\", n_cpu = 8, img_size = 416, checkpoint_interval = 1, evaluation_interval = 5, compute_map = False, multiscale_training = True):#, data_config = \"config/coco.data\"):\n",
        "    self.epochs = epochs #number of epochs\n",
        "    self.batch_size = batch_size #size of each image batch\n",
        "    self.gradient_accumulations =step_size    #number of gradient accums before step\n",
        "    self.model_def = model_def #path to model definition file\n",
        "    #self.data_config = data_config #path to data config file`\n",
        "    self.pretrained_weights = pretrained_weights #if specified starts from checkpoint model\n",
        "    self.n_cpu = n_cpu #number of cpu threads to use during batch generation\n",
        "    self.img_size = img_size #size of each image dimension\n",
        "    self.checkpoint_interval = checkpoint_interval #interval between saving model weights\n",
        "    self.evaluation_interval = evaluation_interval #interval evaluations on validation set\n",
        "    self.compute_map = compute_map #if True computes mAP every tenth batch\n",
        "    self.multiscale_training = multiscale_training #allow for multi-scale training\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oI0p5QnYyK2"
      },
      "source": [
        "opt = parameters(epochs = 3, pretrained_weights = \"/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/full_data_drive_short-1.pth\",img_size = 1248,batch_size = 2,evaluation_interval = 10,checkpoint_interval=50,step_size=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPTzU1mCavSJ",
        "outputId": "27f232c4-7e9b-4f7d-b0d4-83717593ba71"
      },
      "source": [
        "model = Darknet(opt.model_def).to(device)\n",
        "model.apply(weights_init_normal) #applying initial weights to the model. Since it is using module apply recursively goes through each layer and initializes the specified weights weights_init_normal is a function in utils\n",
        "# If specified we start from checkpoint\n",
        "if opt.pretrained_weights:\n",
        "  if opt.pretrained_weights.endswith(\".pth\"):\n",
        "    model.load_state_dict(torch.load(opt.pretrained_weights))\n",
        "    print(\"Model LOADING Finished !!!\")\n",
        "  else:\n",
        "    model.load_darknet_weights(opt.pretrained_weights) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model LOADING Finished !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIGSAkmDK3af",
        "outputId": "5c35702e-5874-4051-9691-a535a525f9fa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "t1 = time.time()\n",
        "sample_mapping = []\n",
        "for i in range(85):\n",
        "  with open(\"/content/drive/MyDrive/data/sample\"+str(i)+\"/sample.json\") as f:\n",
        "    samples = json.load(f)\n",
        "    for sample_token in samples.keys():\n",
        "      sample_mapping.append(sample_token)\n",
        "  t2 = t1\n",
        "  t1 = time.time()\n",
        "  print(\"Loop time :\",t1-t2)\n",
        "\n",
        "train_samples, test_samples = train_test_split(sample_mapping, test_size=0.2, random_state=42)\n",
        "print(len(train_samples),len(test_samples))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop time : 0.7430083751678467\n",
            "Loop time : 0.689805269241333\n",
            "Loop time : 1.6217267513275146\n",
            "Loop time : 0.7403721809387207\n",
            "Loop time : 0.7077381610870361\n",
            "Loop time : 0.7581527233123779\n",
            "Loop time : 0.6940150260925293\n",
            "Loop time : 1.3219966888427734\n",
            "Loop time : 0.7285408973693848\n",
            "Loop time : 0.8154399394989014\n",
            "Loop time : 0.7321743965148926\n",
            "Loop time : 0.7574620246887207\n",
            "Loop time : 0.9641721248626709\n",
            "Loop time : 0.7219796180725098\n",
            "Loop time : 0.8346452713012695\n",
            "Loop time : 0.6783878803253174\n",
            "Loop time : 0.7604658603668213\n",
            "Loop time : 0.9617781639099121\n",
            "Loop time : 2.601804494857788\n",
            "Loop time : 2.1841888427734375\n",
            "Loop time : 1.1216273307800293\n",
            "Loop time : 0.7618832588195801\n",
            "Loop time : 0.8730418682098389\n",
            "Loop time : 0.8167538642883301\n",
            "Loop time : 0.7161209583282471\n",
            "Loop time : 0.6617677211761475\n",
            "Loop time : 0.8085622787475586\n",
            "Loop time : 0.756169319152832\n",
            "Loop time : 0.7421016693115234\n",
            "Loop time : 0.8598659038543701\n",
            "Loop time : 0.8231449127197266\n",
            "Loop time : 0.7495584487915039\n",
            "Loop time : 1.2142624855041504\n",
            "Loop time : 0.79964280128479\n",
            "Loop time : 1.0499556064605713\n",
            "Loop time : 0.9875872135162354\n",
            "Loop time : 0.6245691776275635\n",
            "Loop time : 1.9192876815795898\n",
            "Loop time : 0.7492837905883789\n",
            "Loop time : 0.9263687133789062\n",
            "Loop time : 0.7353310585021973\n",
            "Loop time : 0.9886338710784912\n",
            "Loop time : 0.9305715560913086\n",
            "Loop time : 1.0774881839752197\n",
            "Loop time : 0.7415530681610107\n",
            "Loop time : 0.9836604595184326\n",
            "Loop time : 0.840064287185669\n",
            "Loop time : 0.7346673011779785\n",
            "Loop time : 1.6042613983154297\n",
            "Loop time : 0.7642936706542969\n",
            "Loop time : 0.734076976776123\n",
            "Loop time : 1.7834141254425049\n",
            "Loop time : 1.2666394710540771\n",
            "Loop time : 2.108121633529663\n",
            "Loop time : 1.3507816791534424\n",
            "Loop time : 1.3215065002441406\n",
            "Loop time : 0.7433199882507324\n",
            "Loop time : 1.1230485439300537\n",
            "Loop time : 1.0838017463684082\n",
            "Loop time : 1.1247024536132812\n",
            "Loop time : 1.2516045570373535\n",
            "Loop time : 1.5300381183624268\n",
            "Loop time : 1.4218926429748535\n",
            "Loop time : 0.7572531700134277\n",
            "Loop time : 1.8132045269012451\n",
            "Loop time : 1.957768440246582\n",
            "Loop time : 0.7663071155548096\n",
            "Loop time : 1.0171537399291992\n",
            "Loop time : 1.4360227584838867\n",
            "Loop time : 1.1396775245666504\n",
            "Loop time : 1.099510908126831\n",
            "Loop time : 0.8681342601776123\n",
            "Loop time : 1.5943412780761719\n",
            "Loop time : 0.8159801959991455\n",
            "Loop time : 1.8797667026519775\n",
            "Loop time : 0.7105498313903809\n",
            "Loop time : 1.913630723953247\n",
            "Loop time : 1.586151123046875\n",
            "Loop time : 1.0661823749542236\n",
            "Loop time : 1.663940668106079\n",
            "Loop time : 0.978736400604248\n",
            "Loop time : 0.7040901184082031\n",
            "Loop time : 1.5502309799194336\n",
            "Loop time : 1.8744032382965088\n",
            "Loop time : 1.9462721347808838\n",
            "27319 6830\n",
            "CPU times: user 873 ms, sys: 126 ms, total: 999 ms\n",
            "Wall time: 1min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-DPwj_UKGsU"
      },
      "source": [
        "sampler = ResumableRandomSampler(train_samples)\r\n",
        "sampler.set_state(torch.load(\"/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/sampler_state.pth\"))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tAWjHQVsV9D"
      },
      "source": [
        "dataset = ListDataset(train_samples,  augment=True, multiscale=opt.multiscale_training)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    #shuffle = True,\n",
        "    sampler = sampler,\n",
        "    batch_size=opt.batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=dataset.collate_fn,\n",
        ")\n",
        "class_names = dataset.categories"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdpDzAWZM0D"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr =0.001)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tGdtKinQ-ll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4393c29efccd450283c6b1d91aa962e6",
            "3f893cbaa30a46ddabc070fe68322b00",
            "3685483498b246dd988eeec445bada69",
            "312286c600e142dfa6e5d1787795f332",
            "1d6d5090e43049dbaa709791dc241a2c",
            "7ad365513b484a22ad8b8b52cfb6e7c1",
            "ad684af2a3304e14a658d7975284aa0e",
            "2459bfcdc8b94865bc8d3975d2892511"
          ]
        },
        "outputId": "9fe5537c-5719-40a6-e754-f8c86c0e4aa1"
      },
      "source": [
        "import time\n",
        "import tqdm\n",
        "import json\n",
        "\n",
        "#\n",
        "if os.path.exists(\"/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/step-short.json\"):\n",
        "  with open('/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/step-short.json') as f:\n",
        "    data = json.load(f)\n",
        "  step = data['step']\n",
        "else:\n",
        "  step = 0\n",
        "for epoch in tqdm.notebook.tqdm(range(opt.epochs),unit='epoch'):\n",
        "    model.train()\n",
        "    time2 = time.time()\n",
        "    print(\"Before loader\",time.time())\n",
        "    for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "        time1 = time.time()\n",
        "        print(\"Total data loading time :\", time2 - time1)\n",
        "        print(\"Back inside batch loop :\",time1)\n",
        "        batches_done = len(dataloader) * epoch + batch_i\n",
        "        \n",
        "        imgs = Variable(imgs.to(device))\n",
        "        targets = Variable(targets.to(device), requires_grad=False)\n",
        "\n",
        "        time2 = time.time()\n",
        "        print(\"Model excecution start:\",time2)\n",
        "        loss, outputs = model(imgs, targets)\n",
        "        time1 = time.time()\n",
        "        print(\"Model excecution time :\",time2-time1)\n",
        "        loss.backward()\n",
        "        time1 = time.time()\n",
        "        print(\"backpropagation time :\",time1-time2)\n",
        "        loss = loss.cpu().detach().numpy()\n",
        "\n",
        "        try:\n",
        "          print(\"Step :\",step,\" -- \",batch_i,\"Batch\",loss,\"Targets = \",targets.shape[0],\"Ratio =\",loss/targets.shape[0],\"Threshold (0.2):\", len(non_max_suppression(outputs,conf_thres = 0.2,nms_thres=0.5)[0]))\n",
        "        except:\n",
        "          print(\"Step :\",step,\" -- \",batch_i,\"Batch\",loss,\"Targets = \",targets.shape[0],\"Ratio =\",loss/targets.shape[0])\n",
        "          \n",
        "        if batches_done % opt.gradient_accumulations == 0 and batches_done != 0:\n",
        "            # Accumulates gradient before each step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            print(\"->\",optimizer.param_groups[0]['lr'])\n",
        "            step += 1\n",
        "            steps ={'step':step}\n",
        "            if step == 400000:\n",
        "              optimizer.param_groups[0]['lr'] = 0.0001\n",
        "            elif step ==  450000 :\n",
        "              optimizer.param_groups[0]['lr'] = 0.00001\n",
        "\n",
        "        \n",
        "        if batch_i % opt.checkpoint_interval == 0 and batch_i != 0:\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/full_data_drive_short-1.pth\")\n",
        "            with open(\"/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/step-short.json\",'w+') as f:\n",
        "              json.dump(steps,f)\n",
        "            print(\"Saved\")\n",
        "            try:\n",
        "                out = non_max_suppression(outputs,conf_thres = 0.3,nms_thres=0.5)\n",
        "                out[0][:,:2] /=1024\n",
        "                out[0][:,3:5] /=1024\n",
        "                draw(out[0])\n",
        "                print(\"threshold :\",0.3)\n",
        "            except:\n",
        "                try:\n",
        "                    out = non_max_suppression(outputs,conf_thres = 0.2,nms_thres=0.5)\n",
        "                    out[0][:,:2] /=1024\n",
        "                    out[0][:,3:5] /=1024\n",
        "                    draw(out[0])\n",
        "                    print(\"threshold :\",0.2)\n",
        "                except:\n",
        "                  print(\"Nothing !!!!\")\n",
        "\n",
        "        del imgs,targets,outputs,loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        torch.save(sampler.get_state(),\"/content/drive/MyDrive/SingleImgDepthYolo/checkpoints/sampler_state.pth\")\n",
        "        \n",
        "    if epoch % opt.evaluation_interval == 0 and opt.evaluation_interval != 0:\n",
        "        print(\"\\n---- Evaluating Model ----\")\n",
        "        # Evaluate the model on the validation set\n",
        "        precision, recall, AP, f1, ap_class = evaluate(\n",
        "            model,\n",
        "            test_samples,\n",
        "            iou_thres=0.5,\n",
        "            conf_thres=0.5,\n",
        "            nms_thres=0.5,\n",
        "            img_size=opt.img_size,\n",
        "            batch_size=2,\n",
        "        )\n",
        "        evaluation_metrics = [\n",
        "            (\"val_precision\", precision.mean()),\n",
        "            (\"val_recall\", recall.mean()),\n",
        "            (\"val_mAP\", AP.mean()),\n",
        "            (\"val_f1\", f1.mean()),\n",
        "        ]\n",
        "        #logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "        # Print class APs and mAP\n",
        "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "        for i, c in enumerate(ap_class):\n",
        "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "        print(AsciiTable(ap_table).table)\n",
        "        print(f\"---- mAP {AP.mean()}\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4393c29efccd450283c6b1d91aa962e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Before loader 1613737255.4744835\n",
            "Inside DATA Loader : 1613737255.4800022\n",
            "Time taken to open the json files : 3.5628159046173096\n",
            "Time taken to prepare the image files : 25.278982639312744\n",
            "Time taken to make labels : 2.067218542098999\n",
            "End of dataloading CLASS :  1613737286.3891594\n",
            "Inside DATA Loader : 1613737286.3931684\n",
            "Time taken to open the json files : 4.356966257095337\n",
            "Time taken to prepare the image files : 28.763410329818726\n",
            "Time taken to make labels : 2.073481321334839\n",
            "End of dataloading CLASS :  1613737321.5871687\n",
            "Finished colate function :  1613737321.7252822\n",
            "Total data loading time : -66.381418466568\n",
            "Back inside batch loop : 1613737321.8559005\n",
            "Model excecution start: 1613737321.8598237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:208: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:209: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:210: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:211: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:212: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:213: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:214: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:215: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:216: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:217: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:219: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:223: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:224: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:225: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model excecution time : -0.5118880271911621\n",
            "backpropagation time : 0.6188247203826904\n",
            "Step : 1962  --  0 Batch 18.538008 Targets =  62 Ratio = 0.29900012477751703\n",
            "Inside DATA Loader : 1613737323.0565412\n",
            "Time taken to open the json files : 6.000091791152954\n",
            "Time taken to prepare the image files : 19.42886519432068\n",
            "Time taken to make labels : 2.386303186416626\n",
            "End of dataloading CLASS :  1613737350.871938\n",
            "Inside DATA Loader : 1613737350.8735297\n",
            "Time taken to open the json files : 3.6304492950439453\n",
            "Time taken to prepare the image files : 15.288123369216919\n",
            "Time taken to make labels : 1.8469297885894775\n",
            "End of dataloading CLASS :  1613737371.6396904\n",
            "Finished colate function :  1613737371.7565098\n",
            "Total data loading time : -49.92871451377869\n",
            "Back inside batch loop : 1613737371.7885382\n",
            "Model excecution start: 1613737371.7926788\n",
            "Model excecution time : -0.2787306308746338\n",
            "backpropagation time : 0.358349084854126\n",
            "Step : 1962  --  1 Batch 18.062107 Targets =  10 Ratio = 1.806210708618164\n",
            "Inside DATA Loader : 1613737373.6942995\n",
            "Time taken to open the json files : 3.974522829055786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-99137599bdc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Before loader\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total data loading time :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-80d3f4107f27>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mimage_dataf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dataf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/sample'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0msensorb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msensorb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblackout_cameras\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m               \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc-2Uf6MlQiE"
      },
      "source": [
        "precision, recall, AP, f1, ap_class = evaluate(\n",
        "    model,\n",
        "    test_samples,\n",
        "    iou_thres=0.5,\n",
        "    conf_thres=0.5,\n",
        "    nms_thres=0.5, \n",
        "    img_size=opt.img_size,\n",
        "    batch_size=2,\n",
        ")\n",
        "evaluation_metrics = [\n",
        "    (\"val_precision\", precision.mean()),\n",
        "    (\"val_recall\", recall.mean()),\n",
        "    (\"val_mAP\", AP.mean()),\n",
        "    (\"val_f1\", f1.mean()),\n",
        "]\n",
        "#logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "# Print class APs and mAP\n",
        "ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "for i, c in enumerate(ap_class):\n",
        "    ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "print(AsciiTable(ap_table).table)\n",
        "print(f\"---- mAP {AP.mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eNn34W70xfj"
      },
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "  print(param_group['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCrpmVLrjGCn"
      },
      "source": [
        "torch.save(model.state_dict(), f\"checkpoint/yolov3_sample-6.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loApNq16LsjG"
      },
      "source": [
        "model.load_state_dict(torch.load(f\"checkpoint/yolov3_sample.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shkMUZFghdKy"
      },
      "source": [
        "img, tragets = next(iter(dataloader))\n",
        "img = Variable(img.to(device))\n",
        "model.eval()\n",
        "target = []\n",
        "for t in tragets:\n",
        "  print(t)\n",
        "  if t[0] == 0:\n",
        "    target.append(t[2:])\n",
        "out = model(img)\n",
        "out = non_max_suppression(out,conf_thres = -199999999000,nms_thres=-9999999999990.5)\n",
        "out[0][:,:2] /=1024\n",
        "out[0][:,3:5] /=1024\n",
        "#print(loss)\n",
        "draw(target)\n",
        "print(target)\n",
        "print(out[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUXxDqZayNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fca097-bdf3-4078-aafd-5c63aaa00d2d"
      },
      "source": [
        "print(tragets.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10RWCATMGcqG"
      },
      "source": [
        "plt.imshow(  img[0].cpu().permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jHwWqMFX8n5"
      },
      "source": [
        "draw(out[0])\r\n",
        "print(len(out[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2qIApnwh5pK"
      },
      "source": [
        "draw(out[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jfQRuyqhHq"
      },
      "source": [
        "#Testing (Nuscene's Basics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1v635cuk858"
      },
      "source": [
        "classes = nusc.get('category',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVYn959sH3Pz",
        "outputId": "7e1bafa7-c9fc-4a69-c631-391fb9189069"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Darknet(\n",
              "  (module_list): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (conv_0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_0): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_0): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (conv_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_1): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (conv_2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_2): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_2): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_3): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_3): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (shortcut_4): EmptyLayer()\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (conv_5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batch_norm_5): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_5): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (conv_6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_6): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_6): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (conv_7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_7): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_7): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (shortcut_8): EmptyLayer()\n",
              "    )\n",
              "    (9): Sequential(\n",
              "      (conv_9): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_9): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_9): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (10): Sequential(\n",
              "      (conv_10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_10): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_10): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (11): Sequential(\n",
              "      (shortcut_11): EmptyLayer()\n",
              "    )\n",
              "    (12): Sequential(\n",
              "      (conv_12): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batch_norm_12): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_12): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (13): Sequential(\n",
              "      (conv_13): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_13): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_13): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (14): Sequential(\n",
              "      (conv_14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_14): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_14): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (15): Sequential(\n",
              "      (shortcut_15): EmptyLayer()\n",
              "    )\n",
              "    (16): Sequential(\n",
              "      (conv_16): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_16): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_16): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (17): Sequential(\n",
              "      (conv_17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_17): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_17): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (18): Sequential(\n",
              "      (shortcut_18): EmptyLayer()\n",
              "    )\n",
              "    (19): Sequential(\n",
              "      (conv_19): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_19): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_19): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (20): Sequential(\n",
              "      (conv_20): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_20): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_20): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (21): Sequential(\n",
              "      (shortcut_21): EmptyLayer()\n",
              "    )\n",
              "    (22): Sequential(\n",
              "      (conv_22): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_22): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_22): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (23): Sequential(\n",
              "      (conv_23): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_23): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_23): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (24): Sequential(\n",
              "      (shortcut_24): EmptyLayer()\n",
              "    )\n",
              "    (25): Sequential(\n",
              "      (conv_25): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_25): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_25): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (26): Sequential(\n",
              "      (conv_26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_26): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_26): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (27): Sequential(\n",
              "      (shortcut_27): EmptyLayer()\n",
              "    )\n",
              "    (28): Sequential(\n",
              "      (conv_28): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_28): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_28): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (29): Sequential(\n",
              "      (conv_29): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_29): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_29): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (30): Sequential(\n",
              "      (shortcut_30): EmptyLayer()\n",
              "    )\n",
              "    (31): Sequential(\n",
              "      (conv_31): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_31): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_31): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (32): Sequential(\n",
              "      (conv_32): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_32): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_32): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (33): Sequential(\n",
              "      (shortcut_33): EmptyLayer()\n",
              "    )\n",
              "    (34): Sequential(\n",
              "      (conv_34): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_34): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_34): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (35): Sequential(\n",
              "      (conv_35): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_35): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_35): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (36): Sequential(\n",
              "      (shortcut_36): EmptyLayer()\n",
              "    )\n",
              "    (37): Sequential(\n",
              "      (conv_37): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batch_norm_37): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_37): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (38): Sequential(\n",
              "      (conv_38): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_38): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_38): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (39): Sequential(\n",
              "      (conv_39): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_39): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_39): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (40): Sequential(\n",
              "      (shortcut_40): EmptyLayer()\n",
              "    )\n",
              "    (41): Sequential(\n",
              "      (conv_41): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_41): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_41): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (42): Sequential(\n",
              "      (conv_42): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_42): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_42): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (43): Sequential(\n",
              "      (shortcut_43): EmptyLayer()\n",
              "    )\n",
              "    (44): Sequential(\n",
              "      (conv_44): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_44): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_44): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (45): Sequential(\n",
              "      (conv_45): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_45): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_45): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (46): Sequential(\n",
              "      (shortcut_46): EmptyLayer()\n",
              "    )\n",
              "    (47): Sequential(\n",
              "      (conv_47): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_47): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_47): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (48): Sequential(\n",
              "      (conv_48): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_48): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_48): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (49): Sequential(\n",
              "      (shortcut_49): EmptyLayer()\n",
              "    )\n",
              "    (50): Sequential(\n",
              "      (conv_50): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_50): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_50): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (51): Sequential(\n",
              "      (conv_51): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_51): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_51): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (52): Sequential(\n",
              "      (shortcut_52): EmptyLayer()\n",
              "    )\n",
              "    (53): Sequential(\n",
              "      (conv_53): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_53): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_53): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (54): Sequential(\n",
              "      (conv_54): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_54): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_54): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (55): Sequential(\n",
              "      (shortcut_55): EmptyLayer()\n",
              "    )\n",
              "    (56): Sequential(\n",
              "      (conv_56): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_56): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_56): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (57): Sequential(\n",
              "      (conv_57): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_57): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_57): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (58): Sequential(\n",
              "      (shortcut_58): EmptyLayer()\n",
              "    )\n",
              "    (59): Sequential(\n",
              "      (conv_59): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_59): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_59): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (60): Sequential(\n",
              "      (conv_60): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_60): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_60): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (61): Sequential(\n",
              "      (shortcut_61): EmptyLayer()\n",
              "    )\n",
              "    (62): Sequential(\n",
              "      (conv_62): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batch_norm_62): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_62): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (63): Sequential(\n",
              "      (conv_63): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_63): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_63): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (64): Sequential(\n",
              "      (conv_64): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_64): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_64): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (65): Sequential(\n",
              "      (shortcut_65): EmptyLayer()\n",
              "    )\n",
              "    (66): Sequential(\n",
              "      (conv_66): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_66): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_66): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (67): Sequential(\n",
              "      (conv_67): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_67): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_67): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (68): Sequential(\n",
              "      (shortcut_68): EmptyLayer()\n",
              "    )\n",
              "    (69): Sequential(\n",
              "      (conv_69): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_69): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_69): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (70): Sequential(\n",
              "      (conv_70): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_70): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_70): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (71): Sequential(\n",
              "      (shortcut_71): EmptyLayer()\n",
              "    )\n",
              "    (72): Sequential(\n",
              "      (conv_72): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_72): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_72): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (73): Sequential(\n",
              "      (conv_73): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_73): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_73): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (74): Sequential(\n",
              "      (shortcut_74): EmptyLayer()\n",
              "    )\n",
              "    (75): Sequential(\n",
              "      (conv_75): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_75): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_75): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (76): Sequential(\n",
              "      (conv_76): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_76): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_76): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (77): Sequential(\n",
              "      (conv_77): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_77): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_77): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (78): Sequential(\n",
              "      (conv_78): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_78): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_78): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (79): Sequential(\n",
              "      (conv_79): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_79): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_79): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (80): Sequential(\n",
              "      (conv_80): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_80): BatchNorm2d(1024, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_80): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (81): Sequential(\n",
              "      (conv_81): Conv2d(1024, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (82): Sequential(\n",
              "      (yolo_82): YOLOLayer(\n",
              "        (mse_loss): MSELoss()\n",
              "        (bce_loss): BCELoss()\n",
              "      )\n",
              "    )\n",
              "    (83): Sequential(\n",
              "      (route_83): EmptyLayer()\n",
              "    )\n",
              "    (84): Sequential(\n",
              "      (conv_84): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_84): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_84): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (85): Sequential(\n",
              "      (upsample_85): Upsample()\n",
              "    )\n",
              "    (86): Sequential(\n",
              "      (route_86): EmptyLayer()\n",
              "    )\n",
              "    (87): Sequential(\n",
              "      (conv_87): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_87): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_87): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (88): Sequential(\n",
              "      (conv_88): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_88): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_88): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (89): Sequential(\n",
              "      (conv_89): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_89): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_89): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (90): Sequential(\n",
              "      (conv_90): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_90): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_90): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (91): Sequential(\n",
              "      (conv_91): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_91): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_91): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (92): Sequential(\n",
              "      (conv_92): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_92): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_92): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (93): Sequential(\n",
              "      (conv_93): Conv2d(512, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (94): Sequential(\n",
              "      (yolo_94): YOLOLayer(\n",
              "        (mse_loss): MSELoss()\n",
              "        (bce_loss): BCELoss()\n",
              "      )\n",
              "    )\n",
              "    (95): Sequential(\n",
              "      (route_95): EmptyLayer()\n",
              "    )\n",
              "    (96): Sequential(\n",
              "      (conv_96): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_96): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_96): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (97): Sequential(\n",
              "      (upsample_97): Upsample()\n",
              "    )\n",
              "    (98): Sequential(\n",
              "      (route_98): EmptyLayer()\n",
              "    )\n",
              "    (99): Sequential(\n",
              "      (conv_99): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_99): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_99): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (100): Sequential(\n",
              "      (conv_100): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_100): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_100): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (101): Sequential(\n",
              "      (conv_101): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_101): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_101): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (102): Sequential(\n",
              "      (conv_102): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_102): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_102): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (103): Sequential(\n",
              "      (conv_103): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batch_norm_103): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_103): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (104): Sequential(\n",
              "      (conv_104): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batch_norm_104): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "      (leaky_104): LeakyReLU(negative_slope=0.1)\n",
              "    )\n",
              "    (105): Sequential(\n",
              "      (conv_105): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (106): Sequential(\n",
              "      (yolo_106): YOLOLayer(\n",
              "        (mse_loss): MSELoss()\n",
              "        (bce_loss): BCELoss()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxTzVdt1HyNt",
        "outputId": "6ac1b271-df32-49d3-cc08-c7f605a70757"
      },
      "source": [
        "img, label = next(iter(dataloader))\n",
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 1184, 1184])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q89sfq4sHyPY",
        "outputId": "ee8f2cf5-f9d8-4dfb-a58f-9a56faab6f6d"
      },
      "source": [
        "loss, out = model(img.to(device),label.to(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:217: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:218: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:219: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:220: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:221: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:222: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:223: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:224: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:225: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:226: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:228: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:232: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:233: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:234: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EL2lkUEUaeZ",
        "outputId": "2538f9e0-5e4f-499e-ad7b-beb97ff23bc9"
      },
      "source": [
        "print(non_max_suppression(out.to(device))[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6K8n4CZT7Ft",
        "outputId": "14c726dd-6045-4fcf-d9c7-d2ac2f625efc"
      },
      "source": [
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 86247, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-DD17QtR2dq"
      },
      "source": [
        "del imgs,targets,out,loss\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FgzPg8rc4Bse",
        "outputId": "09b09246-a40c-4a3e-ee09-ab10d16f41a9"
      },
      "source": [
        "print(label[:,2].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8393)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Rju84T256zm-",
        "outputId": "2d9cfc4c-89f4-4450-9bc5-0871adf180e2"
      },
      "source": [
        "print(label[:,2].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1982.4609)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3TwxDkCqj-g",
        "outputId": "08e44627-9bf0-4ef6-fbb5-2840a68bde2d"
      },
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes \n",
        "\n",
        "nusc = NuScenes(version='v1.0-mini', dataroot='drive/My Drive/data', verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 1.387 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A13iNdjFqkm_",
        "outputId": "2757e855-84de-40c4-fcb5-0b087ca3c486"
      },
      "source": [
        "scene = nusc.scene[0]\n",
        "scene"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': 'Parked truck, construction, intersection, turn left, following a van',\n",
              " 'first_sample_token': 'ca9a282c9e77460f8360f564131a8af5',\n",
              " 'last_sample_token': 'ed5fc18c31904f96a8f0dbb99ff069c0',\n",
              " 'log_token': '7e25a2c8ea1f41c5b0da1e69ecfa71a2',\n",
              " 'name': 'scene-0061',\n",
              " 'nbr_samples': 39,\n",
              " 'token': 'cc8c0bf57f984915a77078b10eb33198'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkw-qStdrcmq",
        "outputId": "5bf69695-0ca8-4faa-deb4-de210e99c4d9"
      },
      "source": [
        "sample_token = scene['first_sample_token']\n",
        "sample = nusc.get('sample',sample_token)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anns': ['ef63a697930c4b20a6b9791f423351da',\n",
              "  '6b89da9bf1f84fd6a5fbe1c3b236f809',\n",
              "  '924ee6ac1fed440a9d9e3720aac635a0',\n",
              "  '91e3608f55174a319246f361690906ba',\n",
              "  'cd051723ed9c40f692b9266359f547af',\n",
              "  '36d52dfedd764b27863375543c965376',\n",
              "  '70af124fceeb433ea73a79537e4bea9e',\n",
              "  '63b89fe17f3e41ecbe28337e0e35db8e',\n",
              "  'e4a3582721c34f528e3367f0bda9485d',\n",
              "  'fcb2332977ed4203aa4b7e04a538e309',\n",
              "  'a0cac1c12246451684116067ae2611f6',\n",
              "  '02248ff567e3497c957c369dc9a1bd5c',\n",
              "  '9db977e264964c2887db1e37113cddaa',\n",
              "  'ca9c5dd6cf374aa980fdd81022f016fd',\n",
              "  '179b8b54ee74425893387ebc09ee133d',\n",
              "  '5b990ac640bf498ca7fd55eaf85d3e12',\n",
              "  '16140fbf143d4e26a4a7613cbd3aa0e8',\n",
              "  '54939f11a73d4398b14aeef500bf0c23',\n",
              "  '83d881a6b3d94ef3a3bc3b585cc514f8',\n",
              "  '74986f1604f047b6925d409915265bf7',\n",
              "  'e86330c5538c4858b8d3ffe874556cc5',\n",
              "  'a7bd5bb89e27455bbb3dba89a576b6a1',\n",
              "  'fbd9d8c939b24f0eb6496243a41e8c41',\n",
              "  '198023a1fb5343a5b6fad033ab8b7057',\n",
              "  'ffeafb90ecd5429cba23d0be9a5b54ee',\n",
              "  'cc636a58e27e446cbdd030c14f3718fd',\n",
              "  '076a7e3ec6244d3b84e7df5ebcbac637',\n",
              "  '0603fbaef1234c6c86424b163d2e3141',\n",
              "  'd76bd5dcc62f4c57b9cece1c7bcfabc5',\n",
              "  '5acb6c71bcd64aa188804411b28c4c8f',\n",
              "  '49b74a5f193c4759b203123b58ca176d',\n",
              "  '77519174b48f4853a895f58bb8f98661',\n",
              "  'c5e9455e98bb42c0af7d1990db1df0c9',\n",
              "  'fcc5b4b5c4724179ab24962a39ca6d65',\n",
              "  '791d1ca7e228433fa50b01778c32449a',\n",
              "  '316d20eb238c43ef9ee195642dd6e3fe',\n",
              "  'cda0a9085607438c9b1ea87f4360dd64',\n",
              "  'e865152aaa194f22b97ad0078c012b21',\n",
              "  '7962506dbc24423aa540a5e4c7083dad',\n",
              "  '29cca6a580924b72a90b9dd6e7710d3e',\n",
              "  'a6f7d4bb60374f868144c5ba4431bf4c',\n",
              "  'f1ae3f713ba946069fa084a6b8626fbf',\n",
              "  'd7af8ede316546f68d4ab4f3dbf03f88',\n",
              "  '91cb8f15ed4444e99470d43515e50c1d',\n",
              "  'bc638d33e89848f58c0b3ccf3900c8bb',\n",
              "  '26fb370c13f844de9d1830f6176ebab6',\n",
              "  '7e66fdf908d84237943c833e6c1b317a',\n",
              "  '67c5dbb3ddcc4aff8ec5140930723c37',\n",
              "  'eaf2532c820740ae905bb7ed78fb1037',\n",
              "  '3e2d17fa9aa5484d9cabc1dfca532193',\n",
              "  'de6bd5ffbed24aa59c8891f8d9c32c44',\n",
              "  '9d51d699f635478fbbcd82a70396dd62',\n",
              "  'b7cbc6d0e80e4dfda7164871ece6cb71',\n",
              "  '563a3f547bd64a2f9969278c5ef447fd',\n",
              "  'df8917888b81424f8c0670939e61d885',\n",
              "  'bb3ef5ced8854640910132b11b597348',\n",
              "  'a522ce1d7f6545d7955779f25d01783b',\n",
              "  '1fafb2468af5481ca9967407af219c32',\n",
              "  '05de82bdb8484623906bb9d97ae87542',\n",
              "  'bfedb0d85e164b7697d1e72dd971fb72',\n",
              "  'ca0f85b4f0d44beb9b7ff87b1ab37ff5',\n",
              "  'bca4bbfdef3d4de980842f28be80b3ca',\n",
              "  'a834fb0389a8453c810c3330e3503e16',\n",
              "  '6c804cb7d78943b195045082c5c2d7fa',\n",
              "  'adf1594def9e4722b952fea33b307937',\n",
              "  '49f76277d07541c5a584aa14c9d28754',\n",
              "  '15a3b4d60b514db5a3468e2aef72a90c',\n",
              "  '18cc2837f2b9457c80af0761a0b83ccc',\n",
              "  '2bfcc693ae9946daba1d9f2724478fd4'],\n",
              " 'data': {'CAM_BACK': '03bea5763f0f4722933508d5999c5fd8',\n",
              "  'CAM_BACK_LEFT': '43893a033f9c46d4a51b5e08a67a1eb7',\n",
              "  'CAM_BACK_RIGHT': '79dbb4460a6b40f49f9c150cb118247e',\n",
              "  'CAM_FRONT': 'e3d495d4ac534d54b321f50006683844',\n",
              "  'CAM_FRONT_LEFT': 'fe5422747a7d4268a4b07fc396707b23',\n",
              "  'CAM_FRONT_RIGHT': 'aac7867ebf4f446395d29fbd60b63b3b',\n",
              "  'LIDAR_TOP': '9d9bf11fb0e144c8b446d54a8a00184f',\n",
              "  'RADAR_BACK_LEFT': '312aa38d0e3e4f01b3124c523e6f9776',\n",
              "  'RADAR_BACK_RIGHT': '07b30d5eb6104e79be58eadf94382bc1',\n",
              "  'RADAR_FRONT': '37091c75b9704e0daa829ba56dfa0906',\n",
              "  'RADAR_FRONT_LEFT': '11946c1461d14016a322916157da3c7d',\n",
              "  'RADAR_FRONT_RIGHT': '491209956ee3435a9ec173dad3aaf58b'},\n",
              " 'next': '39586f9d59004284a7114a68825e8eec',\n",
              " 'prev': '',\n",
              " 'scene_token': 'cc8c0bf57f984915a77078b10eb33198',\n",
              " 'timestamp': 1532402927647951,\n",
              " 'token': 'ca9a282c9e77460f8360f564131a8af5'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0WDSEtHOK0O",
        "outputId": "cb95afc0-89c6-4a4e-8d01-97c7815145bb"
      },
      "source": [
        "for first_anno in sample['anns'] :\n",
        "  anno = nusc.get('sample_annotation',first_anno)\n",
        "  vis = nusc.get('visibility',anno['visibility_token'])\n",
        "  print(vis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 40 and 60%', 'token': '2', 'level': 'v40-60'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 0 and 40%', 'token': '1', 'level': 'v0-40'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 60 and 80%', 'token': '3', 'level': 'v60-80'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n",
            "{'description': 'visibility of whole object is between 80 and 100%', 'token': '4', 'level': 'v80-100'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u035LGwgPRiA",
        "outputId": "ec34f518-7c50-47c1-c7c8-99daa9a837ba"
      },
      "source": [
        "vis = nusc.get('visibility',anno['visibility_token'])\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description': 'visibility of whole object is between 0 and 40%',\n",
              " 'level': 'v0-40',\n",
              " 'token': '1'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "O9b82vLGnJ4v",
        "outputId": "2d32c171-6c7a-4c6f-dfd2-e96a6c7dd2cb"
      },
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/data/v1.0-mini/category.json') as f:\n",
        "  data = json.load(f)\n",
        "for d in data:\n",
        "  print(d['name'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "human.pedestrian.adult\n",
            "human.pedestrian.child\n",
            "human.pedestrian.wheelchair\n",
            "human.pedestrian.stroller\n",
            "human.pedestrian.personal_mobility\n",
            "human.pedestrian.police_officer\n",
            "human.pedestrian.construction_worker\n",
            "animal\n",
            "vehicle.car\n",
            "vehicle.motorcycle\n",
            "vehicle.bicycle\n",
            "vehicle.bus.bendy\n",
            "vehicle.bus.rigid\n",
            "vehicle.truck\n",
            "vehicle.construction\n",
            "vehicle.emergency.ambulance\n",
            "vehicle.emergency.police\n",
            "vehicle.trailer\n",
            "movable_object.barrier\n",
            "movable_object.trafficcone\n",
            "movable_object.pushable_pullable\n",
            "movable_object.debris\n",
            "static_object.bicycle_rack\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "_SLzR8LenKIE",
        "outputId": "b6c32e34-7bb9-4eab-f3f3-6577eaecc7d5"
      },
      "source": [
        "img,targets = iter(dataloader).next()\n",
        "plt.imshow(  img[0].permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1b0582a5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD8CAYAAAB6iWHJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9WbAl2XWe9621987MM9yhblVXT1XdDRANgCCgAAmIpEzbgmTLQUkMkZ5E2rJNOxyBFztCDtmyJb/YD/aDX2zRIQcdiKAiKIVCtGTRQdIUZWviaIIDCJAYmui5uqu6a77DGXLYw/JDnkI3wAsSbACtdji/iKqbJ+85effJ3PvPtf+19jliZkxMTEx8JfrPuwETExPvTCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJe3XRxE5HtF5Isi8ryI/JW3++9PTEx8bcjbWecgIg54FvhTwHXgN4B/x8y+8LY1YmJi4mvi7Y4cvhN43sxeNLMB+Ang+9/mNkxMTHwN+Lf57z0OvPqmx9eB73rzE0Tk48DHdw8/8ja1a2Li/8/cNbOHvnLn2y0OfyBm9gngEwAiMtV2T0x887l23s63e1pxA7j6psdXdvsmJibeYbzd4vAbwNMi8i4RqYAfAn76bW7DxMTE18DbOq0wsyQi/ynwfwEO+Btm9vm3sw0TExNfG29rKvMPy+Q5TEy8LXzKzD76lTunCsmJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzecviICJXReSficgXROTzIvIXd/uPROQfichzu58XdvtFRP5nEXleRH5HRL7jG/UmJiYmvvF8PZFDAv5zM/sA8N3AfyIiHwD+CvBPzOxp4J/sHgP8aeDp3b+PAz/6dfztiYmJbzJvWRzM7HUz+63d9gp4Bngc+H7gx3dP+3HgB3bb3w/8TRv5JHAoIo++5ZZPTEx8U/mGeA4i8hTw7cCvAQ+b2eu7X90EHt5tPw68+qaXXd/t+8pjfVxEflNEfvMb0baJiYm3xtctDiKyBP4+8J+Z2dmbf2dmBtgf5nhm9gkz+6iZffTrbdvExMRb5+sSBxEJjMLwt83sJ3e7bz2YLux+3t7tvwFcfdPLr+z2TUxMvAP5erIVAvwY8IyZ/Y9v+tVPAz+82/5h4KfetP8/2GUtvhs4fdP0Y2Ji4h2GjJH/W3ihyL8I/BLwWaDsdv/XjL7D3wWeAK4Bf97M7u/E5K8D3wtsgf/IzH5fX0FE3lrjJiYm/jB86rxp/FsWh7eDSRwmJt4WzhWHqUJyYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs5lEoeJiYlzmcRhYmLiXCZxmJiYOJdJHCYmJs7F//NuwDcW4UtfzSmC7B6O380h468fPFMUdl/laW9+7df8TRm7Y37peG9+bAjC2IA3ji0Cgo4/1eGcIiKUYuScMSuYjd8P9Hu+TsR2/+2O+aX39qXWy7j95tfJuF9VcKrkUijFMLMvvQ7kjWO/6fx9+Z+X37PnrbM7F6o0dU3fD5Tde36jHW96P1/2Snmj1Q/ev3z56x5cAwHYXRPbnTazN1+xN87l+NbtjeM+OIYIZobstseDvLH94Fq98fyxX5WcKQ/61pvegn1ZO99o05vPzJf+sze1701NRn7P5pf3lQf90L7U3DcPiS8/Z7v+8cb7+HLe0eKg6qiqJbmAlYRI2Z2zBwO9IFJhFhF1UIQi48V89PKc5X7FzZtrVquI84o4B+YAo947QlGEgkkCqVA3XhQzxUukYAxDR+0PkFkgxUROA+qAPFBrQ7EIZJxPVDhyhiCJOnik8jiv1H5GqD1N3bBYHnLp0sNcvfItvPe97+eJJ6/yxWee5TOf+U3u3LnBenOT9bqlmEBM9Am6IWNFUUmIKMHX1HOHpZbcrSH3PHTpErfu3mdIYCVSUJyH/XnD4w8/yvve/R5eef01rr12g7PjNcerji6BpEIEMCOjUBIIxGIgHkh48ySMQkYtkHJBJWPFUPUYGcQwMZxrMCeU3OPMIwZRwJlhRcE5XDCWh4f85b/0l/nE//I3uHnrBcQ8MXeoMcqCCSaCSBqvh6+BDEAxwYuhCoaSk2GWKWJ4c4gERDLiQX0gRsNyxjBUDbIiUoCCeocXowoO5wrOGaoBVzsO9w/Yblbs79Xs7R9iUijZuHr1abpuw8nJfZyvqKsFPihHR4/y2JUrvPLyNW7fehUrUMrYfkPBErkAJMw86iru3LnL669fBysUgZKVYh7JEfAUp1guCEZKAh5SjGQreAJSMlnG65IxkkVEPKKM53IniKUIOEPwmBgqhukM55R7t54/d/y9o8UBEZwTjIyJIownWcxAM5gHKYCOJ8GB4PBOufL4Ed0203Ub6sqDFXLK491ABob2jKqesX90yGZ9gpWESkMhI5LHk1831G5O04xRhuZCqqGql3gbKP1AyRBCRVM1VKGw7TNeAsEFQgiIzzRVRdVUzJrAYq9iMZ8hAqvVmpOTlrPTM+oafIBQHVA3YDly6fGrvPbaNdQpuXgwUPV4X3B+juGoZy1l8Dz12GWOj++DQE4ew6h8IbhIzKccn91nGFY0wWiDowlQREEKASGVQM6ZJBXOO8Qy5EzBgVO8OawYpo4QDCyhliimCGNnLGTGrpi5dPEyqp5+s2EwMO2wHJjVC/b2ZhyfrPiRv/YjtJs1zjwZ8Ooxi6g6SjIqdQwmOGeEKpCyIaKoZarmAOcc/fYUIY3nRQsqbryBqMN5j1/sUVlke7YGCg6HCGQBbw71IBYRN6eqKxbVKHJaOZbzQK2B5cE+fexxMvDY40+QU08cVlQVzOdz9i8ckuLAcllz8eiI7eqUYbhF7ECdUoUl6gIiuhO7miJK1244O7vN3nJJzC0xubE/a4NJBXlAqIhqlGKgPSlBFWZkMhYFDQ6RgiaHSsSKw6GY1KA9mj0FQ52CZpxAkQoXFCseF6qvOvze2eIAiDiCU4oBIuTUM966PUgGcYg5TMaOaabMm5rF4oAXX3wRiqJVwcxjOe2eFyhDJGtg2Hb0py0ZwbktzfKAUFWIzlgcHvLoY1e59tzn2K7W5GyoM9IDJVLBSkZKhRHIONStEQoijoLQuBp14EWw7CkxsmlPKHcKMfVsu54br7/K6XpLH1vabsWQ1hDnHF26xEuvvEQuUEoPBsnGLl5KgtITJJOKce/0mFiUYoVMGsWyVORirFY9t/x1tustZ6uOdiiAIUUpeDKZzBhU1fOKSwcX6dLAnbu3KRmK2SjLUsYw22wXlToUR7aMFBD1lDIO0BgzYkI0oao8+0dHnNy5g6mx7ROSN2xXa/oIYoViniBKFsWK4lQpriARTB1iA81ij+V8hvfw5/+9j3P39Tv85N/7MUSVMhTMlFgKlRfUOYRMjmfE5MbBaTpGGCWO/cElTBV1c7IkYsysCTR1gZwoto94z707LVo7HnvoIpuznk27QmRAnBBjx+rkBFE40xPW6xVdv6Vve1arNcVA6WhqTzVb4hzEoWW1OuXOvfusNxlTIAVcGK9fIZNVII/Xz/LAYIZlARtIsWBiY2RCRiVQxEiWUasogFnESsZkFy14Q4qBr5jV+2zaE4JmtKSvOvbe0eJgu/mRmeB9wcQhOGTX+UAxKYgq4gIqHosDla/phpbNACYFRUjGbsCCE6UQiDFxdv90VNWUERX6diBtV7hQ44Ln3u07DBvD4dEwTl0oEZNxgHmdj4LjhEKPloJYg2hCbEDZp2SljxnRxJAybDviUFDfIHKf9WbL6cldcknkDDkCZcurr7xAjmOkX4qBFIolnL9IKj0zqWnCHKNnterIWRAnqA84G09gNyRSalEndF0m9okcoU9ClLSbaguhqiixRZ3j+PgeuWQEUBPEuVEEd94FxbCSMHGogBU3dnBnOAdOaoiZTgaKZTQbztXkbOQcyQxYcmQKYhkVoVLDMiBj2JttgCzjtUEpolQF9i9d5PHLV/jH/+D/4PadewQ8RRLFMrPlnJIM5zxp6FHx5GjkPiKWEWcggXr/gG6zpVDQDEjGq6CayTkRhxoqRcXoEtSzgg8Lbt49QXNEXU1VJcw5VAJtu6ZuLqCyoe+29H3LEAf6vqNkI8Yt93NB5TYmRkwDUjz9YJAHMI84hdSN4mUeUhoHQDEQJQjElDEcxQYERd3owZiASULjeLMyNcQEKYY6R7GCZMU5hRIZujVeoCAk++oS8I4WBwFUA7lkSi7g0ps8voy6QMmjqVLiQNEOEUHcQJdGA85KQV3ASgTAa8C8gzRgWaFqCFVAtB/n3cMa1BFCoj05xTkhhMxqvUGsRtRDMpIaXsYLUQvEfkszD2SULBlJA04c2QyXWkT3yKVjiJDSQAwNoV4gEhj6NdvtKch4wZ2MYXY3ZOq6odqbob7swgZHNavw9UVczvTHZyznDU0dePzhQ4aSqZuGzeaMrkvkOJpqw5DIMdGnwpANFcFlJTshZ6HkiBVlaDskGeP9RPFeSLmACEUEF2Z4EeKwIWelSAESYm4UGq2o5w2+WlD6E0p2iPds256Co57NWc5rVscnxKEDdUiGZEaWhKDjQE1GVI+VgvcFyYW0PuPmtVfY3Dsm50TadqQhEVPGeSX2HS7UmBp4iGY4c6M3YYVRwQYajeCMKEJOQo4FKoeYx9xoJ1deKSWxXmWKJiiRyg8EHFoZQ3bUdaGXjlLAuY52ELp+oO96Ysyk1FOikYvtjPFxyleKx/CotIgqWhKpGFIcDjBxJDGyjb6DGOScKFIopSAEMEFlN7U2Gf0ilBwiKkYpHtFAcJCyEEsmF6jUU2TAuwrRCrOvMD3fxNctDiLigN8EbpjZ94nIu4CfAC4CnwL+fTMbRKQG/ibwEeAe8INm9vIfcOxRFEoBDyUL3jtKNpx6nFNMhKqCPjry4MFlQu3ZnK3JsaCAN6XLgCiiHhc8RYXURyT19EScC1AiQQMiRqGCkhi2HdZtUXM7sy6CeiQK6kGz0OuAE8O3aZxa2ICphxLIGTQoRSJNmI/zz8ojMqcONUKmmTVcvnyVzXYNVojzfUqKFDNS1fPud7+HYYg8+8xnyKkmn5yRiuFF8bJCDPZmcLpOzGYNH/nQh/j0Z34dP1PC8oDFYs6sCcQonJ2dcvcscnK2IhcjS8abEK3gVMgZcIydlDGTUjUVQ0ooRu63WHDsLw/YrtYkUVIZBxWieBNMEoeHM+rZBVYn9xlKYba3ZFbBvfv3OD5pWSzmdHGDmpJLAlW8eLI5ck44rcAgi6AZshWSU/RszWa9JlQVmKJlvMbZRh8kDobzFd4JHmGIERuDPGLJKMb90y0+BKQ4TBR1MNvbIw4byqC4xuGbms020vaREhOzmScjmBNcUbxFjAV9O6DBj6YxY+QwxJ6UtxTzxBjJpUOkJpsyDC1ORmM2F8OKkTF6S1Smo/nuAAooZItUjF6EWEZsNOfRmmQRSh7NRg0ghtqDDM2YlRpMEQQpGXOeaAW1QqFHSsL5r17N8I2IHP4i8Aywv3v8PwD/k5n9hIj8r8B/DPzo7uexmb1HRH5o97wf/P0ObJaBOEZXxXZJMEcpPYLgQk0fO7Jz1E5JlYJ5QnAAiBoUxVQw8RTLUBK567AigIzz6RSwYogKmYIXoxBxCn3f4YtSYgcaEPNUFEx0DN9KwjtFxZGS4Z2j9gEfhOWiop7XzJsjmrrmXe/5IKGa8cLznyEOZ2y3Stet6PqWC4cXuHbtBkOvWE60Q0uoPauzFc69SlPVnJ2uwSKmIE4xV6FOEaBNcNZl+jxw995Nbt3r6GIEWRGqmmWj+Mpx6eCIDz12lZdefomYISZYtz25DAzJcbbq8d7RJyGXhIrj4PAS3bCl71pyvyUNxkk5wYBFFYhOMDUW8wV9gtlsnwuHl8lO2ZZId++U1PbUeGazGd2q4+z0DLPRKB5TbjZ6K1oIOLA8mtFFoRScOiwXIoYzIXY9huG9kg1ICsJo1lomZMHVYYwqnI5mtAqIp+SCm3ksFUyN2d6Sp7/9X2W53ON3f/WncLIh5zm+mhHYEEUZ4kA1KMwGkgk4oR8SOUUWviKnDZmart0Qh46SC7kMZEvENM6PTXpMAmnn9IjaOEtNhouQRMd8Y0qUwijGOZHMkRNkDCEgmsjkMSshEdHd68hYEVQc7EzakiNYQF1FIeOloRDJZcz2iISvOv6+LnEQkSvAnwX+e+AviYgAfxL4d3dP+XHgv2UUh+/fbQP878BfFxEx+z0Z/TeOj6JSkbXFSsGJAysE9eATJcbRrBwyUgW8C/Tbnv29R3AhYvk2zimqDtGCZ4xCTDzjvXHY5X4HMh7LhlMo4nG7MAxLdCWh5hHLNN4jAi4UFrOKD37bR/j8Z3+bwoAxGmnLvQPa9QmnZxHdJGBDCJH53iFdF7n12muIX3Bgc0SUfjhhf++QYRD6OBBqo7YG7wNVXXDBo8HYe+jCKIq+wjulrgJx3bI3D3Rti+wudj8UtKpwRckWyX1hleIYpTT7hHXLC9duj/lxXxMcHOzt8+Fve4rf/O3PMUTDWaZ4R7LCyektfHPANkaCBHxwpJxQlBgL1J7KeZZ7Sy42DUam2EDbZUjg3YwQZrSnd9muN1gs43TLCgUjI3j1lFRQzaRScOZRe+Bt7Dp/tnHKURTznhh7SgIso5KJKL4YRQsxK8UJUpRoA2Z5rD1RjxZo2y1elCBKGRIvP/8ZKgrOWtzsiCEWvF8SuUXSMnob3nDUWBpNxL4bDdim7rBcoU2k69aUMoxihJFS3pm4LeARxima90bOYDYg5hEtlJwQzeTix2hHZJwKmY1p2eSJOeFFxgg2OZII8mCKoTp6DmXMYYqMKV7EkUsGjKGsUe9wGiiEcar6Vfh6I4e/BvyXwN7u8UXgxMweWKDXgcd3248DrwKYWRKR093z736ZIIh8HPg4gKpi1gOFghvd5lgoLlKiQ50hSVCU0hVMe5xEqqpGLAIGTompB4yCjolfA/Fjx7GSKKKjk2sKTgFDJeAoOJ8xU6rQ0A5GFqNkgZRxWnjmdz/LkAtDjKgphcje3j4xJbIVqsoTglHXSxAhNHOqvT2c91TzBfXVj6LXfpV6scfe4ZImG2KF6vIB3faYR44aHr50iRRbmq0npRooVM2C4Fu61FHE0ccOxLG/DBwcLDk6rClU5FyQIhxcWDIMxt7+IWLG3uEB282Wrs+07YAJfOG5l7m/6igl4VVRH0gYjXdYv2Imjipk6vmCxWLGrTu3iQnSEMFlTlYdF+oaMzhbnbJuB/YvXOQv/Ov/Iddefolf+eX/k3Q/7zq9YDzwAxIuGSLjAHEIkUJjniygOhZtlTrQqOIXMxpfcf/OPSDhVImWCIRxoBhk3aX4XMZFT9ZRaITRJ1GbYebIIZPTwPrmqyyaisoJq9MT9hdLTu5dJ+XM3t4e226Use22Jfg5JWTarqcKQoyAZiQ5un4gpV0fKYrDUWyc9/tKkSK7WhVDXM0wCDmNdThKoU+eYt2u7iSDqykJ8pAoRcbotpSxMAsI6knJyCliOJCxnsMQCuO0A+mxXc2JAF4roGCakeqbYEiKyPcBt83sUyLysbd6nK/EzD4BfAIghGAmDpPxgoOOVYQJTAtkwSSTRXFALkbQwNHREV0rBO9xIpg6KBERQ7VQiodhDMe08uQ4Tlh8lRF1eJd2ufaBnKEvsO8GoKYUSNaNQmCZLg7gFCceLx2iFVJnDh++DGLMljUiMw4P9zk4WjCrDzlenxEqTyFT2nuUMjCbzQhVRbc6wXuPlEjqM1VdoURy6VERGqcUoKocZRD2ZvUuNK+ZeVCXMSIxj4MNi1TzGd4XZlXNlaN9jrcnvPvpy/Rdy+psS10FTleJxgVmjZB6RxIdC8eKsr/c4979+5Si4CqCGCm1OF8xm8/ohi2pN9p2Q71d8q9977/BM898hvsvP8t6M/Dajets45aSwUtFsh7RgJdMKhFFMFcojNOIbEYAokTMRoNW1PDBsb9ccPX938rqdMXJ6i4xCrMQaKoFsY0YglejmOGzkdTvnO1AWAiVd6SuxWEMccxykB0WI83cM+RC6RPVQkgl05qwOPoW6ujourt4F/EW6dsW1THTUiRRohG9I8YtKWVy6shZybnflX44zAKQaeaehx56hFQStT/mbFU422ZyzqCC7ExUp54+FnIexhQo+YEhMZqOZgiGSDVW/Ao707WQi4zVj7uUu9qYeB4j5TEVKsI3LZX5PcCfE5E/AzSMnsOPAIci4nfRwxXgxu75N4CrwHUR8cABozH5VTFjV0dggBvfkAmGx+8MMMtQ1WNA0LWFbpfXjWU0mhQb1dTKWEWaHUhidnDAhcfezdC2rI9fJ/ZnY7rSCkrBqxGZUzUJ69OYQnRjgZCzMHZoFaRyuFIoRcmpYn8WCEGp6oa+awm+oqoye7N95s1FhqFnudzH+xaAcvIMwXWcnlyn9jVlttiV4XYsFzXBJ05PrpOLsT+f0W7WBKC0iitCioVu2zILymJvTimZ119/laMDIM/IeSzAwjwlZop0JBKBHryyRvGqXD7ap089IVSoJXJKdH3HYjYnlUy1f4kUt1y++n5Wt16k3W5Q5xhSR900iMv0faRd3eNzn/llEoL1W9Yp8Y9/6WeJMZE2K3IAlwUBhhLHkNpFzMbzmE3HkFkKVhxBQazFSo21keN4wupTvzWmEa2i9kZJERdm7O95zJTN6Qn4irp25D7Rx4hpobJ9FMErDH2H4sfsUHDU4hEvpBaKKRmjJCOnwvHxK8yWFxg2r+OdsvfYu+jbG5QckUqIvaJSIGbiYJQCWI2VSFXPGfoxG1Jypg5w6fCQj37HdxNT5lOf/gVOz1ZoHsgScDb6XtEyWgSnNdkyEYdYxDSPWTZziOrox8mA5TQWP+lYyyEPUvcFHJ4ihjmBUlAiiMNKodB81fH3lsXBzP4q8FcBdpHDf2Fmf0FE/h7wbzFmLH4Y+KndS3569/hXd7//p7+f3/AAAVBHyXF0YsWNhbzmx7tAiaTBE8LoeBcYi1A2W0IAr25MEwWH2BhZ+Fqx1LE9voUGT3COHATNBafg64ZihfnhjP25sNlGNl2H96PJ6bLiJeBCRx0Czgq1DYT9OQfzBfPZnKZpWFw6YrnY5+DgAt/6LX+Ueyev8Ruf/zVIo8k6pIHcD6gZN06u0fg5kjzbeEIcWooFihlx6IlpLKXu+x6RilnVo2VA6SlDj/dKvLulqgKUPEbuRamCcrQ3Y94oi/mCs9NjihmLuqLZb5hXQrJMuxHmIfDQxRnHZz2lN8RqZlUALRzOZmz7wnb9KqqF2fKAC0dHhNlDgPH6nVfYQ9m0a5574ZldNaNQSkJqj0t5TNUqqPdoEnJxu8meB5FxLmyFYmOpetAHNbGAK+xmc8SuHW8SljHGKsSmW3OyheDCKNp1QOslkrYgBS8Fa09ZrcFE8c5o5oFqOcf6gYMLB4T9C3Q3XuLi0RN0wymrLmIoIVSsjq9TrKKYcf/my6RY2D+4hMR7bLqBuvEkBnLaYrkbB7ElVAUfjCFHNCfUV8yXh3zwgx/mbHXMr//GL5CT362zSOSiFEuAQzQjJeKAQj/WmGigWMYKZCmQDcUh6sipgD0wKseqSlOHwq5GSMblAkVQDYhGxL29dQ7/FfATIvLfAZ8Gfmy3/8eAvyUizwP3gR/6gw70IB1jlsfUJZBLwTkQE1JOoA0KxCIgmVlw3Hjts7SbQkkQA2DKwX4AS1A83/VdH6OPLb/+a79MisZsueDg4hXIYwHUbLagqaDPPeuV0feRSjMpKV4LJSU0RDRVpLZDHdR1xayeEbzgSGxXG/puzXq94uxky6OXnub6qzd48bkvol5YLBYYSrdNlJSJaaBwRt+PaxBy6ai80sWWkpWYdouySoXJjE3K1JLQHInRaBqjHaDtBqRkkgqaHDjPndNjmlrZm3eoKKUMVLM5y8bR5cTQJfZmgcsHB3hTDnzgrDM2UelTR+wdxe6BZbp+QbXcQ+sFjx1e4tWzNRGDvUOkOWQeM2zv0/c90YzHL1/h8Uce5f5qxUvPfp729GQ39cloGStbFTdWlVpEGGtD1IexAlUyXh3Bj3fJ6DKpuNF72vlHWjJJCkggpoivHCW1rE4jWT3eh3ExlIBoGqNR79i2LW2bWNaZmC/S318Ry4ymmSHxLjElZvuXeP9H/jTXnv0Up3du4rRn6LakoePk+DazSvHUWI6UBCln+qGMVaSpYCHgFCqJuKammV+kbbf8zM/+Xbohcnp6H5NE3k193C5iKS6iNmcYMtlsrMF4MLBxmDAamF4QS+O6CzXUFEwpkjC1URQBMx3XbyC4nQ2cpaJx34TI4SsG8c8DP7/bfhH4znOe0wH/9h/uyEKRcXmUSCIzCsC8DnjHOD8sRsqZ2I+pqhwjs+Dp6Ih5XMdXuczQRYyKCxcaDg8W3Lx9j0tHe9y5e4Z6Q0PB1OMDVLUDZEw3rsdONQsVQqDtW0QiqfeIDtRDoHWJuu9ZbQZCKEhySBGKRupZzWA3+PxzL2IlU0qL6YJuWOHEMQyFIMYjT347/XDKndvXWG8S3oREph96cg7EtHPpRXEuUc8P6NstzjJagQsV0m1ps1GyjemyXDAXQYSuwMm6HRcXDRnzHdIEUmKsUHQ9126vmPlAO4znLCK0PXRDpBbHbN5QqInNAfLo01x/7D2cfO7nCf0J87oiPHoVy0bmSZri6HPB14H7JXG6vY80Hh0qYtfjcYgOo3tfesx5LAnOjT5RyYmSoaoE72s0BEJV8ejFQ773z30fnyYjbcQAACAASURBVP7Up3n2mS+w3SaGfovTCqfKttuglvDOM5812PyQLIGYjL3FPveu/w5DCcyrjLoZXdehOO7fvk7RSKMz7rzwDJcePsByIvYdt64/z+WrHyBujhl6RWtPiQlfIpmKEDymhqtmDHm8qzsfcK7/UlFTCDXmlfVmxcnd25ytzkilsG6NWBxmkV2SAdVAJZBzR9E0mpu7MqexuFrHiEQbhEShIK5ACRTJqNpYLp4S6hl9+RRRAkUCRRJqGSlC2nkY5/GOrpB0Di4ezEAgxUzMAzEaRYxoNtYWCOSSxjtPFpJ6YsrUFTgtLKrAUByQSYPQtj3/8P/+GYIrLOYHhLpCPQz9iqpUNN7Rr1tyUVZtZHduyTYgZswX4H0gR8ixwpyNuewyIw8tsi4oijqHOcc296M7rf2o7D6gWhOHgvmWkj2RgXZ7D9GAujnLPXjXE49z+9YNurYnpQotG1CBXEAyXX+GUlF7R3DGv/Q9/zK3bz3PJz/13HiHzI6S0q5wrFCHBWfdBvoBNUUGIbVbSjGcFzZF0KB4utHlp6KZyZhyDDXqHFED1d4RcXEBW1yk+tbv4v0f+wHs7iusP/NLDGe3Kb4m58TWxpWd0StVc4nl9h7a96w0sjrNSC5YHisXx3oGQ5zHu4LUjqDjUuuFV8o4KtCg7C2W5PAQcvg49eHrWOg52r/C+z/8J7l77Tk+9+u/SJbIzFVUsxnZIm0XaZYP8dRH/ix9e0a7uof4lsOHDqlCxdBuOb1zRjHYZsBV46pFBDE4O74Dqjz87g+xWd0nJWFz8goeWB/fIFtPCEqMFaYLcrmJZSHFjNYBNzukDGuGHu7f3WJ5IHKGOcdmEyFB08y5csGjYsQIfc50KXH/ZEObdbypIOOSnlzGCNoU0XEFK3nMT2gV0FCQQSg6rrK1ZJi4sZRdBa1mCLtaCNl81fH3jhaHUqDtC04KIThMPCE0hKZgMeFVWNZKSnvkridnYRgGnFRkZ4TaUS/GEHTIgjdFSiJnwXKhqgb2945Yb44x26B1Q/FCkcjZOmGWGcyovI6mEkY1awjOEcJYrlrP9rECi3lD2R6Tt0aJmeOuJ5HwjMYQ9ZyuG3BBaGjH1Ff2Y0GXOJwaoRIODi7g6yPa/oxuUNR75s5IUlFIFB0d/TF8jF9aW/Lp3/4kXRsxF1DLxBwRD1YKsQjD6RazQhpX5eB4w6W2YqQiuDiuFxB2nc0VsjbsXbzEposMyai0JuxfBlH6vuXk+jXmLnDW9qR2y3yvYRY8axxV5Vks9uHCFWZxQ0g9lYeZE05OVrTrjKhnKGN1oOm4lqDC8GaIen7gY99Gij0/9yvP055Gnj/5As987r/BQkOQSC7Cvdu32J6cEKSm2i0RNxk4Wa8Jlohd5vTO66xuXsNIiGwhe7rtgHOZ7eqEZv8hUknszY+gDAS/YR4czhtKx+reC3QnioU5NYXaOzabNU49onNS39IzcHzrFiIDHkcRTzLPbL6AC4/R3rxGyYm+ZHw3VtHGPqFq+NBw8ZFH+dZv/x5eevbzqAlPPHGVX/vVn+faK3c46TxmFaqFZAWxceDnnTGuMK4dsZ7SelR2mTwznFOcjOn5R594insna/rtCvWeyv9/dG2FmdFuI2YJ7wVVz3IPNpuI5kJVwSzU5FzoesVXQpg1zBcz5u5hVptXMIyD/QGYEYdxfb3zSjcYbUlcWtT0vZFjTdcmxqynQcn4xZLSrsfqbe945PKjxFTYbO/jKkezPCIyQ32G4DmcGX/sw38UyY6//fP/FKXGNw0w1hF4B6bjGoMqVGRT6mpcRn58/w5miVA1yPY+uUCOxmw2Z4gDczejWDuu5Y8BVwXAU+NQb5xu17js2asKMThm1ViaG8to6tVOiQQ2fUffpXGVq5bdQjIQM0pRgrOx+lKNPhVmM+WDH/wX+N0XP83p5oxsiVnlufLH/wwWQc5OiLnFgO2QaEoCV3FxPmOoluSDSywfe4rN+h5ucwefBuo0sLeIpC6TumFc5+GUyo/eTfBjHn9ZwUs3bjOb77PY26eZbekGIZuSMKQUoKLKA/H4JlFqfBCcMBYODYWDoyX1bGDWD6g3ui6ONSzOsz4+5vFLjxDqhrUlYt8y5NtcvXqJ9sQz319yuuqQ+6c4zfTDwGL/EtSwXW8ITUU3VKS+w6GUqHSpQ4lEJwxDpjKlsUJe3yFnITuD5OmyEUSYLRzD4OjSmttty7c98l5Onn+B5d4eT3zsB/l/vvgC/fV7HOzPefKJp3jmi88yDAPsUrQqCZWKQkXKA6KGkkklg5RxiiZjDc+DdOflRx9jdXKHGLeMH+ZxPu9ocRg/0Wb8gBczI5WBJ65e4dXrtzhdJbanA6t1O5os3pF6z6wSXJhz6eFLvPLqa/T9isVywfpsoO+FUsbPMHFhvFudnd1i3ii6DDiLmDbEvlCXQr08ZHNW6FaJWT3jvU88yuv373P/VGmsx+qBxcKxaYXT1cCRr/md57/AzC/Gcl8xSh7GNFIe30ewOWaeSECtw+mMmFcs55cJ3tFu1whCXVWcrTqgp+2BsqIMmZgzXpQUO2pvNHNlfz7jteNEiS0DkHMmx4zmcY5aTIjOCM4ILtCLjTMUM5w6KhVSShxeusxiOePk7m2ywWzZEELgtZc/C6nHaaLe3iG9uGH7zNNUl59AU6YUI1x4HGvX3O8HLmrFsvFU+3vkJ9/P4Yc/QEkd987usF+Mdsj42nF4UNFWK06PT8lSuLBckvNAKplmXjGb79GWwNPv+2PcOP5nnN3PrLs1PRln46pCp0blAjTVrhgukkqF88K2hVvHLcUL3gWa2REXaqVNx1S+sDqF6/fvEBYB2OBSR7+F9fGMJlSjhyWjKVuJowyRzf3bDD7g6woVxYUG6IhdxIYWdZ6qnrP30FNUOVI1cy696yOc3LrGav3imF50blziXi9Qi/T9gIjn9s2b/NI//Dts68DpC9f4mb/zozR0eF/ISbh77wRPhVehSMZrRU5bxBlHD11is8mU9oSjhx/i1p1743TbCt4pZuPCuVs3r3PlyXdj7ZqUemr/TTYkv5kUTaPzaw6c4Kua/QsNvQ30LSwOjmjPjlGXeeix93F880X69oTf/vTr9F1HqGsuP3SFk7MXSMnoh4EY05jxkEgIgb0nHsLlnr44Hnn0PTg3x+uW16+/iF/uc7hXKDlzvB0wV/HIlScpfY/OA9ktGDY3CC7zsqvRaDi/Yf+xK6xObzFsB0yFYdiQs9C2iWp5gaZxeL3Aw5cvcf3aF+iHRIxg4YhZpcyaBjt+nT7VJBUoRmFAROhLh7PArPFUVcH7gLqCNIe0d+6QcGgJWO7IaVzqPQxwOK/xYcx9V3435xcjG+OCopmnI7PpE76uqMKSvQsHXH36/Vx/9Vni6RbxnqyZsbrXoSoYifnFRzi9f4PcnnCvG2irxDI0PPHep/g3v22f/+3sW2lf/gIydGx14KHLF4ntGYfdiq7/AiUas7phtW5RRrdfPDTVwM/9g5/kbNvy0N4MNFJ6P2Y5QoVqzXs+8H4++KGPcnjxMq++9BKf/OQvUjZ3WLvE8sIBaOC7PvYD/Il/5U9QpOLTv/FJPv+pn2W7eZZBheVyhiMwaIP1A6enp+T9PdanLX/kg9/B3qNPkhFe+uKn2Z6ckGOikUi/sXHBmoy+CZUS6j2a2hhyj7MZ1ieGW5/hvU99gLvXfochZiQnbLYkLi8ys8J+6Di+c51Zyjz90EX6tuXlzUB744sM2y0zrdmWlpt37rPY26N2a0jKhblw86RCBObLBXEYWK0Hzk7vjBEy4zKBPJRx4RYR9TWHWhicx/K4lP2r8Q4XBwMC9d4eFiN9u4HsiF3m3e/9blbru6j3vN5vUVUuXn0PDz3+BPn+59ls7zLkxMwHcEK1uMThI5e58dJnGYZxsIsKMWbm9R6vvX6CqPLcs58jtoX5MnB0uMBLYdMn1mcblvObZC5x5Ylv4fnnPkuNsDer2Hvv07RDoVtviZsVtXrmB0fkoSXHO9ROUV1QRGn2Hqd54kP4xQF68irvevJRTtYdTz71FPNmzsof4YaO1ckd3CIxqw648uHv5PXf+kWGW89huWWuc0gDj11astgb2K465gcX6YbE/MJFFo8+RXXxYW7+ys8xdOlLnzV49eISXeyR+9ukEpnVioaGoesRV6G7Dwd58sn38viT7+PKu/4IpWQuXLjIE9/yPn7hV/8RKzdj/4N/nOrq+whDJDRzmqrh/6XuzX4ty/IDrW9Nezjn3HPuEHeIKSMihxqzRldjF7axu21ctFG7kRqMeEEGIWQhXvkfeODFEhISGIQaGnqwjWzjbhddbavb7irX4KrKzKqcI2Me7nzmPazhx8M6aRBy9QsYZZ23zJDiXsXZe63f+H3jqiSc3edZt0QhND4hqzXLRct7q4SeXnLFCnqgWZQ93q85m57zwqjkztGY8+MZzkXKcsBn/7Uvc3T1kNnJU7rGM5zfZ7K7Q105Rtc/ngE2yylf+qlfIvaB9979AccXa+58+mVeuv0prr34Kn/wP/+XDHrFV/7Of0JDyQdv/ZA/+tO3+Mqv/AI/9dM/x/TkPZ7dfZfBaMjHvvDzvPKv//t891/8IRbF9MF3WE6f433Dxz73edz+Czw56diPh0y2BsTFuzx99IDdg5s0y3Oai0f06wuqoqDpO6qhRvSay7MzpFuyM/ocf+3zn+Nr//tvkSJEEs5pbv/k3yRNT6hWS+YnT9DJMBwOuHH9kIOjPWKf+Ie//Xvs7e+y5XpkIZQ2MtzdJjSenVHishF89Dx58JCYUu5o+YAymtGoZntyyOP7d6HSWAmEPnF6OiPGSDEYEP3yR759H+nDQSRvRBxd/xihu+Th+28TwhJXWR4+/ABpznBlj0pCuxaevP0aNz/+uVzBVYYUycVMo7hx61VQFaFfcllOmZ89x6AwyqCtpqwGYCxGR1ZhSbPy6IMrFJUjEOl1hzMVWiWePHmXw8OP49OUwipMWoHUBNUTZIF1Y86e3GVUOQprCc4So2c4qhntDtg6OOQTf+OXmb3+OmXzkJs3bjEeKurxHluf+GmWT5+S7t5j3+yTRCjOj7m5M4Lhi1Qqb9TNFg3FuCbEM4pRyfWtl1mvz5lOzymsYhhbBrduIv2MQVEy61YoHDF6Dq+UnFx6khJSCPQSUT5wfjZla3fC/q2r9NHz+NH3KYuapKbcfOVzTO58gU4pqqMbuHJAVSQK3zOILWrZUFZDRvWQ0pUcr9fYZsX5G9/j+wjzN/+cMDtlyzdYelbrM9rlJTI6YFRuMVNTYki0fcuNF1/hM5/9Cd557dtIUvR9Q1mUDHcP+fiXfwEVhXfffo0XXnqV5/c/4Mbtl/j8l36OL3/hE/wvf/+3ufrix9jev8Z8/YDL2YL6ylUOX/gUL3/uy3zmzjW+853vcjH3UNREUVz99E+jBgd0zYrh/k1UOaTrPErA2pKT4+dMn82xxlAP93n39e+jquu88uoXOTy8wZ/8k/8JK0f8jZ//CncfPuTxg3cYHRxxfvmnJGOZXDlAUPzEz32Ft1/7l5hqzCoG+m6OWp+zmp5hleD7Fb/7v/0OlTNEk7mP462atp3xmc9+AfEJV1gOJ1f43vv3qNMFe7sG42qePXuOUobYL4koYh+YT2cs12uSNSgfSBqiCjw8fsagVnz65h1WTcl7d+//pe/fR/pwUAqMVdx/95t57FNlUIpRPauz99B6gMVitdCGNdOzB0wvHvKxO1dJwedx3tbg3IDHTx/QrC8oqi0O79yh9wv62RLncogNkcOdMZ/77Of46j/7U2LTYiqFlxU4R+t7OlGMxluMrebK1avc/+AE+inD8U2Wyyf4+Rk6JmCNMT1N32OGA4qiZDiIiHIY6Umn91l9/Wts68h2VXDuFCEWLBZzzv7499nevca//Xd+haqe8Pz+GbMPfsBbjxYoXzI9fRsRqJ2Q+jUgHBx9gl/8lf+Ii6ev894Pvs+4crx7es5Ln3yVk/uv0/WB7aJATMXJyVNCEIzJ03WJvOwUvMdqzfxiypuvfRtrPVE0XhQ7h9dRr32bfryHPXoRuXxMrypSjDiJGIRDFzHhAj0s6PueQxsp45J48pDZDxLdw7co+xkiPQMrrESAiLWWZtnQAa4o2LtiePettzk/n6K6Gb33rJseSbBXl0x2tjl/+gxjDBenT1nNzzAR9rZ20cqxNdllNl+yOJ+RRFjNnlJsXcNIYmAq/FootyYYV2QwrTNMdm+idUO3fsb8wqKwpARiEu+9+T18fSVzKaXi+f3vMT99yuGVFZfvBOLJVehOOZ0u+Lt/73/gxkuvUo4OcIOrHNx6lTh9woNHj/lv/7v/GlEeayNBWnbrkmp1gWaFqyOj0S6z+QU/8TM/y9XbL9E0LZdnMx6+/W26Zs75qsPGNXvjq+ijW4R37nLReJrlnC7MKGIL1uCspUuRskj4pPPWZdxMTlrNcFDgomDLgv3tisL+mM45ABlMsaGCUwgh5bnyura511uN8N0ssy5spg7rMlGUiqKHtg/4PtDOniCS6Pycs6f30WKpthzR93RRU9WOuoLJeELoW5rlgtXFFlu7wtnZGU3T4PuWxewph3sTmsUDSOfYYshi8QCtWrTpiMnjQ8loVGCspXRD6kG9QQBHjCkptypWs2f0MsdMtrl5sEckcu/hQy6nHcTAd775Xa7eeom/9umb/OM3OgajPV6+9Qo/+P4li/kxqQ/0oSOGxLJJBJ+Y7FznK7/8Mf7gd36Tutrm9gsHNNPXibMVyoDvpmxvWVZtRBnFosnzBgPrWOLRZrPYljqaJuRNSFPStzNQQuwautMn6McHbH95Dzvaok5gJFHrxLRfs7d8TDz8FA+On+KbJU47+md3mZ49RvmOo9ogLey6gq4qiSR6LYy2xxxcu0ZyPc/uvs7D17/FtRf2GG7vENOUaCY8e/QOj/7+PXZ3r3Pl4DBvHMaOEBLTRc/b90649+ARTx++yeVihkSd9yS8p28WzJct33v7Oaenj7h4+BaEDukiXdOzOr+PA6yKdBJJFnZ3r/L48XOKbU1dD1j255w8eI9m1XN58YgHD86oak1RVSSliN2a4/sBYy3hrT/ZcDE95WCL4WiP6BLSJMbDPQpnGZoWtbvL0fYB6+kp1lq6/pKdgys0Dx6wnD4lYtjZHzOqKhazGY/v3+Phu+8QfUPXddSVZaAijSuJPtJ/OAojBkeelFQqc0e10QyswpUaEc+Tp89p2x/drviIHw6SoR+iENRmm1pRFAU+KJxLJFlkyk0ks/WsxrctMUVcrfLugo2YUtHMO6xKhNbjxVOWJkMwJFAPDdopep8yaEM7JCXwgZ2xRVKBs2B1PoUrpdjfuYJylmQ9deHYKoV1I3lfwO1hAHzH8uICpROucJQDCOunDOoJW1tjqlIwNmJFKExHap9y/nTB6vIpP/j+Fn/2tQmEjiQtql9S2oKuKNF6TNM2xO4ppydv8tu/9d8zsImD/RvcPbnAuZZnbkFpR4wqwDhwo7yo1S5ZLC+YdELXdzRdYtjl6UufUga+JkjK4AykrgM5w1UFO3svMDm4jn3wDZQrcS98kQAkC5USSuWZXz6g0buE6VOKZk3SitOzxwxF0F3J0JZgLIfDEZVTjA93EXpIl6xXLVfvHJD6CNIynT0hJrh8tmR7WHO5nvLg7rt86Sf/TbYmBcF3LJcNr7/2Ldpuzcnx+5w++iHRN5SFJbYNIaxYzJ7x9NH7vPvDBc38fdaLU0L09F3kjW/8c3av3ebqx/860/PHJJUoh2O6HiIr4vxdUtimWbcsF2eo6Nkb10wmA+xwC9HQeOg2/G3IXQ7o8Ckwrgx724bdnUO2R3cIKfK1P/oGST3gytEVXrnxU2jVsj1IrM/u8c9/+zfzINS6w1QlV298ltH2TdqmIanIiwf7fPL2HX7r//gdXF2inQWzZDpfoozGpkSvMlYw9nn3REmk0BanE8aU1KVBF4lu/VeIifur/AgqL12FjXRDaZS1iARUSnlaMgldG0FFQhQK5VCUOVddCynCanlJ3+WV4RiFkDL4I3WgtCL00LYNo7oiRU9dW4wq2N6qKaxnsj/h1tUK5WrAM7s8Zyot9WSQV4odVNUQSUO2twt8s2KxaJhPz1HGY43ClQ6tCkbVCCFibETR03YB4x3OOl66fQdInJyt0GnJdDFlMq75/C/95wwLx2D9Hm/82e/hTI3ENeIXLBeX1FXF2ck7fPLjn8fbkr/+M7+IbY4pTORee4xKMUNPl89peyFFy9BVlCrSGstkmPKwldWgC7T2+TEXnSE4Kub9BNHookXOv4+ShJgBmAJz9EkezSLb9Nw7OeXB2btcphF9M+Xw6BZUNaltMS7SrTuSdoyKAjesGRYVWjU0fSIoi4oj/KrDFgVd6zk9n6JtwWqZUEk4PV2yNXAUymGLIbYeIPMFIi3d4gnN+ftY1TKcFKiU6UnFcEKMK7rmMd3sCacPvo6rIqNdR2wi733vH1L+sEbZkiSJfnVJ11p0mVkWRE23uCR0LRZNFIsKkcokagtohe/B65LoA4hHR8+otEy2dzi6do1rR7fZ2x3xwgsvMh7V/PzP/CwxJhaLBYtVy6c//UXefOt1VosO3/cIibLWTPYmfPxjn+Hf+du/yu9+9at88xtf5+HllPfv/xHzNrC7s4erJlgJjF0kRI1dK2yTF+pCkVvZoCnKrBcoCnJ3RRes+x/TgiQI0QeSJLSG0BrWqzmEBoyQvFBV5CWrpIgp4iUSwpIYhNRHjCtYrxd0i44UDEVhGBUV9Zah1Jp6q+Lwyojp5YLYez64+zZf+PynGNY1Tb9gtVwzHBYoC4Eekmev3qUsR6ybgCTLoHD4rsM3M+Z93pI7PDrk4PAKz5/eA535Czu7RygEqw3RdyxTYDIu0aZgtbrMba7UUTnJA0W0lLbh5VcOePBUWF5c0DTPCf0KiYHKKSZFQRN6Upwy2b3GrTuf4ny5pLx8m5PpM5aLC0IIkPLo7KQyoDNL03ezvN2Xm4eE2GIQlERcYUjRsb2zS+M9o3qEDx1tH7myO+Fs1WCHR2zduIMPC1bPfshi/hirPUdjzbC9pC8dXnpcgJuTikoCDs2grtgeDtgajTb06y2KomaxmnF2ccFq3lAOhMWsoVl6TKGwAspvsOwSObv/GifP75G8R4vl8tF38O2U0kSK0Yj5es18tqKZHTN/+iaVFS4f/BnJd4yHjt6MuZx1aIFKG2LTEGWKE6hVYmuk2NmegNU8fXLC1avXuf/oEkWHIXKxiqwfBcp6jSkt1hUbkIpiWFjqyrC9vc1kMmR3MmQ0LKmqAc5o6qpmsjWm7VuGg5rj117ng7fv0U8X7Axr1MBinOHo6m2u3vwU3//+t2hXcHpxyq/92n/Ko0en/OPf+x8ZOsVnv/RLfOoTr/JHX/0HnJ28x3LRk4pIkoD3PcpZdBREB0S1+KRZN4pYKGwl9P4vt13BR/5wgMyWtsQQQCLrdYs2idAL0UNdOGSo8EFTWihKy8Xpiv3dCdX1grIwXDncZqsCrfM0Y1VWFCbRB0/bRqKcsz3WREkkc4FXaxZ9gUWxtW0piorzs6ccHn2CuhhxdvqIyWCfZnqf4KdczCDENSJCXe0wGo2YXj4lJU81KBkORyAWrTqscei8+01RVGyNdjk/e4SkDAV1hWE4bDEOlgvF8b03+eP/9TfYGt9gcfpdvLc4q7GmJHVLhJ5ROWC6bgjThzy422DCguOn71EXLqdOSvC+RbzCpw6UxRrJpC1tKGyBqIgwwnsh9HlsXRM5Oz/DOEffNaSY5/gvgGWf2HNLjmbvcXx6zOXJY1595ZPc+smf5sHzx/zL738bQXP16gFFYZB+SLOcc7mYEWLLqulouku2hkM0gcViRdO1GK0YbWmc6tjaHXB9d5BJzTohUVOVjpQCx8cPsz3L5L0PY0o0icJkSI+klr6PNPMpF8/ewJkCpYVCD4lGE9OcSaVIXlBpjSuEK1tjXnnxRW5dfYHr165z7YU7tDHw5OScDx495crBmzz44BFnx8e0a4UrFAGhdJrhAEaDkrKConRU1ZCyqimrEdpkvKHEwHx+Sdss8jo7KbNHgufh8ROOTy5RYnFKEYkM7p7xqU/DL/6tf5fx9ja/89t/l9/7g98lBGG9OmO39vTNOZIUd259EhVbHA9Yd1v5728lKxFsIoUCZ6ALkdmsRSnhRjWg58f0cFAItRNsoSmco7SOnd0KpWCrUpSV5WK64tq1EVf29ymMRemKN966y9Ubu1SVw/uG5eoMHzq6tqceaaJqSFWBKjRFobEm8KGbsXQjdiaHpBB5ev8ug9E2vQtIq5mdPmQqFa1vOD19TPBZa4aKaJ0dDm07o/dngEXjGA4nmbiz0bplvlf2JCiJnJ2fZn2b9kjwWG0ZlCOGNQwrISTPxaNvsnDfy2G+BPpmSV06yqKg2C7p+p6t2vDan3+Vawf7OGMw1nB+cclkBIUtKIsapRN9l+j6DQUpBsCjlCfpAqTBB0WUHkkOHxu0UiidVWuyQaDLdI4yAxbzFQ+f3MMZTVHUfP/d93jr4QMMMKr20NYxHoypihJd96ztkHUvhNiDGCpTcDg+oLAwqBxv3X2fh0+fbJB9kTu3X2BvvJNBraHnyfF5RuInR2lgUhdUhUXbit63mYyUsqJgp9IM9gcMtgx1BSEs0UqBtNAHnCvYGnlC6zDJsF0bbuzssasrBjEhracQ4eOf+xI/c3gTEeHZ4/f5b37jv+Kf/OEzvFGb7UdNVQh1ZdGmw5gRw+Eug8FWXuEvCgZ1boErBd57JEbWzRqfPEYbBoOCX/j5n+CtN99gsVptWJcFN259kl/9D/8Lfu7f+CJWPPfeeZ1vfufPWKyW9H1HCoqH77/NYrrmxrUr/OzP/CKXl8e89+7rzGbHXJaK4+M5oQerAtNFjoBFGfqQEJWw/wqi90PxeAAAIABJREFUykf6cCic5Wh/hNYJhSKGPOlXVdCsPW2T+fzNSrEsFmgbESPs7veEdIEXAybbmqqJwW5lUYs1QhRFXQwZj7apizHvvfUWq2bNi7dfYlgfsVw8Y7K7Q9M0rBaXLJeeeHqBMzVoSMrn7UvdZ8WYpCwmMRqSxRiL1oYYe6wuMEWBdWNcYSncCGMtWaqbZ/1DHKD1emNjAqVLlF2j1YjRsCd4wfsFyUfswKJVgU49ZW2oC8NBUSFACC0SheQV4zrhjM2mKm1ynq4Nznl6n7kDPuQbTUh/IfVVyuVNyJD3AzCCT4mYLE4XQECbNd5pnCkRPUSbAV6X2aalbK5FFCMedSW+N/guIE0ABggFBR0k4fHlmtGgwHUwnFzn1b3baHqIAZTQK42YkqSEw2vDzYJUxLkBoi0h9GhRhNQToqdTDTEJnbeEGLAp4syEoDTKOiQqolvg+4qkLVGv0Srh+47p8TH98QnnpWYw2uP80V2effAOVDVdSPzJn3ydP/wX32LWtHTZksSw1AwHhsGwYmeyxXC0RVmNCB5QGT6k0IgEfPS46PIOjbNUqsjyGxKHR1dZr6ecnk7pw5rBeIc7r3yBwWCLpo1cmdT8+q//Ov/e5X/Ao4dP+OM//mes13O++KUvYV3NW2+9xetvfI/hoOTmCy9yx36a4+OHHB3eZb2+ZLnuMxA3Rp4cr7FOU1rB/RWj6f/KPlopIopV69FiSCqS5kuKJlDV2Xk4cLn91MsKqzRGKeqqRmmIfd5a1NpRVSP8es16lWh7YTwe81M//bcJvuP5s7socbz84ivsHxySNsXOZSusFw1N1yAiaK2I4jGSRS6oLITBDjASkOSR6FBOMM7irMPqMq/J6gKjs29Da4VWgrUZmhuTYHQAWxGj3XRmEqgttOTVW09Cq0QwARUCEoWqHlJJxqBbW4GKRGsyeIQ+L4xpjdHFRhYsJKOwrsLZDOi1XaQP2e/gQ0RSkduWSREBbTXEjIu3WgE9AYWzoI2l1Dn60lZT1iWlLbCuhmrMcPcqw71Dkq0I7YqwmtFOT+gXF1jfo1NHuSmMae1IWuH7lhBXeC84JRubU0FdWsbDEc/OTkji8LHN8F7jMCK4eoDve2JqEW+wqgOjqMZHmOERVhTESDQ+Rw9phUqG0hpsKrGdofENfuVpJTCbrjk9PuW73/g6694zaxpO5sJi3WRpsxYiibU3PH6you8D6VZCmYK6nmBUh9EVZZkR93U9wGpFXQ9wpcMAvumZzS7o+pbe92zv7BAihLRNPdwh+Ja7776ONZrPfuZF9nYn7O1OuPXCdcZ7t9FW8/lPHGC0ou2+wrf+/If8/u/9Ix48eJPkG6wqGQ13ICYKs8aXgZgMZQlbWwMm422uHAh3P3j4l75/H+nDIamErYXtrTyYAiXWBYyrMlBD5zBdyYb/HyF2maxUDYdcv/ExdravUlZD+h6+9WdfY7FcMRpUpDDlrTf+KWVRo7Vw5+VbrNcty+WMyXiM7xu09AxHA+rS4VyFtdlzkVmSeRkspp4PkeNKuewo3JiiFBZtSgaDEuuq/P9djdE6V4vJt3TGim+An8oSYkQpn7V90qN0hdJtRvIjGBzJtChpNxo/QauYqcSSw0ajixxG40mq25iSDCpLNNEmUdgtNF3ucpYGH7JPMYkF69B4qrLEGSGpDMDRgLUFVT3EFgOMttiiRJmKFkewIxpxtPUOzWCX3kyIrmLtDasUaOKIzq+x6w76HhMDJk0zFn7Tulay3FjF83ecUuLGwRZKtlm3PTE22Y9q8p9lxqjadL49KXYICadgcXlOWi/RyqEwaAmoGEipzVq6KAy0p6KnoNjIbm3e0Ow8yhQs1z3rNaw7TRSFpMRWbbh+c4ud7RFt13Fy3PP6D55h3AnjrQ84Otjl6tF1dncVk9EEgN3dXfb396kG+b+7ZgkWzi9PMy9abeOKenNAaEqrKZ0jxkDTecZDh1Y5mpMEW6NqwxtVVGXB7dsv8eLLn2OxuOD06fss2zmz6TNi32VpE3nhrrRATLRdg7Hlj3z/PtKHA0kwqidFjbY9stHLEYSEIElhVUGz2sBggsf7QOwUw62eget4953v4tslwxFcmYz4+ItXGY63QSnm8ykPn91nPV/Se41WHWjHYLjNC9cOubK3zeNHzzi7WCChz4h0k099a7MjwlqNsZaqVJTVKMNv9RpMDSYbspJYYsp6PKc7nNtCm2zntrZEE0kp0IcMSw2xo6q28EnTdzkH7dEEDSYagu5ICaJ3gMYoj9E6k5CN3hxQ2TOpdJlnLHSFkHH9IpqYVhSuIERF3sBy+PjhBmxBSJlnGBM0STaCIBClcmsvRXTboI3DFAq0IqhIr4RkByjVIMsFUSxpa4+oC5QtKFwJRQ19g3iLpIbYN6xTi0RQktAqY9itzvxEZwoG1YhOLGJH4PJ3H1NCqdz2i8FCWpK8R0IgpZR/l7ZFkqD1GqUy/1OkJXYbYpTSGCuUJrHlSlxl0CnR91my04cACZroWXZCl7LjM6bEs4drUu+4eWuPl18cY0zFbN4yu1ySQuDevYc0XUfXtxzsX0WbHBWOthrqeggJBqMt+pjwncf3HmszZ6MoasraIHiu7I/ZGmYbtgCFM1jVU7tqcwFsXpfkQTxOgbE1kfM8/RogxAZlS7RVqCiEJiDa86PXrj7ih0MW5BakFEi9Q9uSyo3Y3zvk4MpVrhweUVcDfusf/T2eny1xOlLYgtG4YjwAwyWf+Ngh49HLJBVYzJecnF4yu/eQRdORfCJGhVUGbXpAYW2gCTPems+o64Lr1w44vLrLyckFF+dLUmqBij54yqLHuAlFoXNeHgNaBSKQuhbtUkbci8HagEaxbjusaUGFDVY8pxiSCw0ghpR6nMv+iJg81hSQegSTXRi2yjeNLbHtBYUbMhlNCAlWbb7xYhJS6klRIWLoVZt9BpI5nNnI1JDSZhJSNDHmBzMkT0oRTQUm13uMMiQFxmQZsZD5AEarLCpOAaUVhQbBo/slrDTEFmnnOEm4vqGPbRYGaUU0GgwoV6KSJ2lPCjHr3mIg+J5eQIqWuhBmyxnr2RnRCEbXG6aBQieFSL+xOwW6FDApQRLUlkPMEKRHwpIYUz4cYo6ejM7in6612LgCbSjrGusKYhsIXSKExLpRNES6mM1oWinWq56HDy85OV+wv1dy54V9RuNdqsOSti148uw5T56dcfuTP8tgb4dQKE4XnqfP7mZfSkrEFHONpF3TdW02uMUc4ZWpRp+d8PrrP2Q27Tg82qcsHb6PYCqUK0mAFiGKIMrmYuhoH332mLaZE4LHqJKoEkoJDo2yBq0ShYJ69GNac4gxMXveoUzFv/U3/xZH165hbImSSLOeslqdc3Z6l90dy7DaZTwUxpNt9vavUtYjmq7h/PSYB4+fMZ8t6BqPpEBAoYKAzmLVqAqUyqjvlDQxZ4Qslw3vvPuYegA3rh9ydHSFk5MTZpcdxirKcohxoIikqDGbMW9BoZWFvL6AYoW1A5R2ZMR+QiRsiExm007KcFVJCqEnSY9ISYqeLnX5RZeN8FYy9k7FjiJ4UDNOLi9R5KEvpbI6UBu7KW7KxpBt0W4bnXzGl8dAjJshGRXRFHktXOtNGJ7FKxqFMQ6jBZTFbDQBSQw+KpCAKLJlW9mMyIse8S0qRVS/wkRFlTxVCkTV4QtFlIJOBrQxC3mU7tHaoiWStEYo0FpTV1CXFZeLJrMaQ0SpZTaSKb2hkYesPtwYoZTOBe0mdATxaAIq9uS7u8ur7lpRuh7V5IN91a7oKTFtjyUSo8Z3kb5P9DHSx5jN3Elog0UDLiXSMvKsWRAu14z2LrBW6L2iVwU7Vyd85gtfZntvP4+6T085XfwpIfTZ6O17ut5nubJfE6PBhwatC3TrWM7h7LhCK8N0NqdwelM0NkTxWJ1nd5arngePntM2K6yBsqjQxhDEsG7WWJU1DUvWjIYVO5OK0SgR/29EsP/n5yN+OMB0DWXZ03SnPLz/hNXygmbdEvrsQBzUmts3DzAmdwaOT+fc+967hK4jxIhohY8BE7KZSLTOMhKTletKV6Cylk0kUitFDA04jd6Iu9pGuHv3OaNBye3bR7x0a5/pbMl8cYZPDUqySkgkZqEpiSQJIwXO5nYXSSM4UD0xAEmhrSKKA9Vt8uT/K/9TyZDEZ8T7BnOO5JtGZCM3iYDPbMo81p2xcNn7YDCojSJPIyhCApM6rBkQ9RpRHm0sKoVNNJAtulYViNZ/QQ9KEhDVoVWJUhv5rAKhJ8SsCRRtQFskhWwVJwuCUEDIwlt8h6QWYkBSnzkIRaBWgkRH00aSavLjusntEY81Y5rOs+5a1AaxqmKEGPLvQ8QQIQpG5UK2MoLTht5fEjoDStA2RznaZLSetRqVADokaboORHUMqgJCwpCjqrbvWKcNABaVC9YhIgJtEGyZW9nrVcK2M7TRtOs8sVvdP+PB/f+MOzd2uHX7BjvjLerCEeImvYsdIaxzBBFTFuGIJkUhpgVRIkEe4kNkODqjLAeklHBFzXK9Yt0EyqJkNj3n6eMPuLw8o2kXxBSoXGRYkrtnRCYTy2R7hBLDxeWa+w86PIof9flIHw5awaQ2XNnT3H/vdepBxdZoxM5ki6JwdL7j/GLBBw+fc3YxxXf5ZVbkKUCjBW0USTK23JZCSgkjBqUck9IRiax9D0ZhVLYcJZVQEZJYtIoYpdE2vzzPjqes1sLe3pj9g5eYz+dcXKzyDUyDiEebItumdLYVoQZE+nzjK1CmQ1OTUgT0ppw5IAsSN6mFyhGEwpE2INYkBqXUBlEeSNHjJCAxG6QR8ralfDi1kcGtmc4qOcVJkShLtM42BBGfC6GiCWKzIYmAVjp3PwhoMYgYQoqIyrUTpTRJZQ8jknmbwZtcFU4VJgkuRZT3JGtR0W/C/oRKHjYtV9lopZVSFC53SsQJ0beEpJGYmAwLom/RdBQSYNN5wWZNIskQVRa55O86ocUQUkJwaJ1rV0qy1VyLy4VHIkHAY2j7nlXv6RfCpEiMdcRZTe8T65hrPj2boQD1oU0jggajSnbHhsm4xhbQ9YrL8xWrxlOXcDDqOdh1DCqhcNnoJUSi5A5RPhQ6glfE1BNEAI0ERx8DXa9p20hRDTEmz044VzBeb9N0Lc6VrOYXnJ88ZT47Z7ma03VrtC7Z3iu4NRSqUpjOGp4+a7mYZvSisRXoH9OtTOcsH3tll8lkwPZkQghwOm148OgBl5cNs1ULUef+vugcOovaqO0DXsBJlrRqk61UKIWIxTjNeDLi2dkMUQUSWjDDjErHkjbKMKM1de0oqhJtckV8sTxl2czYGg65erTHyy/eYrY85/wi0PWSpyF1FogYWwMFSSKKgIhBiwUd0LoGFCIVTgXatEBkIx8RndF4MdcdZKO3i0i+eUWhtNCniFE+uwxENm6LTL5GelDZ12HFIdKRfGY8QkIri5aepBMhZQ+ERmfzchK0ystYymji5ndXkkhKoXTKEZDOrb0UI0mEaArQkaAd3tZ50KasM9MxBFJa50MheXSKmdWpFTH0aEz2N8R+Yx0DTWC+mNN1AS89bIxmSeWDVaTPgN6kEJX5kTqZXMtAEWNEDIRkKYxCJBFTh6UghFyQFEK2jKOZ6Z6ph8oomnXcGNcVvVa5CBo3WkUBtckjY+i4vHCs54JxGeTiVOL2zQm7kwHXrx2yf/U649E2ZV3Sri4R0SgV0LonSXZrpo1HG8l27SiBiAXlkS7LnpVKlDFhrCVGwfsO33tW6zWr9YLV+hIlLQd7NfXRETGseH58zgf3lsznLYn83BCF5DuU+zEFzFZl9kjce3zB5RtPWK36fEOnjQlL5fsR2Lx4atPe6vPLmRLJKLwAvcKYCKLQJtL5yMPnlwQJWCyiFck0WDFoEsaVOKsYDQvM5nD9UIgiOJQ0LBY9y9WcreGIq1ePePnFO8wXK84vp7kabDaTkSpbkCP5xg0poJzeVJoFtCHKGp3yRicpZZkqY2KY4QO5Ki+5eJQ2YFWJAeUjyliCJKxS+Wd8yN5M2RoeJRdbtegsbU4pvxAbyUnwCkHnHZa/iDkSFA4jdpNnK5L4ze8SUEoIKUcj2ti8mSk5MsnFlwoVAkppvJ+jVYHoIredfYNN7SYiCtm1IA5JTTY4aUVKBpM6BGHVBppu8RcRRq7p5AUojSAEUHnC0yjFhgFIEo01WX6k1KbluSE4S0ybVWadJ0WtZbxtGI8tKQjzZaLpA9YaxpOsAGh8ZLromK880ftNGgeoIv9biKB1YGtkuXFjzPbumKLcowuet+9fslhcYqxmq4pMBpHxyFIUJTF1uTgsihiEkAJJShKK1K/QuiL55SZiqknJbw4WQ7te5ratK9k/OuRgt6KZa86O3+fJsznPj4+ZThvaRV7ESsmAydxPrU2e+fgRn4/04TBfdHzzz0+IkteIZcNS1GR+oYjKU4nJo3S+zRQ6V2ZFUEmTk2kFKhGTYLUlBI8kEAdWQfArlM59cApwzjAYKKwtslZM1GZCc6MWkx6iwRkDSbFervjg/j12JltcPTzgyosvc3ZxwcXlAh/aPNOgDEpUFrdqRQygyQ+/SjZ/WaoAIlHIKjMuNmF7fghjCvnlSNncnPqEjZ4QwRghaINOOacmWjCbmxWNpEwhFkmIWpLPoIKYciqjBJwpN5xKIaa4qXvktqlSij6kPBugyPsuEZJV2RW6GQxTErL2noTWJViHIuUCHGzI0BGSB4mkaCC1uI0f00BOPehBkYW4hI3jwudngXzIIwYlLsteJP+75WG1fIEYFYmpyNGUVoiyKJVVdUgeB9dWMFYzGGliF1kss3M1irC9a0h95PyyIaZEUThKLQycYi0OSXnSNpJtZDZFojiGA0c9KGnbFSGVXCw7Vm0mis9PZgyKxN7ukCv7I7YmY6yukQRBIjHmTeMkC5QaoVQgdHN07FESKCuL0QpJnq5dsL+3zysff4mXX3qRk+fP+ebXv8b9R0959uyc5WJB8AqrDaYIpACogPgc8fQRzI9erfhoHw5CyoUbcqHJbKb2ROWDIhHRyWN11oonlXKur/QGfhoxKrsFUTrfqEFQtiBuSEc6ZUuxKzVl4RiMFM4WGJPbSZIybCbfMXoTp7h8iqMRIj4ZSjFcTpdM5w3bkzlXD65wZW+b6WLJ7HJF17XZVGQqlHQAxBhQlLnSrnpERVLMynmUI/ieGDf5Z1L51pUekQFChwA+CkYnROymNhBIKbsSSYEoDmsSMbW5cEqZtzJVnyMQybUUUTG/lEplV6PWpBjxIdH0HYU2eCWEkINfm6BXgkkCrkW0zbdS+lADJMSU8u+p9GaoqUVS3plpY4+KYVNxEYIEjNKb6nnclEk+1CEq0B5JFlLMKV/SaBUR0fl5ULkAmUssGy1cSkTVEWKLoyCFVT4wyaIfBUhYY+2ElDraGFBGGA8M9diRFHRrAbNmNhOWTYfvEkobQso3t1I5nYJcr9opLeNJBRLpupbTy2OUGXF1f8LuZJ/08oLl/IKum7Ne9ySZMxxGtgZbGOlZ9EuShNxBkkWOzDabximuIDYo2WMymvDyyy/y+S/8BIVRfPMbf8Lrr32Hy4tzVssVYNA2YV2BE0VSa5QvCV5QTrJ+MYD5/9mV+f/ZRyTTkUUgxQQ6a8AiuZKtVJ4GjDpm/6FKFE5tKt0etCEEh+iEMY4UQn5so0KCIeoM5yisZlBnaawSRQhC70HhNy+LwegWkYqkQULO+cR8qGCPhDZhVLZYt23g4mLKZDLk5vWbbN04YLq44PnxU3yTMDrkTgnkW1QbRHlS6lHJbsJHTfB5wrHzJufoKBK5diApZmdnzC+hyww9rIukkKUn2hQo0mbyEbSRHAn5DvSmMBYj6B5RFk3M9QYsMeUcP6TcNVr1HuMskrIYJ6mISEUwPq9BEBBVoKUjqMyAMC4hvc8RXxR8ChjKPNeRAilGkA5nimwwiz3EJvM/CeiUiFi0KBQmH4wpkiQPYyn1oaxIEKU2U7IaLdnZYCFfBmI2bgwFJKQTQlBYY8BmWW0KHUYlCg1FZZAu0sZEv4wMrcbVmk4cF7HFB41WRVYZqPwsapMj2ZQSqy7RpwaJmtj2DHaE0AuX01OsKXJaFzQ+tEQfiR6UrtGuom2nOSpMoFSOSJTyCDVF4akKzc5kzPWrV9Di+adf/X3u37vHcjWnbc5Zr1f0zZKmmdP7FaE1mYeZNBI84vP35+NGSi0/pluZIgovOmvJlSVubg4h5GqxqFygUnlgiuRog8qLUZvc1SoNShM1qETeopSILQak1JOTcOFikUhqmYd7Ush9fqNyThoa0D0q9GDyHINRBmNyKOi0w1YJi8LHJqPHnWD0JfHbT8n3it6MW4MrHLFfbQpMWYqK0oTkMDaiY8gPBTbnl2RgrgFCirmNqC0DGzAxQmm4Mh4yGG/TtRdMzxdom9Cs8+h1jLmdisfqgBZHpM9dGGdRuiBgsLrPt6mYv5inEO2xts5RR5sIojE60fcRlVb45BEpSAqsS6Q+0kdQumLr8A7N7JSQAsuL01xIVWaTGuTR8GpYMt69xXo5p1usUKqnUJqAJ4YIypK8RjmHThFlhL6PuDJPgOZ0w2CSJ2xqUKWOeXu0i1TGUOmCg3FNpTTn0yXnbcgFOekQrTCyzLs4RKQDKRS1sky0QSVHUIbkA8rAlYGhS4oUIeDyg+qqTJVSkX6tuOxu8uKnv8gbX/sDEkPG5RHnP/wA3+excNSHYqMSpQcUpcUOnrGerbNU2OaiqSiIvaKwir7LZnFtHUY7SGlj2haSjqikSckT48bSnXwuepoC6btcwEyRbDCqEQloEzBu+0e+f/+vDgel1Dbwm8Cr5FjyPwbeAf4BcBu4D/yqiFwqpRTwG8AvA2vg10Tku//qH0Cet0eRUvYjaJ3zUCRmE3PSmwct5TATR4q5cq+AIF2uPdATMZhkEU0uCm625URSridonYuHYnNOyofINIWkmkTEbPL+uOmORJ9xW6qNJBuJotAiBA/Vpm3VdWETHiZCEkIQgtebSnFm+IkC5Sq0zrRlvICOaF0jNIDQhw6dSqIRdEoEYyhsSfKbn6tKYtAEFFY2W4mbyTjpe6IKJJPZCCgyX9DH/OcukUINWkipQcUcsZnkCMqDsoSUcg1H5WErUty8JD0xQlGMiM6QwiVaIs38mLbr2Ln+CfoOltNn+OQpzKYbpAv6VnH+6G4eDg2WZOPmhc+r4kFJbjnGEomKIm3SjyQ40cTgc7svKepJCb3C+3Vej1YBJyVWEhKEpm9YNi2pg6B1dqpiEJ35CYZsctfBIMZTFlXGq23eFDMAtbTo3uNJKGOJIaF7QZu8txG04vL+e7y9XLFeLKknNVosTbMm9D4XnMUhmxTIuhWxcxwdHXL+7ISkDeVmRR7Jey4SHZKWxOBJfd7iJeWBMZ80ymSymcT8zOZ3Jbe/bfREURmSo0CjgAal6/+TujeLtSw9z/Oe7x/WWnvvs885NfdMskk1B1FiKJESISlSFMWQlCBwbAROgARxjAAGgkBI7hLkKsiVrxL4JjYcJIYyGrEh2VJkGNYQyxIpkeHQZJPNZk/s6q7uGk7Vmfa01j99ufh2dxRELSdRBJAbKKDq1Knh7LPWv/7//d73eVH1+2PrH/96f+/k/7PXXwf+kap+DPgU8G3gPwF+W1V/APjt/a8BfhH4gf2Pvwr8jX/WXy52IN67Bo0A1GomSMDZV0/oAyplfz41YcpREddo0syWrGaQkaogE1AhZ/MkiMdHj8eUetEIVfeec09S8xC0Wo1W3AK+OmqttFyZd7aAvasTaHNUEbTmfUN2Z3bgUkl1NF2BjB8ijglRE4ckCH7mkNgRvN2kdq7KNn5SgWZnXKkZh4Wt1NuFHvqC9xvETWhrNhqUauas0CjYopabo5W0J2f1FDqKNkoOZi3OAd8ss9FE9xdxRtoEOdFqouSEaiXtj185my50eXHBdHmJqpW6DAdHzBdLzu+/zDhdUGsk+BlRHH0cCMLeRhzM5u2UUh1FHC5ENArB6d5QVgmSESohKF4cq93I5j2gSYXUaJpwAjS7JlqFWgqrTeFssyE1pZAobbJxcbAgWhRhcI1BKtFZCnXAsRg6ZlE4mBnl3OMIBGqCCPROaVKZSiEVD9WOAtN6RdHG+uKUB298h5pHEFtQcNUEcy1IMBDM/TduW1O4dGStUBOtNrzLtLZGNdCYgfZUrTSxsW9rFW3vmuUMAyc4nDicl/2kzMbSXsQmbXutBMTG8+/z+v+8cxCRI+CngX8XQFUTkETkzwP/wv7Tfhn4J8B/DPx54L9TVQX+UESOReRxVb37J/wrtq0HaBXB6EWtFZpzzOczNAS0TWgR2xKLnfucOLTuBSNv6UehgRoSzXt7GmqrUCNVTDNwtD0ctOFkANZ4Gs1Pe69AoHlH8AnU3r7oLXQE/j2Rs4s92gopb6BFJM7xDRPVRGlscNLjQ6D5gMYBPxxAddBX5vMjxmm13/VM1FosCVpAZLBtfjN126Fo62nqbVRLT2CDF9DmoWaaM0dmy0JzEa1Wwuqd4pq1Nxt+vVC8HbWCiIml2TSIVsxDUbxSa2ZKSggzSnL4OOHw4D1TMj/CuF1TcmN59UN0/oKLcpswzBAZmXYZT6ao2iRK7MnnnRJRo47ToEIUpTERFzNSUtK4pZbpvSPaEAJOPK0VcFgpsAdpC2iNRGCjieYDpWaaKk69LQoE+t7TxkLnAt4lQ7n5nnnv6TtPahkXPJKUwVu4bwjeDHXDVUordHVH0UQfOzZTxR0c4bajFRnXjHeBqsWmKV3PcvkEu8v76Lij9sHEbhGoG0yKF8QJWmxU3RScG+16aREtO0oFcREfOzr1lJqR4I26LRlpdmSVEM2ARsW5iBOPojhXaW1637vvT3Os+BBwAvxtEfkU8BXgPwRu/ZEb/h5wa//zJ4G3/sifv7P/2J+84iL1AAAgAElEQVSwOEDbq9jqZG+BjUaHpjJtE5AACwLh9/OE4NBqnQjgUNf24pdHnYl1VRydip3ZVHHB4aj7bRc058ltZ2YX5y19GDx1MoS79wOtmp8gxAXiPJXCTBy5VbxXKnbOdy4gUvAuUGTv4ixiKn4AFzp06JDFFTQ70BXSzYyAXSqSihmGakSC6ShqdkB87Ehtg4pSgKye7DPaIqV1BMloMetwykrwBTSg4iycZIN6pDV8V2l02LxB0QKNApqQ1jE2M3L5ojgdaLoh1wk0krN5HyQUWjEreq3WNbI9fZ1chA9/+ifRtuStl34LqeP+ay1GRIoRcck8JcFDVlyxLe+oDi+OuLgG7pyztdL1kc57BgkIxTqBsTizEztuNJnoXY8QSK2iRSlVSFXxKJ0XZkPPwfUFi8URbZMIfeRoeUxP5nC2YN57duNEarCaAro7o8kb9NNknpIrR+iYWV2OeCJO4fjoCh/9c7/AV3/nH3Fx8oDHn/0x7rz+PJQdtYyQJi4f3gEgdkIInpozpWWkCd6ZEF6Lor7DaTU/jwRqyyYau96OuKHQi0An1FFNXwpC0BnqKk4DWSdE3H4BBZwR10Uih0cHrM/++HvvT7M4BOBHgF9S1S+KyF/n/zxC2A2rqiLyJ4Co/u8vEfmr2LEDEcPR2/VbEB+txg5BpVkiTRShY7Y8YLc6wYl5Birga7Gtt3SWk3CKcx21JksSviu6yYirNvJUB80plP1TNVgASdXT+Z5JlaANFzpEMiH05LqFZh6F0uwJWovDieJdZ09A3VCa4Hxv58795KCqEnwgzJ/APfYxvO+YHgY2q0d0nVDSFl8TFKG2TPQBNJuOKmb2mfmOGM2LEfBEQPwMr5nckiHmzbdNKQVtBfWNgMdLQ1wAbNGNwaYdeI9qJbSIi2YvLkUIYqAVFxTVaJbqVgA10bdaQ7eKUPOaGOfk7Ln61Mf41Od+nt/+e/8NabOyhKkzcKx6bwEtp5Ac3nlETGEnOLpSaWnH+OAtVALLWY/21i1aimVOEEFzI6dKPw+4EJBi/A1kImsliBBKpqoneKsZ8D4xHN3gmR/+Meou0/VzDg8fY7mYcdXPWLjAgzuvwtERJ7Vwfv4Osx4uv/sy69GRdxsAxgxaE63MGdzIWy8+z3hxBiXRxjOkbGgl2SRNBNSi47l6wn5HRluB76gt4KXacVMz1WRKc6kWiEGtOVsaQ/SMCcjVjhB4ezuiuS6lA9/syOhweK/EuNtrXodsx/S+9+KfRnO4A9xR1S/uf/33sMXivog8vr+5Hwce7H//beDpP/Lnn9p/7P/yUtW/paqfUdXPiDj7L6oDzKKrzbZc2hziwPkZ2irb1RniOjPNOEfnBMTGebR3ISIOrcmmA9XhnMN14GNv/ghTBW0rLkptnpJNfW9aaIz4TshmcLbP9T3OdTTncXTU4sh2DdDUgbPEYiOA9GbnbT24gdyAMIPFEq4/yXM/9Ytc/4lfZP7sTzBc+YAlCFU5WB5B3+G7SDfMUYlIjSAB13mIHlUbQaamiEy0OlJrxbmOgWpf/z627VWIOJwDJ97eYhsWvjdCq838V1UmWnU0HNEDtVCLswBYA18ULya0+jjn8aeeI+zHhr6fszh+guXVK5S04Tf+h7/G+cOXKYhh4wSqBJtIeMWVsHc2NpSw3x3ZQ6I4yznkmvfAlg7D52SW156h75aoi8wPOro4I6iCi3vNwuOaiczVKbiCb5U0ZXabHenkAbvpkjqdMd7+BnrvizxeK5997of45PFjPD0fKG9/E3f7qyzuvcaw2iEVpFXmy1u4EIiu4Joj5x15HFndf4ecR+OK5i3Ne2gFLzanVE0gHicDZVyRtyv6/piWJ8MGlkBDrZMlJdv5NkFEScUEZR8MGNuaYGQy2z038eZqJdEfzLj1yZ8lBL+3BZhBsO8rrl1C3f7/vzio6j3gLRH56P5DPwe8CPwa8Jf3H/vLwD/Y//zXgH9H7PU54OJP1hvAJPWCYlZVgKrNPtYKWgqUrQmLNPZ2SETeVSospFOdzaOFYnShvUlHEILAfBAWw2DbeBK9D0Tn9xHlvRfdC1WbiVL7cE/wQiDbArDPCwTpLKMgnlwDuUVTv9Uh7PDO+mVEG0PvGWY9s8OrPPGpz/Gf/ts/wr/2M89xPDtiKY1WtyCJslmjKeE6ZfnEE9SarA+jc7hFZzsoEqWc0Yqj1jnNRSQEpFYKDmmCI+Kd4Du1dCKKumDhG5/tGKGOrB6vAdWASqBQyS2bw7NZSavbcyGyKKkYRCbvdjy8+yZNwXtbTJbH16ilcO/15xnHNc3N0SD4YG5KbTClZiPqsD/CuQCdoM6yHLVNBIXl4XUOrtywXUmaSCkjwxH/4r/yb/D4M89xMLedQHTvTgUMBOMk0xgZi+6DWIHmTB9RcWg/w81uMZ6dkN56ifjwIR9eXOOjP/5xPvov/wQ/8okf5ymZcWU8h9P77C7uWbCvd1ze/Q4+2q6zuUrVQlNY3XsDGTfUMrE6v09wvYnDrYDsrYnaUN2SUqW2zLi7wInS0s6u79qgq4ROTWRvZoiTd/MjatOHrmv40BHcvjNDlVaqfS83Wx59+w/QqZp5wmdyc+QyozaL8r/f60/rc/gl4H8UkQ54Hfgr2ILzv4jIvwfcBv7S/nP/ITbGfBUbZf6Vf9ZfbnCSAQFam3Ce96yxTqKNIJX3IslePQS/DwUp6qwOTdSBN4CJVm9Blpbx3lZoM8BOBoXFc9ALu2QR594BoZBrILqBRkaC5Rt672iSbCXXTK57+lLrmFKzf9sphUzwBoXtu4HZfIDoCH5Bdj1Z4ezkAX/tf3uD01fe4OHrf0i7/6pxC5oZgsQ1WvZcnLxpzV+F/ShsgZ9lmu8Q56jtnFYqzjVKKESxZGbRAa8jPiq1Kc4Foov2xG5iPhFfMfLe3HYareGDLbhmFW/gjUWhIeKrRyt4H4012YQQI5RMPxww5ca9N15gvFzRd5HDq0+ybbA5ecMCRTRiVIJnH+ryFO/wZDRlvNpOQp2jNsGYrZFcHEWVIfZ4r/z23/+bkLf77s8ZWmwC1AlEMVNbSVDrCDhCaDgdGJzQh0bLE5evfhE5O+FovuT45kd5/NbThOs90jke//nP8uHb3+T+t95hKh1jCYzZU1zC9wvW9+8zc57JVQqR1CaqC8bFaIW0PjEiVqvm+m5gdTuK1rCPnDukVQt2ubhniHqGGJiw45K92YIQQSreB1rKaBBKbeD2VnHMYRmcUlIjhi0aTc+Q5swsVhK+C392DklVfR74zB/zWz/3x3yuAv/B/8t/YJ+mLAYswYNWvLdnv9lXIUQPeKTsDUJ0zAdPyqPNrp0y5UKjGXPQNbwEchNcq2R1e+CqAU2aLvD+jFKLSQnF03tPF4RcA2Xv4U864RGCiLnaPJRUKKUiQelC5bEbN1gcOEJYcO/BObvthvV6ZGwFdIObzWjjhH/li7xydpfBDfjTN4jOjE7SJVoyZ2jJiekym2fCNZoTpnGLFgUfzHrsEuIK+IhTIWsxgVETnW+U5nDYIlukEbztRhuG3NPWiHGiNDErdjU+RaWiGuy9ax5fMxnBeU8QS2o2gRgjeSw0Kl3nCSrIfMnVK09S1meMj95Bk8FrgoBqR271vUSln0fbRWiHUveBL2NwTJuHyBjwLhBjxbWK7kay2v+vVaUUcF3F+UBzhXGc6JtNPkQ985mwTeDCSJlAcke/nli99hrzvoN5ZJfOOXn4gMfvbgmPzUkPzrn34A6nl4U2v0l2j8jlwjB0hzN0u2MsjVTNCt/EEXxHk0qeKr6ZLVxcv98J26Dc4anN8kHivE2LvO6PeyC+34+JPc5PJoQHKFXIU0Z8pLgAzfQE54tBeFq03EpRnCQSvfFAnN8niy3fozUi0/dpZFvEbNONZjZlxbbA+64IELQI3puZSTB4R3OVUmAQT6FYWlAtsKVYGtKLuQ3BUnsh9uRcKV6o6QLf2VMnJ95jKFQdKVXNLhw8tTli9jaBcA1NnlozIMxj5NkPXWO3m3j99kjVU2oxTsJsMRD2piY2hpHXeptIYXbtaUJQWplILZnZSwFn+RLwOBVUKp1XFn0gtR3eRXrp2IiHCLntCDIYoJWMBKVFRySgzRM6NTp1AA92kbr9D5zlWJqxBzvviK4aq6pYAEydI7hGadWgOt5R1KGbR6YE7BKeLeHgkMaGB3deoqUJnB3T+tjTmiUQx7HQ94HZtRk/9Qv/Ft/43X/I+aPbKIHiC1FAsGdtKZUQPVKNqTglcw7W1qi10Jrio6dT3YNxAl0MyDhSRMkamHWVbj6wnXZoTkyTg7EnXG/sJuHum6/y+fyreBFuHF/hu69+mZfe/Dqnl6f4xz9F1UANM6ZpS9d1uCHiNqM5PwWkenRKmA+0GNKtWXBKxeMoqNrON7i2h7sYgMZ7c/HmClRHP1TUJ5yD42Vgmqpl1oIiQUlTJvpICBVxHXhH2TWkmYUc6e2Y4RZotRBd6CPqhZozWd6fIvk9vThYV6bDqXUlIGqiorOJhYgHV2zW7wX1FcXgnIvlEkmJNG6REHDbSGnNcv/OG1/BA80Ro+CoRMVcgCi1BEoxEWd2cEjKEylNOA3vcR5chFzNFFOa4EKD6ogOnnzmKqcXF9y/t6aI4CUSHEjXsbx2k+3FmYWxXEHyiBsnFnqLttuxOjtl3gWi5a3JAk4bmUb0+94DF+n2i4brBsQ1wExPIXhm2jHlhN9fos7vx4EhGHA1KIPze4YiDO/m+kPEqWNXk0XZdU4ttksQNW9Ec5byECzV2KqnaUCk0scFOY02NquZOETaow2xO2K3W5stu0KrVnPYOTg8usbh40+wvPkBvvH8V7lYPwBxeG8E7hIdWmA4OKQ/vsHqwR2mMTMmw/6J9wQp9F3E+f0RqdmWaNE7JJsZagiOMY/0vYdNwovgfCSPSt8npk3jclvwqXHS3+arL/8uh27gzjsv8MbdO1ymgD58wDRtqDXRtFIu79Mtn2CXG6FOWAKm4n0jEOj7nlQaJQUT0DENR7TtKVRmi3/3AeiJZNQefg7jabSeOEtcbDIlG5fEBTHqlkBrxhEZZhY3R9SuOd4NpQleMnEGeT+ckD3S0LXv02yFCHhnzh/dl4Fall/3CUlDuluDgIFiZ1ePuPnUR9g8eIOSKiFGfOjIc6XtMgUb0akIc9eo4V0mgJrlrVnTtgt7+MbedUkD77yJOAjgkZxtpW+ZpmJZBa/M9uLm+XlFuo5QFQKIDzgfWF2ek9NIFzpas6dvpz2x7fjQhz/O+QDv3H6BplvGUWhs9k+C3vzxWkjqqa2nFSW6bFqscwTnkNkCV3aEtsETDN/mPT4WvIM49HSdEuOCXCZ8bcQQbGvrPeRkU5diQmH14KstvI29ddpVfPC45qlUgq9UVfqDY7YP79FJImXP6vScmhI5n5hGlNi79ZoZ2sSTy8jq/AHnDx6w215w5SgwzAI520QKlOIdUnesNxcUZ+zKYbCjkEMgZCgVLx6tnqJGK8cJqSb6oUeDMBQ181xOpFqYzXuij4iv5JwJXWN9vuNifsprL/7vHM5mnF5e8ODhhpwHarrHlLfkaqyQ3TaT2kOmlshazcnbKlMVQmcEp8Wio0gij0qVhDYbTNa252cIZmHXhLqIZtudeW1Mu2AaSXCGqVMHrRDF0znPpFZIBN68Ha3aLlBNQOfd1GiLOHEEEeOAYNezD9+nxwoAF81zYOZ7Eylbg4I5wMSZ41GLWY7WJ4/Ynq+ZB+Xw6CYHtz5APHyc+7efJ90/QWWyOLATNMyMc1jr3kXY6FxHGGwUVIpSa6OMa3INBF/Q/dSkFMV1GKUJJYip91T7Jo/bHSUps1lH9ft4txZy3nB8/Sk22wdsT7fmWGPHbBgYz+/znS//CsvDayyPr3B255TobWyXUdMSWsE7TxCFriCaDY5Lh0cJ2pE2K6rPiHdmohJH7zxdhOiUg2XH4fIGw+yQ7fYRY9rRWmC2OMKJI08b8Be43YYkgTpV5L35fEZDJog5KEcs8eqcAVtVR1ypVtDa9cz7YwIj/dVjLu/dZnZwRBgiaX3BVDNdsB0J5RKIDH3cj0b333Kxda9SkQx1ekRulfliZsa3hgmukycAySeaNktcihUpOxoi+x1S10Gt5GQ3VM6NMMt0LiCh23sFCuNmR9o15LHAartjl3fspsLRM08zX3yMdnlK3qxs19oNzI9vIKsL0uU9Qw1WoQBjnghFyamRYW/gw0J1km2CoQb9UYVaKo5AZaC2cS9CerQ2Om9pSpFIbYVxajgKRQN+z0TVAloLPtiNX3DmcWmJLnpKEjOtlf3O2X2f7hwAs7l6R/mj+yGtiOyfiJKJwwJqppIJ/YLF8ojxYs16u2b35ktkXiJtViQP3XzJYnbA5dlDfFyQt2tKGfHSGYQFawXyDupecDOJ2dqXBNtp4J392hmFyPmAZOuBHLoZosFKa1OhNMcQQbyjj3Oe+/TPcveVL/Hq6ZnlH7QnT5kwEzRVTu5+lz5EUjIlnwwuBnzxwIhmJXWFQRNVgTqCrIAZVfYYs2bvlQ8RfKaL1hnRdwdcv3adK8dP8OnP/Ay7zTlf+drvsBsbi8WSp59+jjSNvPSdL3Feq/UdOJCqVtUncU/ZbhazduClod7q/Y6u3mJzfoJXj+4Xs1ytnfzZH/0ZPvjJn+XlL/8GJ689T4fgVY0CPjaqZkopVB9w3trBxHmcjEg2K3Cqlib1zUbXtSTLY1Rlag2nGaERBpsCLJZH9DNPqZ5KBufQOjGf93gqNSl+AGYVSUKoPfPDyLheISXz8KSQNaKzBa7PnN7/LrUUmoPYF1KBnC+oF6dE3zN7/JDV5WTY+7gkl4xIIHamHZVWaMV2rzHu/SXNIsO1tX2o0PwnVa35zKZvjdYCtTU8mdoqfRho0nBOjYlRldocwUe8ZGiCb55hrnQ+MjZvwcVmjuDWlFq/X3cOCk6LJdSEfRTV23m3Cf3C7NFNE2jBOUdwQtptaQ6KOIIPxNkRY0p0/cC1a09z7fqTbL/5RRbHTzF193FppCmM2wubVZeJgvVNiBMKDiSTJ/BuojqPZKWWumdKFCuJa1CrEro5i3lPjMFMT2Rqm5lxpRW0bpjSjn7Yu/hqZTMW+uC4efMq+cGG9ekZMXikFcQLs2sf4OLBbfPISyT4gJfI8uAK03gCIhTNFK2A4kJvqUt11v8RhS42rl6L3Lh1i+OjG1w9voq/esirr9ygGwLzWc9nfvQnOD4+4NHDO2zW5+x2GcVcgCIOaZNF552RpULz5FLxPVSB0wevI97b2Mx7bjz+BP0w4/Dah7h//zW+/Ov/JWW8AGEfjMrsUqOq0HeO1kBdZd5FovMUPDsq4iAOHfPls5TNI9ariZYyU63vbbWLV3pxdD1mMVfH3UdnaN57R2UkhojWRN+ZKJ2TstQeaYFyOXF12REH2I6Ki47WD5R1tiOVH6DzuNDRSkZdx8wFOg3vuV7RLbeGmY2wvRK7K6RpxIcDm1TIRMMyNK2NpCT4qHtrvXlmpHjUGVCo5kzoAq00fDDhtaRIbmY1D3lhR91mvI1W34UdKbUY57OqsitC8A3XC6Ww9904e7i9z+t7enEQEZqDWjKCiU0tNfCWS/jAsx8htR3ri8RufWojpGHBsDhgd3oPV9asdw6f4YPP/iCvfftrXLY7eFHKesXZ+gVyzRzNOsT1RIFaA7WO+12BbRpchJrtDW2tWiio2qSjCUhMRDpCP0dqpR8itQkNxXtPJ8aQ8M5RUuKVb/we0+W5TVbUqttmMkPSjgdvvUbZbik5MWplCI5FD+v7r+Pf5TNWiM4iwtO0opSCq/Z+iVigiJJwPqOhwztrJA+znm42x4WBs/OH/OPf/DsEP2OzO4OwZLXOfP4Lv8tssWCzHWl41IF3EfVmgsrVEZ2xIS0zYuNQ7z2lKSEsqUxkcQQX2O12tLzm9Rd+m9aUGYmaKmMzf0WMDvFCbNAHz9FhT985QhTWu8a42bLaCn0AFyK7yzOCeFo26Es3RKI4k/W8OT9jaHgnlOop60zKjdasQ+PGjSMqVjGnKDUI0s2YH11Bw4YDJ5Ststkoy1uHyGxOutxSyOy2FWlzxt0pLUByHZontruRbcosZxE6z2JpCIH58YL1esPpox3DbMSHynprnA01gIQxKRp77qbVKALEISLBdhDBCQ/XI847Oj9jmuznLWdQIbdm2lIDcRVaR2u611KypTB9b/2qc2N0bC8FXKALA+fvc/99Ty8OTuCjn/w0L3/7q+Sp4dTtUfOAKK+//G1uPPm0VbTXQggdfdeb+u8GSkosrz8OeJ760Mf47ssvcHH6iPNHpzhpOBU8hZQNRDocLLiyiNy+awxBcR6P0mrBq6K+UJoaZ6B466hsnU0UaPTBc+NDz3Hz2i3O73+XrltSpy1jqlQyffXcePoT/Ev/5i/x+V/773nw3W/QQiC3y/dW8DgsEM2kabStojbGYph4dQU1PDOdU5yvXF5ccNhFcwKWDYdXHuPi5G2kmBbiWrbFKk+E+YAjcP/e22Y534x0/ZzoM8NwyDY/4vbtM+JwwOXqhLwnZ9mI04haBghRXOgo2cbHXgIgBOdI4wVdL5QaqeJptbE+f8hsviCmNR+68Rjfvn2Xfubouw4njSOUrIkhDrb1x6Ha0w1QtdI3JYhH82QW4tZY9BEBXA9ei1UNVm+zglrI2UhTLljC0jlreTp+/DHCotpxjYaLB7iy5vjqDfL6IR/44OfY9DPyr/wdWu+Jjz3GcOVxLu98jaPlB5nyhvJIOHrmg+yycPPDn+Tkjdu8/vzn2U6nMFZq7fHe8+RHjrk436IninjPZqOUXPdMSxO2o3OWf8kRkULaaw+sd3SDY3l8zMG1OQ9OLklTojsc2O4S817JtSOlLYrSOQ/JWCHiwEdr81osDpDYWJ2PtCpM1ZkgX+w+Csv3jz59by8O3rFdnzPEOQdDz2p1apXxNKJzdAq3jo4ZnrzOKy+t8F3H1cNDfvLn/iLv3LnN2fkd1rstaX2Pr3/+V+k1IxHG0cpu99EtmzFLYggW0/35z36GP/jqV7goE00Cnfd88hPXKNXztRfvUkvdMw7sCdGmglfPuj1iXG/YHvV0B0f0i8jUjPA7dFfIm3NWqxOkH0m6IvuMwzG4zgp5uwOiU2hCFyOlms4yVcOna1O6YFbdsTXqzgxOKSaCO6SRuXz4GrWZXdsFOxYNizmb1cjDhw+JYcZ6s2OxGNiOhbk0bjz1UR5/7CN869tfYHlwncv1Haa8toQfHqKzr1fB9wNdb4h3agBnGHW8hYVUAyEM6DqRNCGukWtF4pxlrPzkR36Ik3sXnLeNlQe5nhIEbT3TLpu6XpSJxlSENDZLwxZlqpEmmWkaUSJdB20LabI+yOr2ZqBmqDiN8NO/+Je4/uTH6I5mzDvh1ee/xDRdQD2nVGso931EvGd59Az+iR+kpA2tP8D5SMqX3P/aV5hCQPWAba5cPLjPbpyIV55ks1ozf+ZjPO17Hnzzd1jOHa0oFxcrpl3HbmVs0HHKzA9mNsFyaqgADXtNAhAFCqg3c5RmxC9sRN2U4KzLtBsK0Vsqt5WK7I196hy1YsjBZgg4Pz/k6hMLNqsduImWoeYJ5wYzXlVPLd+vxwrnOLx+jdNHDwgu8uEPf5y33nmbzeoCxPEDTzzDw7deNxtrKRzOF9w47Pndf/C3SNMO7ypTUWLnuLrs0CGwurhkdv0x1pcbTjcjmWaRbrVegl0uvHn3HTbV4JuqQoyRp5/5GFoaL3znZB9qUWND7EesIsrxtSc4fuxJDuQRl2urc99sR+aHM1otdN2Murvk1//mf8Gjk7fwraIu4EXwoswDbM4emvutlX3wxgaIPjhyK+Qy0YUOUQuKiUvUHCjV/j85O3j3RtGeMBg7s2aDorx975TOOVquuAA1C9vNJeOYrKim7ejCAX23w+mEhELNO3Q/1u3cnjYE+FlkypXoAs53KJmrN59l9eg2IpU8TmxP7tGmwm56wP028Xd/5x9zXirNB9pUGAalXx5Tpon1Zk0rym63xXuHdwt8gK63AF10kdNHl4Cntsbi6JCWhamc4iUwk8hTtx7nUa308znL4yNuf/tbfOv3/ikSA1ITx9cOOXzmKt47qIKWjO8L/WxJUMed23dZnbxC9pVtdsRSmHJitZuQ/B0u8oxNSkyrS+ZxzqPf/3UYjtk8eptYGk89+1M4t0befo0rN28hr75FcAnvPd7vuaGI+UAUYjB3b2vZhEIsHOZdhwZrpwrDjCYW+1cHRXWvcyTE28JRNUBQxM/RNOJo5O2WwEDXdcyv3mBajcwOO7q4YHHrE7z14hcp+v6pzO/pxaGWxu2Xv0nOlbFWNtszdqNxBEtNfPOt1+l6xyc+8gSvv3XOWDyLw5vUt+9xemZjJhVFfOBwOePjH/kAl5uXeOrZH+Zic8bx+SmvvvEaaRJCVykoq93Im6fn4ATnLP8fvWe+OGSaGtErSR0Bh0ilqgMBkcBu9xC9t6O7esg4jozbNUE9WiLdwYJw2JM3FwyzQ+bDknFcEbrAYujJ25GDxYxb1w95+/4J64sNwTXmywNU5hwcX+P0rZcppVC80LuBGAqpJOYtUNjgKjYro2Nx7Ul2m/vkNJIkUJqSmxLKGsVTVJjpnMlXHp29zTC/ztAvOTjocf6I4+NjQuj2eLORMW1JUyGXvLcpT+x2ax6drShaLOnoApvLh0aE9oGDRaUL8MTTT3B5dkHeTqy04bs9bU483TDwQ5/6Kc5W57zywhcQ9RzcuMLNp3+AfnaTktdom/jpv/AXWF0ueOHzv0ZOmctH99levM7xlYFx11GnwuHQ8xf/3C/wpZO7PDzN5LLjcvUGdVwTqtvH8QNOH0P0km6IqBrdqdY1DsfrX/4N2tkJebPl4MaM0hyX28TF1Agpsx3PKSqMaeRyvcKJJ9VHtJYJNF76yo2ryCYAACAASURBVO9w7doMgpDKjsPjJdNuZHbsCbFjtZpMY1Azph3NPE9/4md47TsvsF2f4PAUp4hGZj/yr8Mrv4mPxp3s5544c/hoY3OnnhDNXl7LRHfwGM/+6L/Kt3//VxinM6t97A4Y5JLzs3N7mE0Tspjx3I98jruvfoN0vnnf++97enEARXKlFWtGKrWAYjh03zEfOm5ev8o/9/Ef5mL9Zc5OHvGlP/g9Izt7i3s7zURVorOtpxfhxRe+YFTl5gjeGIJdbMy6yDhVYhSiNiYXQRzNC5vVQ7Y7M+7EsGcwek9flKyOTqzabbu54KztmKaJ3gu1g5w26OmWHTbOfHTnG7TmEZQ2VbZpg8Nz9uiUaZmZz2Y4X6i5MY0bPIXLtKYUEzglV3Z5hzZFI3jf0fs5ta2IwxHT9ozp8m1Dhokja4FQ6VRpzVsyVfbkIYQpFe4/eJ1rVx/j5NEFIpXnPvKDfPrTP8bXv/5VXnjhS5xd3GWXzI2qtRFUOb56i252zNQc146fJKUNY07MFzd48Sv/hOA7Yj/n5lPPsVl9ifurHR/78JMUDaw3lipdHh/y5ptvcHn+kBBsJF1az60PfJaf+LHP8c69B3z9q3/IoweN2A8Ed8HisU8w7gqn73yH44OBac+cPN1t+a9/9ZeZ9mM/5wvLZc/NZ6/hvVB2JjRPuwtcLEQ/MfgZm7cfoheZ5ZNH7NYPqeMWpbE6Kyxuzti0yipX5nvDUMaSb6puj8m3/ENqQK1IDw8f7bh+dsJuvWHKG/K5gAZEoevNqh+6wLoW7t35Fv2B5/ipZ8irDZdnZ2jN8NL/ihsmgizo4oLaKrvTEamOqoJ6yxt5KglHyRO3v/UHzOee9WT+j+16x3zh2a4mxJnfIt1Z8Vt/+z83Klj7Ph1lGgzFmSdd8z4MZNzEo4XjaAE5X/Lyay+hLTE/GIxlIFbSsqkF7x2zTqm7C9arNxFfmflIzZmH5xu8eEJwLPvAfN4zjjtKq8S+o5aKd0J0NkbNaaSzAg17AjlFnSK10TvLaHi371OQyLCIfPIHnwOpvHPnbd5+8wQXjGZsLooGzdyI4pT1znOxPse7yOCrbamjMR/ztlAqzIdIbUYo7qNj6Gd0faVUc961ZiaxIQRuHB1y7/yEop7Dg5vUtiLt9sLt/BquU6TaHF4VhvkR6/WWxXzJ0eF1jo+vM18ccnD1CbLC9OgBra0NwacKztOHiBZrvPr3/6P/jF/9lV/m93/r79NiRxNHCku+/c0vcvPxpzl5eI6q46knn2QaN+jiMYTKnTfvMcsnbJLj2sIxdccEH/nnP/MsX395wVe/8pvcefFrrDcrTu+9wXAJedqh2rEYep575iqhm3P68B1amDNoITjFyYzYzfBhjldh1ISmwvbhKfPDnsvTB7TFAeN2Q1NFLq2JW2olNyHpiAuVcf/9mqqnj8ICmEpH1kBlsvi+G2je7M6pekvuVtuxoR0l7QlgXlFxdJ0FB1tq7NYXLJ+4zvGT1zi7m7m6PGK6SDhZE2JAfSTOHL1vdOEKRXdstiPaAnkv4Pro0LxjPHsFJ0LojF2qbaKJsLh6TFqPCI5wcERdPyIOQimB3fvcf9/TiwOIwTSreQoU8F5QdcQOOucYZh0SCkNUumDR39XYcFHQnTLvhOU8kFIkxA7fPJ1XVtYeQ9HGvAMNGR89qZgJKpVEkMHMPiVTq2Uu1CUcVlirNMQ7FuLQ4NBqttWiQoyV2aKnKtS048a1K8xngcPjJXfvX3L77RNzVoYOiQa/HZySnBW0bnPCl4DPmVnfo7EhVWjNtpUh2IW3nrZcn0WiO6C6S6Z0YbZkGs988Aepd17h5PQu4/aC/uCA5aEiwbGcX+Hw6BZ9F3jn/ksM86uA0g0DOMfzL3yZb377BXbjhjRuuXL0GP0wsL08Z7NdM00T6/VI7BK0yunJmkenDwylHwPXrv8A5eIdBj8hAxwNcHgQ2IznnD+Cn/zsj/GN1x/w8OHbpIuH/PTP/SxNG2+++EXkoMJ0xt/4b/9nSm6U1R226RHnp1tWjy7Zrl+jmx8grVFqZdydwbQhhAMmL2gygbTWDJOzuPa4s94M75GcaOMWF4TtbkM3c0hYcHG6YbMpSFZyg2lXaWJQIN/NIFR2ZU98UkfwGXEdYzNcoRCIzpHSyHDQ47oNre2Ic0/LELrAtCm4WvbnqgK5oN4xrbfcu/0OOl5SpkbJ5hadB0/aXeJ8IY0JCQb09b7ZmJJKTWYjtQmbLRZWXNSobUtLgTStbJzeHZLHS2oWu+aH90e6fE8vDroPQon3NKJ1SWoliGceI0dXDvHBYt3rbaa2RioWye1C42juaBVS8nS9sDg4JG8TU5k47D0HR9bd6Lxydm5naGj0QRn6jpwnqoDQUVIm1Ubn7CJpeyaCNg/NeiODTzgxgKwLHprjrbfvs9kWnnvmgMXBgqvHR0zbxDtqZ0URO+q06kA8B32wZm43p5ZKhyOlSi9wfCXQzxqL+YIQenbTxOXFJaWaV6CmkRgqQQLHhzfQOnH9cIGLN+iGBV13hdA52mjn683mLvdPVhRNdOOanK5zdHgNMMV8N604Wg5snWe1PeX60VVuHV9lypmLyzXv3HubUgoiUKdz/u7/9F9x+ughQx9xZU2QwnqzYb1KOP+A9SbRuZ5aMy9+53VOV5cgheHwgOATQ7dgu8p4t2NQUH9I3p1Si7LLp7RpSy2N2bKh6ZyDg0JrjXXxHCwOEGm09TmtKDnbsdF7pZs1REb64Onnns5DP4Np6ynTyMVW8GHL4Y2O5SGUtWfcZfaGZw4OIrkkcrUQUxCliuU+em9daE11763I5KlnPjRoGfB00bMrynAQ2VxaWKwLAR/gMz/+w3z8hz/L7T/4Ai/fe5PzlPeWfGHcTHTdAHim0YpzxEOIFZ97o2lFK+Q15mQlSE9Vi3/ZFObAdp/J9JE6rdAs+/g9uPZnB3v5M301bZSWiTi8d1aA6nooGa/CZlMZesXFyOmusFgsePqJa9y9+w5dhc22GlJjNhBnDlzP5bQjCoQYWR5Ehv6YWgrnp29Tyx4VXpq1GnljLJaUefHFN5hUyDUZSXjviKRF1DeiVJxYkKqpR6RQy4bUKkkbDy+U5SIyjmvGXWY+eOYIeE/NBQ2Kug7vMj50LHullgjesVj0pJypJdMF+5al1Aji6GOgC55SL1E352jw+JhYLAKb1SX4zp50u8p6O+5t4I2ma5QOHPTdMbPZMZ/99Gc4PDxCVfft01bpltJoLIk0kmuilolxynzk2Y+y3WROHt0l5Z2lI68eMs47prSiG45YbdfMhqss5oNpIs6zWAb+8PkXuPnYFZaLSN5t+da3XuLm9Wc4WxV2l/e4qP+U4fCDqMBue8oQM2m7pgsjTAXvBPGBGGDoPeP2DJpDvZUZiYPoG7P5jK4fbLep1nehqefs/pZWJ0pRqxT0K5bhGpcbaKmy1YbsMlUHSnM0r3gvdB609PZQCZ6mhU56ckuAgjjGqbCgJ+MRL3RecT5Sq022wjDQ9RFfJm5d7XnlS1+gvX3bDFsCOTscExLMi9APHfgdsfd0MbAZlZSs70QUclGcmDmw1j0izjlaNnhwqcZDpbp9d4V1tJTJQ/996nNAwe/bV71voErLlegsbffG22csB3jyAzdJKRP6DbM4YzlzbDeKdMpTj10lDpFaHd4pBwcwn80IXQBpXD064v7dUyRWhl7YpoxGwcdmLAdndu2cRqZ9kUvNDUck1REfssFWnYJXAvvjj2v41hNRhq6yvTxn2sL64gyRnoMZHF5ZcrnKe2+A42KTGbrAVITclFyEMk5sdjt8U+tzUIeEC4bYM19EZosesdQXXoVdnXBNGXeZoRPyODHuKkkq4q3G3ZrCM0Pv6IdDlgdXOT68buMwGcg1s02JnBI5J0o2gE3J/B/tvWnMJdl53/d7zlLLXd6t9+kZkiMusRhTCimGISFbkSWFUpTAigEhkpHEjGRBcIwAifMhkJAAjj8mMILEiSNZiJ3IjiPZkQhLIBJMaFkKI5mmKZomORySM8PZe+93vVtVnS0fTnWzNXyH5JAz3R3j/NC336pTdavOPbfquafOeZ7nj/cK78B5Q4gWXW3T1B44RlvBIMzbmhgmrNb72Go7B0clj96tOTzouH6tzwaaAWssTdty4+Y1/tnnXuTibo3EwPUXn8P5p3PGcO+5+Og2V68dQ8zKWm2baJVmMrVcudWjw0CKOSNSTJpJDVE5posp3i3wwYGLGJ1D1msbmOw0BBdxKeaZjNQxOzfPxnG9JoUaYyqqiYHjhDEqj2eZgA1VTq6b8sCyBroxW7Y1OYgrDh2zOdTtjJPDns2yo51a3vujP0ucbnj6Y7/BC8+8wsHBgsVyYOUS/eCyvJ5RSNR0y0RXBXRITOqKwXc5pXzoiTEQB4XVCu8HojLkgVIYfI81ln6xIQ0572pMKfs2JPB9wFaa0P//dCrTaMW0sTgXGPqAKKGuDP3guHHs8DiWThFiDp0mahYLR7+C2WyL89Pcdet7j7EWW0UuXtxDqwkhOYZuwwsvv8D6JLJckx1K+kREUyGYSrDGUM2y7FijTfYFUAGjNUa1aFONSWMjzguz+R67s6yO/NwLN+kdvP3Rs4QBXrxyg7beQ+kNQYTFgaNzieACYhwhwMmQMyn7KChx2MpQSXaFTWMqOGNqtNLAmt5HJpMJkjwKRdvksO3F+iYHi33mW1X+hQvge4dLHi1C00zQpqayhq3ZOep6iyE1rMIUrSxG95gqq18H5YiVZ9P1hInGrRaY4Igomq2LVJMdblx7lsQapYXlyVWshulkxq0bN6kqQdKAxrM114Ro6LqcSXnaarZbzcmJ5eZ+pN1q2NuuWfuIUYaQFG7IvZLLl2aEPhJCYkhhTOLrsAZsPSX1A6LB+8jGRcQbNv4EiEgSrPb4kNiZ1dRNhYsRFyN1W43JdIQzlx/h4MYVhi4xP1MjjWa+U7E40uxd2mF94ui7DcENiK2RkPNUapN/qZUIm/VAu6kZNjkK0vVZRMdqjbU1L37xYwTr8W7N4dJi5hNM8HDiMUpQTZ0NeIoEN7B99gL7rxyx1gOiPbEbYy1Vg5IOrVtEddlJzSqM0RjTgEpM5xZ0ZLLSo/u8gpgYhogyM4xxPHN86/T7737e7K8Xay0Xz8/wfgAJeK9xUeGdgqg4CZqq0lhjmM/mTOrs7799ds50VuUsv2KY15a6qbBa08cqJwRJBuopk0lg1sJ0IhyvHBf3tjGTCW7oIPrc5WeMiZeE+EQfxryTPnFu6xzGwFeeewl85OxMaCthtY5Ukmcubl69RSL/whye7IOAH3JeBlIePDJamCohkafCDDoL3yIkn9W9tYwCJD4wJEF0oK4SStb5Qhr1mFLssVYxm24has165XFeEAu1raiqae7ZGM/O1lm2d84g9Q66OcPb3vFuzu22sBpY7R9ydHib5eaYvu9Yr5YMqyO6oWNwkdnOBf7En/wAZ8+0/OZHn+CpJz/BsDkgREvXL5HoaafbzCc1Q3fI1rRmMt1m2kwJ786ShT50bDYrtIpcPntMDOB6x+HRmr3dOXt7WyQxDL3j3JkLpDAK7agsxTe4nscvZ4lDLTbHGjhh6AeGPrG/37HZRIzN0nVRhC6MXp1hIKiK0GWdER8SBy+9SFh1pBQZxhmRlLIG59H+GiEhWmFnFpV8jvOIWfGsqSwu5nT5GsfW1g5Ht7ocFVsrqq0JPmyxOTpi6FZYrdj0S3RsmO80PPLYHKUdQQSt6rtjGZcemWPjBWJyEALpMninCYMh4VGypmrOkrzDux6J2agmApMZpGpKSgPb021u3jikspq3vesMUSqqquaZp7566v33UBuHuml5z/v/ddaLQ7Q4VovreNfjhyUuWjZrx965HfZ2L/KWxw0Se4zdIonP3cjo8U7o+37MBdEznexgbUtMG1KIzGYz3NCzu7PhUoDNsMF1A6IDq00ODRfd5rwJaaAfU7f7XiFJ2N+/hXMDbWMIznHj1i1uHRzjUkKCICT6kENqjRnVp7UQjKJSLaZVGNUgaZV7DEMkxBw/IeRsRkGygUrklPBZHFioqikoBzGrKaEdSnmEKXrMk+AHjVaGegIKj24nKDNHK8Vksk3TnqWanKOZn+cdb38rH3jPLlYL128lnvzSNZ5/4SlWJ7cQIiFqTlZHrFeHGDtn6/xbecvlHbbmNTu7uzRNg+sjYejAD6PIrqc/Ps6ZlKPneLnBmAprNFYntK3Z2T5PO9nh5v5TOchORc6fqxEGTk4OgIiY7MLd957NMGRxF2uQOOBdTp5SGUtbC7puqKeKFCxaT/C+y+nsxVBVFjvJiYfdUNPMW2zdYpTC2kDXe+p5w1YKJDE8/8VnSMHzrve9BaVqVHT4tGLoA94rrM1JVnwUFocd64VjNjVILazXa2yVcEPAWoUxML+4x+X5WwknL2AFbNOQMChxiK4RZXD9ikgkrIX1sGJ9dMJZpblyHBmUMJ1Zpo2gbMBIziLtXMAFB+vA6jiy7jtELAfHDqM2hOi5enADJDFsDLevnKBaMPa1TcBDbRy6bsUX//knqOuaqmpxfU8MGxSKKIHpZIJKgcXiEFPV1PXZnD/ACYNbE92K9SqQUiLGHtO0TOYtQQHMCKFjs77KanFCvxogKgbf43zWlsi5FTu0XaNEEWNOASZiUSZhlOSEq0bR2gqxTQ5lTh5lNM7nWQ/vPXXbsB46KhNRNPjoqestwjAwRI9gcG5ARKMbk0OegyNSoazPodhRYTSoaDFVhVaeNA7MqaiJUeXcFNYReiGZnItCBDAaa1qstdimZjbbZjK9zPlL7+Ts5XfTDwOzrSZngibR9zmXxHJ5yGq9wPs13nuc61Ei1FYhvuMff/pL7J8s+eznPkkMEaMnWBXwqiOFHPzlnM8RmkNP0IahW6KtgaSxFm4fvsC0nXJ4tGax7BBVUZvEY4+dZW/3DEfLfTbDgt5tcuCbNmidBY2SMpgaJBqG4HEbl0WGaEECdjpHNg1hWFHXBjF5LMn1A0YpDm8t8cMCJYFpJYRkOOo8lVHY+YarLx2ws9sw29U5IY04hlATQ8fqxBFVoLGGlBRmophPahoN62UiuQgh0kwFWxn8pmNz8FWqagfMSc7YJOusaRI3hCgoscQ4oGyNVIDbsFkumUbFbhu5ulix3m/pFFAFrK7xEolujTYJlFBvw0S3JHHEZAkxklzAk9MVEipgQERyCvPX4KE2DkoJqMR64TgajjEq4YLHGEVdN/h+DVFRTQxa15i6oaonxM0JoQusNkt88KhkUFrQOnsH5sxJ2R+BIKjksXVOsqFdRW0k6x9WE7re5SStUfKvMQq0oIIB5Yg57janPVcJks6pyhJUFlxYZd/30FMbg9UGkT6nCJSOLnXU1ZTazlgubzOdNCjdcHR0iKotW23NycmGCoW1Nd7l7nRlIsoaIitQHSkKk/YMySpSGtDjyHYIAWU1oipsNcfqCk+FqBntdBvbbjPZucT1L/1TPvH7PTf3vxutBXe8RMfEWy89znq9y3q9ZLNZst6cMLgN3q147vnP8uXnv8Bqs6TfHHFhb5sQeqw1TJpdbh90tNUU19+m9wND51i6JSopAmmccajQOrA4PmY6MVhr8jy/TywWGy6cn/OudzxOVSV88Ny6cY0vP/U0TimM6nBkmUQtitALxhi00kR6XEx0XUInTdAuhyhn3TxCEgYGlLbMpgFSoqor7BTm3iA6Ypuafi00GpaLJcF5TC3YVqMsnG0bxGYtlRDyOJRSNYEV1VxIQVPvNIixaAkEDFYAfzTmLw2k0CCVYI3Fh4TWJiuHqYBMp8xtIkZhf1ghKXBmqyVFTXccGLqcVNf5nOlK/AbRTX7kTVlcOA92R5KuMJJzd6YYQCw+DqOX7Ok81MYhKwaD0Q5VZRWlylhqyU5AWlsm88jubsswOJaHL9DO9mjrlq0LW6i0xTAElt0Jq+UG72F9tI+L10ghJ5AJPuUR+CEQR5EVnQwQSH5AlGY2n7M86dk6O2O5WGBVjYih606YTqb03WLMA2hz0hmdBwxjchhV5cjD1I8iLnlwLKIJYYPVgoprNt0KqwPrvkOxwBpDZGDTBayxQBq7gIkEWJM1MnqXZfWMgDY5VgAEpcAnQVcTmvllcAckNM47ZvNHmE63sXaKVFNiSly79mVefnHghee+DGJI/RHrxTVWi1ssV7foug4/eELqEcBWLUkrrJ4itoK4QvwOXXeQRXd8IrGhWx7mPJkOUspixT4lYsjdbcnSURjbknxAA3Y+ZVJHduYNyV/n2aef53gxEIKwuzvn4GBgCDmZbtVETBRsE3P+D1EI2edFRFHVihQHdIhU1hFDhdKRZhdiyErVNQqsJniodWDStgyhp640b31Hg5JE33m6XkgpYb3P8SVqoJKcZ8RTM3QJY12OiFU1vfJYnTNoiaowOssqiFjE5qxjdTXFuY7oI5ICAz1KNEpZ+nBITOC6lAc8RdBJ8D6gGkVdG9IQaOc6K6QHM14djhQ13il6n6gxJBXRJgsV1VVDv+lJgyLUzWvefw+1cVBaEVVFH07wa7JK8Zju++yu4dz5KcOw5CvPXUOiISlB3dgneY0Yj02aSaOYbFnO7GyxNT9D3W6RpOXo8Da3b13n8OSYMCSSC1lgVTQueGyl2IQerSInx8coEicHfXY2MRpiQGlNO5njQ6JpLJPJhNu3rjOZTqjqlv39/Zygw0AcQKK6qz1QWY1PJk9Bje7eIVagPVF0frxIuQcSVBb4HcIa5bKa9NAbKkyexYgJFzXK9VRknUiAmBqm596HsQ0n129C2rC9d4mqrdHGMtva48L5C7z0zO9z68rn8EE4Pt5HiWW1eoVufZXBgRuWkAyNbdBUOLciOoPyQtAR+jwYl8JACiEP/A2b3MuSkDND4dFRgzLYqFFWYyvDpJ0wnVpm0zmTOtI0U2yVE7kMQ0JE88531ayXnnW/YbE4wv7xLfaPNpwcOw4PAyvnYQliDT/8g9/HpUuPcLB/lZvXbnBjf59NyimBV8uIKIcSjQ0gBJRPSCuIy1PQyRt6MxCjp/NZ8EYpQ1IaZR1iWqL0qCT4aOljR/CKmDRWQwiSB1AHjxiVk7ymnFuTaPDJIVSo2BNTZLU+vqNlPIrapOwElzYYUSipctticUPE1Jpm1qJUhfKew+u3WR8vWKzzVKUPPSlEPJFH3nKRH/oz/y7RJ2qbo5yTKCSteeKjT3D72g20fi3n6YfcOIhoKq0Zoib4HqWF6azlkUdqlss1z79wlcHlKUetLJW1RBUJQaMDdK5jtRb0sZBkgTVXmEyn7O6d4/yFx3j729+CoScER9etOTxacXJyxMkyMAxL1CDkHyiV8/u7RKVVvjgwWB1YHN9CROP6DUfdCUoMQ7+mH9ZIHFC2oq0mxGFA24ELFx5jsTzi+HCFtonp1jau79gMa6IYVKyzvJ5KSBIEhw6BGCtiyCIlpm6yUpECHweWR9B1a9rZmvPn2pw1WiZsnf9uAhUHVz6F7ze85dHH2dp6BBcTCcN6E3jumS/wwpc/zmKxj08VTQzU1YTgOxIVip7KWibtWS6evcBm03Pz1nNsNussVounrlvatiKOyllKmazARaSuWqZtxXwyZzZtmbQ1bV1T1TqHxJPouoHFYsXR4QEvvvwyJ+sVm02k6z3z2ZQf/IH3cXBynWe/epW9nSnnL57jscc83kf2D3pu3z7g6Diw7jz7166yvVVx6eJFHn/7H+P3fv8PWFx5mX5Q9JtAXcFsGnAOjJi7CmpSaQgeKkeIlpiaPHZkhRhzsGtPTZ2EFHLWpaQNIVRZvLepIPZZj7PvUHWirvYwdkZlJ1iddV2NrmimWzRNg7EVX/jU/8PxYcfgegiRKBbnPTpmH5t/6yM/x3e/9/shMYogO5TKvTBS4Dd/+X/kuac/w5DAjdoXohKGBEHwq2v03RGLIcdZbLpDYtC09pCz21mU+bU0KR9y4yA0jYZUcf7MLjtbwslywQsvHtFvEkYnklYkl/P1kVYkVeV0bwJKC8FnYVZlPP0gDN5zeHyL559/jsoomkrRTiw7W1tsbbVcfuSdWJUYQs+mDxyfHNBvHKu1sF6v8C5l91QlJOmytoXXBMnTXVqyv4PSCSUKfGLTHaN1j09w7foLpFRn8VslY/ZkQww1xhr29i6QiOzvXydGw9ZsC2MtN65dYbV21NagRyNRNZq2ahnsClvtMp+NUX9Ng24eJ+iKg1f+MZIcbTNh0w+o1QrbbGGaPZSp2L/yB3TdLXy/ogsbkI4YAq5bEDz4oKiqGZNmyvHxIevV8V03YiHm7NZNC9IyaTXBWc7vXqCuwShLYxVNY7HG4H2g69YcHN1muT7hZDGw2HSslz2rLsdoeB/xYRSGETg4PuTFV67y6MUzfPnLL/HFL75ESgYszGaai7tTHnv0DO98e4VzAyeLJc8//SRP9yC25fayw7kIkjBITlXvAqqakIY8BRkl0EjE1DVKT2ibPWbTClPNaFqbb2prUbpGKcXRwVW+8slP0iefBYeVp1OCxNyprxo4v3uZP/mn/wIXH30Htq4wuslOSAR8DPSbnuAdn/7EP2O5PEKlrH6VUs6m7kOidwPPPfV5mtowdCckP9BtHJ07IvY9XXeI71/mzNkJfczyzrVWiIE+CLVsePKTT2TNVEmIqtAqkiQw21bMd1qMrXjq6a/Ts873X0qvPVr5oNnd3U4/8qf+ONtzz/HxEa+8tOF4scqRh0kwMeEQjNUYZTGmB2XRKifaDAx5wNFkd1GtElpqJA6je0Ei+vyFiMrJOKpGM2kqZjPL9tYO2/MJk8kUo6scu+GW9H3k6MSxWB2zXPYMfY9zmhj7nKTEmJx52etRRyDHXqRgssisicQx7YcRj5gKvELZAYUZ08B7uj4RY55uXSxWrNYbBgeVVgSEnemM3bNQaT0qfwMSUWpCjQ73wAAAEzxJREFU5yvqqiLhsXbGpK1pmjlV1SBmh3p6ns3qCpujL3J8fMLh0ZIuBKZbl6maPS4+9m62dt6C846hW7A8vsqV536P9fKQtp6iUsRoRdO0GBuxumI+aTl/9ixtU9N1SzZ9R7fp6LueRbfKN8TQ4VIWtCUMBBIhBIScWTn4RAh5cNhHRUqwM2/4Uz/wTm7eXPEHf/A0iz7rdbok1GJJkpjUmr2dlgtnFWfObWErz2rlORzO8YU/fArnEhLBEdDKoiRkV3cfEKNoreHsI2/hp/7j/5LtvUs0k2keD0GhtcF7h/cdKSU+9/8+wW/9zf+JIQasNWjJmhJtA4uFI+lEVVm+9wMfYLHo2b91E58iOI/ruxxtrBJvf9fjLI9vUhtNNWkY3AIZkykrsWxWHmUsk0mFUgltIIUZSsNsqgmyysl1U0TIeT7rukXpCKnOWhg6pxsMLuEGoe80mz4PrvZ9wiXNJ574wmdSSl8na/lQ9xwqq9nb2eall7/C1SsLhiHSD54xGir/8hrJKlhJSFFnp5C7EnoGVVkM8Wty7iaSrEUhiAh1C8QAyuRpHzew7NYcn8ArV46xojF1zbQ1zOc7zOYtW/M5b3tsi7p+nJA8bhjo+g3LpWO5XLFar1h1a4Yu4YNDGY1FjaK/WclaSzYQmpYgEacCfgBJAYKidz3rIQ9GGd2ztV1R2XEMIiQG50Ecq67iZFizdgNGaVARLR1NbbD2DFvTMzSNxlYtYiegp9jpHuDZHL9ADCti6DDG0FiLSOTcI3+M6fw8jzz+PUj0bJY3ufb8MctG2DaTrFGqTJYHkIqQNqyWS1bLYzZuA0kTUoIwjOItwnqzwfd9VpFNuc1dkuy0lQCEFOMYeZvLVHIgieVizc1bB1y8eIa3PL7Dl56+BSESqPNgn4msO4W/Hrh6I2DVMbM9zfmdFrsTOVoMpKhAZUcoLRGlIyHklPAMgcEo9tAsjq7SrQ9IITG4NSLZU3UYPK73hGHN9Ve+wO6lKX3vqaqA0Q0pKmwd2b0wyd1577n18pfohg0mTtBVwtaK6kxD09bUzRTRwuXHLqCbCW0lqOoxJpNdTN1QNzO0MTlTVPLE2BH8Mb4PDMOA6zrW64blYoVLPZvVkn7dM/RLhnVks+pZbza4qEgOBh8RCfgw+tEkTWVAbP2a999DbRycD3z1hascH2pSqvGhGw0AGK3RRNKoSGxMRJN9EdApZ0xm9ELUBi3Zr17pBBhk1DCIMeV0bAHA5cg+lZ9Fs5dZZDP0bNY9B0c9gZwCv60tk9ays7vN9mzG9vYWFy9uU9tHACGEFYMLrFaOk/UhR4cbVuuB9TDgnSOhMSpHmeqkMDpCa/BDYhgGfJ/9343VY7e2ZjbrMFbTD5HNqkMQXLfOQVgVaNWhpGIyqZjNWmazhrrKAsEpZfk6bQ24Q1bHL1LrRNVcpKkHYvIMQbPpDWd2zkJYkoYTlgev0K0PccuraAKqaRAqumFAgOnOGSqrGNaHVDahbUOl2zzN2weG6CAF6lbTdz2ud4Rh/Pwh4HyHCYJXioTBKE9K2a1ZaQspklA8/+IBj156hPd896PsXz/m8MhndRjAuawc5fTA4AWvPMNNw81rgUuP73P+wg7Bb7IIjptCGpjtzNBkeUVV9bRtSzvb8NlP/G8ktUHHhig530WOw9H5lzgqgut57PIuYnMFlJogaqCaWrRUaFGY1tDWNaqGptmjqmu0BWsmGKMQciDWMJyw6Rb4roLk6QfYrE+4tbnFarFg8D39Zs16tWS96ujWgWGzIQ4R7yODBx8SfpQYVDqLPwlZmkDHmEWeFWjRY6JdT0RyEFrfv+b99x09VojIXwJ+jvxb8AXgZ4BLwK8DZ4DPAP9BSmkQkRr428D3AfvAT6WUXvhGx9/enqX3vecim03uTg99x9oNVMmgokWZLExK8hhlR7m5HEKsRLBV9s9PyRJVDl5JEhBqfOzJYUKgbSDFLLWXoielFm1yQEoM+cNlXUiLVuTuXczPj1n+LOZkoOKYt3O2tlq2d2q2t87QtDVVbbMcelC44Om6Nat1YLHs2Kxz/sZu4+g3PYtl5PhkifOOzo0DrRPF1pal1hWknkhF9BFFJIR1DjSaCvV0i6ataOsJTbNHXTcYXY3+HzVVcw5rDN1qH4XHGqGpJyAKZSrOXnoH060zHB4ucOsltp5Qm0T0C6Jz9N6jxeckJSFh6xkf+qEPszpZ8vlP/x5dt0JUTtuviDjnc5c2ZJX0EEaPyb7PA6op4tyG3g0osazXK4Zhw/LoFlFqkljWmxWu7/DR8b3/8kXm0ylf/eotPv3kK7je411ExcTgs4GISnAYquhJaM6ca5lsVygDla6yE1mlaSY71DVUlUFXM+q6pZnuMpnvYK2glUJUoK5akJzgl6hJyRN8jxuWo2iyIfiBoc9OX95bhv6EbliDt6w3jhQ8m1XPZr0hhsjGCWHoWa4GlI90DKioCBIxJj/GgEZiZIgDooToIslJ1nINCZfiXR+e4D1BCeIDLpDTBpCICrKnbb7OTVIklZPTpuSx1kAy3Lx149THim/bOIjIZeD3gXenlDYi8veB/xP4ceCjKaVfF5FfBj6XUvolEfmLwPeklP6CiPw08GdSSj/1jc4xn7Xpve95jH51RIgqi3TEnuCFYch6lyIKq+LoA1Gjq6x0jCKnM4+RlBRBRQwOHzVGGUgKUdkYoBLJC4GAIuufp5hFSEXFu05UKFBJcmZgSSjJF0vEADldmNIWo9To/99QN0Jb7bC7NWG+VTOfN0zaCqUsScConCnah8h6uWT/8Jhbt29wfNJx7WbHZjWQUkIJTLfrrBCFQlSN6EDTeNp2ypnzc7amuzTthErNMLbOjy2mpm4atuZniBGc71Fi2JrvITqniRsGx9nz38X3fN+/ijHCS889w9GNlxm6VY71UDnvRIiRYciu6KSIMg3z3UdYnRxAWNP5jhgcfb9CS9Y0jyELvRqjR2UmoetXhJB1Hdt2jnMdIQZC8PkcXZ89T3VNih6rBRf8KNiSNS0Xi57B93Rdz3rlGFzPerGk6yNr50mdZ9l1vPv9j3P50VkOzJMGpQzK9ECLSEsSRYpNjqsMHqUrYnQMfU+/8cSwJnSOTd+z2ixJnWeIkdVizbDOClsuCEPXgWhcSKgYiCoHy3VD7tUieQo6hYQyBqurfL1FIaQ8KKpFEQVUpXIykygQs/9DCFnUOcYEPvd2fciJYiRFiDoPjqsclWl0GiNwe5JuMMnnXzppEVGkFLFWiEpz+8b1N8U4/BPge4ET4B8A/wPwd4GLKSUvIh8C/quU0o+KyBPj8idFxADXgXPpG1RAKUmVNWPXkvHZNHHvO7KQSy4fS8giY6NA6biexi1390np7p9733fvETIpu5m++hRk8d072YS/7vjj1vxWNe6R62uMRo835tfen+46MKWUtTVDCNkIpjufNfeI4t2CbFy01WOU5p32UXc/Tm4fuXtB3KmnyJgYd/ysSmmUqkASKbr85nsaOtct3R0fGA+e2znlNhKtid4To88GhXsP8bV2vHOsr2352o4iQryj/Cxfa6E0OgPkbzLd+Tee/169Rxl7czAMHlPlXCAi6p5veNSlDNnb9E67k7IjVbr72fNj5Z2qpnv+u7N877UiZKFhGb0283d5z3Uh+VwyXhd3vos7p797Ehm/7LuNlxgv5rt1l7ttcS9fa9/c1Pde+/K1E93rFClCDOGNHZBMKV0Rkb8KvARsgP+b/BhxlPLVDfAKcHlcvgy8PL7Xi8gx+dHj9h/5eCI/D/z8nfV+eO28+mNNXmP9fs7CfLNzhVetuzfszI4I/Rt3vH/RcP7Vbf96eG2R2T/K670Gv51r87XOcXpZSq9av7dO3+LpXzuB3DdBRHaBnwAeBx4BpsCPfbvHu0NK6VdSSu8/zZIVCoX7x7dtHIAfAZ5PKd1KKTngo8D3AzvjYwPAo8AdD4srwGMA4/Zt8sBkoVB4CPlOjMNLwAdFZCL5YfKHgaeA3wV+ctznI8Bvjcu/Pa4zbv9H32i8oVAoPFi+06nMvwL8FOCBz5KnNS+TpzL3xrJ/P6XUi0gD/B3gvcAB8NMppee+yfGL8SgU3nze2NmK+0ExDoXCfeFU4/CdPFYUCoV/gSnGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVYhwKhcKpFONQKBROpRiHQqFwKsU4FAqFUynGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVYhwKhcKpFONQKBROpRiHQqFwKsU4FAqFUynGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVYhwKhcKpFONQKBROpRiHQqFwKsU4FAqFUynGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVb2ocRORvichNEXnynrI9Efm4iDwz/t0dy0VE/pqIPCsinxeR993zno+M+z8jIh95cz5OoVB4o/hWeg7/K/Bjryr7BeB3UkrvBH5nXAf4N4F3jq+fB34JsjEB/jLwrwEfAP7yHYNSKBQeTr6pcUgpfQI4eFXxTwC/Oi7/KvDv3FP+t1PmnwA7InIJ+FHg4ymlg5TSIfBxvt7gFAqFhwjzbb7vQkrp2rh8HbgwLl8GXr5nv1fGstcq/zpE5OfJvY5CofAA+XaNw11SSklE0htRmfF4vwL8CsAbedxCofD6+HZnK26MjwuMf2+O5VeAx+7Z79Gx7LXKC4XCQ8q3axx+G7gz4/AR4LfuKf9z46zFB4Hj8fHjCeDDIrI7DkR+eCwrFAoPKymlb/gCfg24BjjyWMGfB86QZymeAf4hsDfuK8BfB74KfAF4/z3H+Vng2fH1M9/svON7UnmVV3m96a8/PO3+k/EmfCgpYw6Fwn3hMyml97+6sHhIFgqFUynGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVYhwKhcKpFONQKBROpRiHQqFwKsU4FAqFUynGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVYhwKhcKpFONQKBROpRiHQqFwKsU4FAqFUynGoVAonEoxDoVC4VSKcSgUCqdSjEOhUDiVYhwKhcKpFONQKBRO5TvWynyTWQJfedCVOIWzwO0HXYlTKPX61nkY6wQPpl5vPa3wYTcOXzlNbONBIyJ/WOr1rfMw1uthrBM8XPUqjxWFQuFUinEoFAqn8rAbh1950BV4DUq9Xh8PY70exjrBQ1Svh1pIt1AoPDge9p5DoVB4QBTjUCgUTuWhNQ4i8mMi8hUReVZEfuE+nvcxEfldEXlKRL4oIv/JWL4nIh8XkWfGv7tjuYjIXxvr+XkRed+bXD8tIp8VkY+N64+LyKfG8/89EanG8npcf3bc/rY3sU47IvIbIvJlEfmSiHzoYWgvEflL43f4pIj8mog0D6K9RORvichNEXnynrLX3T4i8pFx/2dE5CNvVP1ek5TSQ/cCNPBV4LuACvgc8O77dO5LwPvG5TnwNPBu4L8BfmEs/wXgvx6Xfxz4vwABPgh86k2u338G/O/Ax8b1vw/89Lj8y8B/NC7/ReCXx+WfBv7em1inXwV+blyugJ0H3V7AZeB5oL2nnf7DB9FewA8A7wOevKfsdbUPsAc8N/7dHZd339Rr7c08+HfQmB8Cnrhn/ReBX3xAdfkt4N8ge2peGssukR20AP4G8Gfv2f/ufm9CXR4Ffgf4IeBj4wV0GzCvbjfgCeBD47IZ95M3oU7b400oryp/oO01GoeXx5vJjO31ow+qvYC3vco4vK72Af4s8DfuKf8j+70Zr4f1seLOF3uHV8ay+8rYtXwv8CngQkrp2rjpOnBhXL6fdf3vgP8ciOP6GeAopeRPOffdeo3bj8f932geB24B/8v4uPM/i8iUB9xeKaUrwF8FXgKukT//Z3jw7XWH19s+9/2eeFiNwwNHRGbAbwL/aUrp5N5tKZvu+zoHLCL/NnAzpfSZ+3nebwFD7jL/UkrpvcCK3E2+ywNqr13gJ8jG6xFgCvzY/azDt8qDaJ9vhYfVOFwBHrtn/dGx7L4gIpZsGP5uSumjY/ENEbk0br8E3LzPdf1+4E+LyAvAr5MfLf57YEdE7sTI3Hvuu/Uat28D+29CvV4BXkkpfWpc/w2ysXjQ7fUjwPMppVspJQd8lNyGD7q97vB62+e+3xMPq3H4NPDOcWS5Ig8Q/fb9OLGICPA3gS+llP7bezb9NnBnhPgj5LGIO+V/bhxl/iBwfE938Q0jpfSLKaVHU0pvI7fHP0op/XvA7wI/+Rr1ulPfnxz3f8N/nVJK14GXReRfGot+GHiKB9xe5MeJD4rIZPxO79TrgbbXPbze9nkC+LCI7I69og+PZW8eb+aAxnc4gPPj5JmCrwL/xX08758gd/E+D/zz8fXj5OfP3wGeAf4hsDfuL8BfH+v5BeD996GOP8jXZiu+C/inwLPA/wHUY3kzrj87bv+uN7E+/wrwh2Ob/QPyaPoDby/grwBfBp4E/g5QP4j2An6NPO7hyD2tP//ttA/ws2P9ngV+5s2+zor7dKFQOJWH9bGiUCg8YIpxKBQKp1KMQ6FQOJViHAqFwqkU41AoFE6lGIdCoXAqxTgUCoVT+f8APrUmF/5bEkkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "mvAnhNUPhFgU",
        "outputId": "61f9f234-19e5-4919-bc14-6ce530b34873"
      },
      "source": [
        "output = model(img.to(device))\n",
        "print(output[...,8].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9947)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xX05wMiohJNz",
        "outputId": "8a34bb29-1ead-4d03-c55e-f21ecfd27f9b"
      },
      "source": [
        "output = non_max_suppression(output,conf_thres=0.9)\n",
        "print(output[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 11])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "Bw4rYBEDWKaY",
        "outputId": "bea4e00b-1ef4-47d4-a455-c83b3053007e"
      },
      "source": [
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000e+00, 0.0000e+00, 4.7458e-01,  ..., 3.2640e-01, 1.1154e-03,\n",
            "         4.6662e-01],\n",
            "        [0.0000e+00, 0.0000e+00, 5.0075e-01,  ..., 3.5600e-01, 5.4577e-01,\n",
            "         9.9790e-01],\n",
            "        [0.0000e+00, 8.0000e+00, 7.4861e-01,  ..., 3.0260e-01, 3.0156e-01,\n",
            "         4.1066e-02],\n",
            "        ...,\n",
            "        [1.0000e+00, 1.8000e+01, 4.8719e-01,  ..., 2.3620e-01, 3.9791e-01,\n",
            "         1.0534e-02],\n",
            "        [1.0000e+00, 1.9000e+01, 7.3987e-01,  ..., 1.5480e-01, 9.9424e-01,\n",
            "         5.7565e-01],\n",
            "        [1.0000e+00, 1.9000e+01, 3.6606e-01,  ..., 1.3800e-01, 7.7270e-02,\n",
            "         7.6702e-01]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU7biujtWx1R"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib as mpl\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcQwss6bXKoR"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111)\n",
        "for annotation in targets:\n",
        "  annotation = nusc.get('sample_annotation',annotation)\n",
        "  cordinate = [annotation['translation'][i] - ego_pose['translation'][i] for i in range(3)]\n",
        "  cordinate[0], cordinate[1] = rotate_around_point_lowperf(cordinate[:2], ego_yaw, origin=(0, 0))\n",
        "  rotation_yaw = quaternion_yaw(annotation['rotation']) - math.pi/2\n",
        "  print(\"rotation_yaw = \", quaternion_yaw(annotation['rotation']) + math.pi )\n",
        "  object_rotation = rotation_yaw - ego_yaw\n",
        "  height = annotation['size'][0]\n",
        "  width = annotation['size'][1]\n",
        "  x_temp, y_temp = rotate_around_point_lowperf((cordinate[0],cordinate[1]),2*math.pi -  (object_rotation+math.pi/2), origin=(cordinate[0]-width/2, cordinate[1] - height/2))\n",
        "  x_offset, y_offset = x_temp - cordinate[0], y_temp - cordinate[1]\n",
        "  rectas = patches.Rectangle(xy=((cordinate[0]-width/2) - x_offset, (cordinate[1] - height/2) - y_offset) ,width=width, angle = (object_rotation+math.pi/2)*180/math.pi, height=height, linewidth=1, color='blue', fill=False)\n",
        "  ax.add_patch(rectas)\n",
        "  ax.scatter(cordinate[0], cordinate[1], color = 'red', s=10)\n",
        "  break\n",
        "ax.scatter(0, 0)\n",
        "plt.xlim(-80,80)\n",
        "plt.ylim(-80,80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3xsHPdyHjeJ7",
        "outputId": "5c344ee7-135c-4ae3-cfee-5b98fa617b5e"
      },
      "source": [
        "dataset.categories[18]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'movable_object.barrier'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwvIc7iFZ__d"
      },
      "source": [
        "#Angle testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "sbhkMNF-aCGL",
        "outputId": "5c6caf94-8306-4097-bc84-e906730bed04"
      },
      "source": [
        "#30degree\n",
        "rotation_yaw = 0\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5 1.0\n",
            "0.0 0.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "pT11qiCtaLLS",
        "outputId": "317537dc-fd14-4746-cdc2-d7675279f749"
      },
      "source": [
        "#120degree\n",
        "rotation_yaw = math.pi/2 \n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 0.5\n",
            "1.5707963267948966 1.5707963267948966\n",
            "90.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "BjJ5mE2TaN6H",
        "outputId": "7ba8beba-ad6b-4538-da10-41a70485e032"
      },
      "source": [
        "#210degree\n",
        "rotation_yaw = math.pi\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5000000000000001 0.0\n",
            "2.220446049250313e-16 3.141592653589793\n",
            "180.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "qXNmm2SQavVU",
        "outputId": "b7f0c088-e125-4c9f-bcb1-51b081760cb6"
      },
      "source": [
        "#300degree\n",
        "rotation_yaw = 3*math.pi/2\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 0.4999999999999999\n",
            "-1.5707963267948966 1.5707963267948968\n",
            "270.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TGwbnheayIx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2iY7HwCTlm"
      },
      "source": [
        "#Intersection Area Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLZVErYZxX4D",
        "outputId": "981748f9-b220-487c-f394-b8263b135038"
      },
      "source": [
        "from turfpy.transformation import intersect\n",
        "from turfpy.measurement import area\n",
        "from geojson import Feature\n",
        "f = Feature(geometry={\"coordinates\": [\n",
        "[[-122.801742, 45.48565], [-122.801742, 45.60491],\n",
        "[-122.584762, 45.60491], [-122.584762, 45.48565],\n",
        "[-122.801742, 45.48565]]], \"type\": \"Polygon\"})\n",
        "b = Feature(geometry={\"coordinates\": [\n",
        "[[-122.520217, 45.535693], [-122.64038, 45.553967],\n",
        "[-122.720031, 45.526554], [-122.669906, 45.507309],\n",
        "[-122.723464, 45.446643], [-122.532577, 45.408574],\n",
        "[-122.487258, 45.477466], [-122.520217, 45.535693]\n",
        "]], \"type\": \"Polygon\"})\n",
        "inter = intersect([f, b])\n",
        "area(inter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56901155.05995309"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CePQxQ8Uxaye",
        "outputId": "9dc7c7f0-4be8-40bf-957a-5f31e8e5d2b3"
      },
      "source": [
        "!pip install turfpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting turfpy\n",
            "  Downloading https://files.pythonhosted.org/packages/00/9f/3327e2fbddbad1585acf37dcc9aababda0cb007c2d8180a3ae0251fa4339/turfpy-0.0.4.tar.gz\n",
            "Collecting geojson\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/8d/9e28e9af95739e6d2d2f8d4bef0b3432da40b7c3588fbad4298c1be09e48/geojson-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from turfpy) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from turfpy) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from turfpy) (1.18.5)\n",
            "Building wheels for collected packages: turfpy\n",
            "  Building wheel for turfpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for turfpy: filename=turfpy-0.0.4-cp36-none-any.whl size=32317 sha256=76b9dc9f683d5b6eae6f404c1b15fd5ee26c92a78086a865e43952be574641aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/5a/1f/216423b705f113c02e3c287b8c5ce31df9a8c6a5526c4da56c\n",
            "Successfully built turfpy\n",
            "Installing collected packages: geojson, turfpy\n",
            "Successfully installed geojson-2.5.0 turfpy-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HSDxXySxjqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "fe754063-6902-4ef5-abde-b27a51aaa6dd"
      },
      "source": [
        "a = [2]\n",
        "b = [1,2,3,4]\n",
        "for c,d in zip(a.unsqueeze(0),b):\n",
        "  print(c,d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-74a9976d81f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'unsqueeze'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7NbIu4A7ija"
      },
      "source": [
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "    \"\"\"\n",
        "    iou_scores = []\n",
        "    # Transform from center and width to exact coordinates\n",
        "    b1_x1, b1_x3 = box1[:,0] - box1[:,3] / 2, box1[:,0] + box1[:,3] / 2\n",
        "    b1_x2, b1_x4 = b1_x3, b1_x1\n",
        "    b1_y1, b1_y3 = box1[:,1] - box1[:,4] / 2, box1[:,1] + box1[:,4] / 2\n",
        "    b1_y2, b1_y4 = b1_y1, b1_y3\n",
        "    #b1_z1, b1_z3 = box1[2] - box1[5] / 2, box1[2] + box1[5] / 2\n",
        "    b2_x1, b2_x3 = box2[:,0] - box2[:,3] / 2, box2[:,0] + box2[:,3] / 2\n",
        "    b2_x2, b2_x4 = b2_x3, b2_x1\n",
        "    b2_y1, b2_y3 = box2[:,1] - box2[:,4] / 2, box2[:,1] + box2[:,4] / 2\n",
        "    b2_y2, b2_y4 = b2_y1, b2_y3\n",
        "    #b2_z1, b2_z3 = box2[2] - box2[5] / 2, box2[2] + box2[5] / 2\n",
        "\n",
        "    rotation1 = [angle_decoder(r[6:8]) for r in box1]\n",
        "    rotation2 = [angle_decoder(r[6:8]) for r in box2]\n",
        "    \n",
        "    print(\"Rotation 1 :\",rotation1)\n",
        "    print(\"Rotation 2 :\",rotation2)\n",
        "\n",
        "\n",
        "    if box1.shape == box2.shape:\n",
        "      for x11,x12,x13,x14,y11,y12,y13,y14,x21,x22,x23,x24,y21,y22,y23,y24,r1,r2, b1,b2 in zip(b1_x1,b1_x2,b1_x3,b1_x4, b1_y1,b1_y2,b1_y3,b1_y4, b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation1,rotation2, box1,box2):\n",
        "        f = Feature(geometry={\"coordinates\": [\n",
        "            [list(rotate_around_point((x11, y11), r1, (b1[0], b1[1]))),\n",
        "             list(rotate_around_point((x12, y12), r1, (b1[0], b1[1]))),\n",
        "             list(rotate_around_point((x13, y13), r1, (b1[0], b1[1]))),\n",
        "             list(rotate_around_point((x14, y14), r1, (b1[0], b1[1])))]], \"type\": \"Polygon\"})\n",
        "        b = Feature(geometry={\"coordinates\": [\n",
        "            [list(rotate_around_point((x21, y21), r2, (b2[0], b2[1]))),\n",
        "             list(rotate_around_point((x22, y22), r2, (b2[0], b2[1]))),\n",
        "             list(rotate_around_point((x23, y23), r2, (b2[0], b2[1]))),\n",
        "             list(rotate_around_point((x24, y24), r2, (b2[0], b2[1])))]], \"type\": \"Polygon\"})\n",
        "        try :\n",
        "          inter = intersect([f, b])\n",
        "          if inter == None :\n",
        "            inter_area = 0\n",
        "          else:\n",
        "            inter_area = area(inter) * z\n",
        "        except:\n",
        "          inter_area = 0\n",
        "\n",
        "        # Union Area\n",
        "        try:\n",
        "          b1_area = area(f) * torch.abs(b1_z2 - b1_z1 + 1).item()\n",
        "        except:\n",
        "          print(f)\n",
        "          b1_area = 0\n",
        "        try:\n",
        "          b2_area = area(b) * torch.abs(b2_z2 - b2_z1 + 1).item()\n",
        "        except:\n",
        "          print(b)\n",
        "          b2_area = 0\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "        return torch.Tensor(iou_scores).to(device)\n",
        "    else:\n",
        "      b1_x1,b1_x2,b1_x3,b1_x4, b1_y1,b1_y2,b1_y3,b1_y4 = b1_x1[0],b1_x2[0],b1_x3[0],b1_x4[0], b1_y1[0],b1_y2[0],b1_y3[0],b1_y4[0]\n",
        "      r1, b1 = rotation1[0], box1[0]\n",
        "      for x21,x22,x23,x24,y21,y22,y23,y24,r2 ,b2 in zip(b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation2, box2):\n",
        "        f = Feature(geometry={\"coordinates\": [\n",
        "            [list(rotate_around_point((x11, y11), r1, (b1[0], b1[1]))),\n",
        "             list(rotate_around_point((x12, y12), r1, (b1[0], b1[1]))),\n",
        "             list(rotate_around_point((x13, y13), r1, (b1[0], b1[1]))),\n",
        "             list(rotate_around_point((x14, y14), r1, (b1[0], b1[1])))]], \"type\": \"Polygon\"})\n",
        "        b = Feature(geometry={\"coordinates\": [\n",
        "            [list(rotate_around_point((x21, y21), r2, (b2[0], b2[1]))),\n",
        "             list(rotate_around_point((x22, y22), r2, (b2[0], b2[1]))),\n",
        "             list(rotate_around_point((x23, y23), r2, (b2[0], b2[1]))),\n",
        "             list(rotate_around_point((x24, y24), r2, (b2[0], b2[1])))]], \"type\": \"Polygon\"})\n",
        "        try :\n",
        "          inter = intersect([f, b])\n",
        "          if inter == None :\n",
        "            inter_area = 0\n",
        "          else:\n",
        "            inter_area = area(inter) * z\n",
        "        except:\n",
        "          inter_area = 0\n",
        "\n",
        "        # Union Area\n",
        "        try:\n",
        "          b1_area = area(f) * torch.abs(b1_z2 - b1_z1 + 1).item()\n",
        "        except:\n",
        "          print(f)\n",
        "          b1_area = 0\n",
        "        try:\n",
        "          b2_area = area(b) * torch.abs(b2_z2 - b2_z1 + 1).item()\n",
        "        except:\n",
        "          print(b)\n",
        "          b2_area = 0\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "        return torch.Tensor(iou_scores).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRjtf3CfM444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01414af6-c988-43db-b7d9-5e43ddce250c"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "box1 = torch.rand((10, 10))\n",
        "box2 = torch.rand((10, 10))\n",
        "print(box1.shape)\n",
        "bbox_iou(box1,box2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "Rotation 1 : [tensor(5.3684), tensor(3.4848), tensor(3.8935), tensor(5.1658), tensor(2.4932), tensor(4.0954), tensor(3.8423), tensor(2.7136), tensor(5.4920), tensor(2.5544)]\n",
            "Rotation 2 : [tensor(0.8164), tensor(0.6099), tensor(4.2859), tensor(5.7752), tensor(4.0303), tensor(4.2136), tensor(3.9088), tensor(0.4121), tensor(2.0061), tensor(1.9678)]\n",
            "{\"geometry\": {\"coordinates\": [[[0.288922, 0.278553], [0.610439, 0.696292], [-0.174727, 1.300601], [-0.496244, 0.882862]]], \"type\": \"Polygon\"}, \"properties\": {}, \"type\": \"Feature\"}\n",
            "{\"geometry\": {\"coordinates\": [[[-0.34191, 0.181218], [0.150601, -0.342846], [0.565712, 0.047272], [0.073201, 0.571336]]], \"type\": \"Polygon\"}, \"properties\": {}, \"type\": \"Feature\"}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAPGDWYYM-KN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684172cf-16da-4fc5-810c-33981659fd5f"
      },
      "source": [
        "from shapely.geometry import Polygon\n",
        "p1 = Polygon([(737.098633, 267.267059), (725.742432, 256.388184), (751.798462, 229.188828), (763.154663, 240.067703)])\n",
        "p2 = Polygon([(0,1), (1,0), (1,1)])\n",
        "p3 = p1.intersection(p2)\n",
        "print(p3.area)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCywKdPUe-_7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5_fg5onvsN1"
      },
      "source": [
        "#testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtQlxjfFvvLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8fc8cb-b267-4793-aefb-f8546c854105"
      },
      "source": [
        "%time\n",
        "import os\n",
        "\n",
        "img_folder_path = '/content/drive/MyDrive/data/sample0/CAM_BACK'\n",
        "dirListing = os.listdir(img_folder_path)\n",
        "\n",
        "print(len(dirListing))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs\n",
            "Wall time: 5.72 Âµs\n",
            "398\n",
            "CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs\n",
            "Wall time: 3.58 Âµs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMJQzKcsv8t2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}